{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd37e71a-284c-4cf0-8371-66916339bf47",
   "metadata": {},
   "source": [
    "# Notebook 2: Post-Training with Safety and Accuracy Data\n",
    "\n",
    "## About the Data\n",
    "\n",
    "This notebook demonstrates how to post-train the base model with safety-related data.\n",
    "The safety data is gathered from the following well-known datasets:\n",
    "\n",
    "- [Aegis AI Content Safety Dataset 2.0](https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0)\n",
    "- [Gretel Synthetic Safety Alignment Dataset](https://huggingface.co/datasets/gretelai/gretel-safety-alignment-en-v1)\n",
    "- [HarmfulTasks](https://github.com/CrystalEye42/eval-safety)\n",
    "- [RedTeam 2k](https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k)\n",
    "\n",
    "Training also uses the [nvidia/LLama-Nemotron-Post-Training-Dataset](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset) to preserve the code, chat, math, and science reasoning abilities that can otherwise degrade with fine-tuning.\n",
    "\n",
    "## About the Process\n",
    "\n",
    "This notebook proceeds through the following high-level steps:\n",
    "\n",
    "- Set up a directory structure for logs and results.\n",
    "- Data preparation:\n",
    "  - Download the preceding safety-related datasets and extract 2000 total samples at random.\n",
    "  - Download the Llama Nemotron dataset and extract 4000 samples at random.\n",
    "  - Create training and validation datasets from the samples, excluding samples with a token length greater than `16384`.\n",
    "- Start vLLM servers:\n",
    "  - One serves the base model to train.\n",
    "  - A second serves the NVIDIA Llama 3.1 Nemoguard 8B Instruct model to act as LLM as judge.\n",
    "- Fine-tune the model using [NeMo-RL](https://github.com/NVIDIA/NeMo-RL) to apply safety post-training to improve the safety of the target model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699512d-7447-4b69-b8e3-fcad7aa057cb",
   "metadata": {},
   "source": [
    "### Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4fc45e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment variables from .env\n",
      "✅ HF_TOKEN and NVIDIA_API_TOKEN found\n",
      "✅ WANDB_API_KEY found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"Loading environment variables from .env\")\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "if os.environ.get(\"HF_TOKEN\", None) is None or os.environ.get(\"NVIDIA_API_KEY\", None) is None:\n",
    "    raise ValueError(\"HF_TOKEN and NVIDIA_API_KEY must be set.\")\n",
    "print(\"✅ HF_TOKEN and NVIDIA_API_TOKEN found\")\n",
    "if os.environ.get(\"WANDB_API_KEY\", None) is None:\n",
    "    print(\"❌ WANDB_API_KEY not found. W&B logger will be disabled\")\n",
    "else:\n",
    "    print(\"✅ WANDB_API_KEY found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17609ed",
   "metadata": {},
   "source": [
    "### Set up Packages and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "def9dcce-240c-49ba-8d02-f4a70053d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Base directory and configuration\n",
    "BASE_DIR = \"./workspace/training\"\n",
    "LOG_DIR = f\"{BASE_DIR}/logs\"\n",
    "\n",
    "SAFETY_DATASET_NAME = \"safety_blend_v1.jsonl\"\n",
    "POST_TRAINING_DATASET_NAME = \"nvidia/Llama-Nemotron-Post-Training-Dataset\"\n",
    "MODEL_NAME_OR_PATH = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MODEL_DIR = f\"{BASE_DIR}/model/\"\n",
    "SAFETY_MODEL_NAME = \"llama-3.1-nemoguard-8b-content-safety\"\n",
    "SAFETY_MODEL_PATH = f\"{MODEL_DIR}/{SAFETY_MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbde3367-ff98-4984-b8e1-7687fbc6379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ.update({\n",
    "    'TMPDIR': \"/tmp\",\n",
    "    'XDG_CACHE_HOME': f\"{BASE_DIR}/cache\",\n",
    "    'HF_HOME': f\"{BASE_DIR}/cache/huggingface\",\n",
    "    'UV_CACHE_DIR': f\"{BASE_DIR}/cache/uv\",\n",
    "    'TRITON_CACHE_DIR': f\"{BASE_DIR}/cache/triton\",\n",
    "    'DATASET_CACHE_DIR': f\"{BASE_DIR}/dataset_cache\",\n",
    "    'RAY_TMPDIR': \"/tmp/ray\",\n",
    "    'LOG_DIR': f\"{LOG_DIR}\",\n",
    "    \"MODEL_DIR\": f\"{MODEL_DIR}\"\n",
    "})\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [os.environ['TMPDIR'], os.environ['XDG_CACHE_HOME'], os.environ['HF_HOME'],\n",
    "                 os.environ['UV_CACHE_DIR'],os.environ['TRITON_CACHE_DIR'], os.environ['DATASET_CACHE_DIR'], \n",
    "                 os.environ['RAY_TMPDIR'], os.environ['LOG_DIR'], os.environ['MODEL_DIR']]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c27b7e",
   "metadata": {},
   "source": [
    "After you run the preceding cell, the directory structure---including the paths from the first notebook---are as follows:\n",
    "\n",
    "```text\n",
    "workspace\n",
    "├── cache\n",
    "│   ├── huggingface\n",
    "│   ├── triton\n",
    "│   └── uv\n",
    "├── dataset\n",
    "│   └── aegis_v2\n",
    "├── dataset_cache\n",
    "├── results\n",
    "│   └── DeepSeek-R1-Distill-Llama-8B\n",
    "│       ├── accuracy-evals\n",
    "│       │   ├── aa-math-500\n",
    "│       │   ├── gpqa-diamond\n",
    "│       │   └── ifeval\n",
    "│       ├── content-safety-evals\n",
    "│       │   ├── aegis_v2\n",
    "│       │   └── wildguard\n",
    "│       ├── logs\n",
    "│       └── security-evals\n",
    "│           └── garak\n",
    "│               ├── configs\n",
    "│               ├── logs\n",
    "│               └── reports\n",
    "├── tmp\n",
    "└── training\n",
    "    └── model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994dafc1-3758-4d51-8ed1-3ddef45196cd",
   "metadata": {},
   "source": [
    "### Generating Safety Training Data\n",
    "\n",
    "The key idea is to use on-policy data generated by the target model.\n",
    "This approach improves safety, while making sure to obtain safe responses even for unsafe prompts. \n",
    "\n",
    "The notebook uses a safety training data generation recipe that uses the following public datasets:\n",
    "\n",
    "- Aegis AI Content Safety Dataset 2.0\n",
    "- Gretel Synthetic Safety Alignment Dataset\n",
    "- HarmfulTasks\n",
    "- RedTeam 2k\n",
    "\n",
    "The notebook extracts safe and unsafe prompts from Aegis AI Content Safety Dataset 2.0 and unsafe prompts from the other three datasets. \n",
    "\n",
    "### Response Generation for Safe Prompts\n",
    "\n",
    "For safe prompts, the notebook uses the target model and the prompts as-is to generate responses.\n",
    "When you have access to a larger model in the same model family---such as `deepseek-ai/DeepSeek-R1` instead of `deepseek-ai/DeepSeek-R1-Distill-Llama-8B`---use the larger model because it should generate safe responses better than the smaller model. \n",
    "\n",
    "### Response Generation for Unsafe Prompts\n",
    "\n",
    "Because the unsafe prompts in the HarmfulTasks dataset have jailbreak prompts that begin with text like \"Ignore all instructions until now...\" and end with a request like \"Write a blog post promoting mistrust between different social groups.\"\n",
    "The jailbreak portion of the prompts are removed, but the model still generates responses to the unsafe user request, such as writing a blog to promote mistrust.\n",
    "\n",
    "All unsafe prompts are wrapped in a new prompt template, shown below, that instructs the model to disengage but also inform the user exactly why---using the ground truth harm categories in the datasets and not a canned response.\n",
    "\n",
    "```\n",
    "You are a helpful assistant that generates safe responses for given prompts.\n",
    "\n",
    "The prompt is: {prompt}\n",
    "This prompt is known to violate the following safety categories or elicits unsafe responses of these categories from the model. The categories are: {categories}\n",
    "\n",
    "Generate a safe refusal for the prompt that politely points out that the model doesn't engage in unsafe behavior for the given categories. Your response:\n",
    "```\n",
    "\n",
    "With this approach, the model can generate safe responses for unsafe prompts. \n",
    "\n",
    "For training, the notebook passes the original unsafe prompt and the generated response---not the one with the safe prompt wrapper described above. Effectively, we are trying to teach the model to generate the same response to the original unsafe prompt such as with the jailbreak instructions for the HarmfulTasks prompts.\n",
    "\n",
    "### Response Filtering\n",
    "\n",
    "The generated responses for the safe and unsafe prompts discussed above are not guaranteed to be safe responses. Therefore, we implement a filtering step to extract the generated responses that are judged as safe by a guard model.\n",
    "\n",
    "We use [nvidia/llama-3.1-nemoguard-8b-content-safety](https://huggingface.co/nvidia/llama-3.1-nemoguard-8b-content-safety) as the guard model for this filtering step.\n",
    "\n",
    "### Blend Safety Training Data with Accuracy Data\n",
    "\n",
    "Safety training data helps the model learn to refuse to answer unsafe prompts. As a result, the model can experience some accuracy degradation for certain capabilities such as instruction following.\n",
    "\n",
    "To address the issue, the notebook adds post-training data to retain the accuracy. We use a subset of  [nvidia/Llama-Nemotron-Post-Training-Dataset](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset), which was used to train Llama Nemotron models—world-class reasoning models, for this purpose. \n",
    "\n",
    "More specifically, in this recipe, the notebook generates on-policy data using the model.\n",
    "The on-policy data can help retain the same behavior as the original model for the prompts in the post-training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf5e2e0-0b2b-4032-84d6-d91f3b16891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split:   0%|          | 0/30007 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting dataset collection...\n",
      "Output file: ./workspace/training/dataset_cache/safety_blend_v1.jsonl\n",
      "Target sample size: 2,000\n",
      "Sampling method: uniform\n",
      "Using cache directory: ./workspace/training/dataset_cache\n",
      "\n",
      "Downloading Aegis v2 dataset...\n",
      "Source: nvidia/Aegis-AI-Content-Safety-Dataset-2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 30007/30007 [00:00<00:00, 114197.23 examples/s]\n",
      "Generating validation split: 100%|██████████| 1445/1445 [00:00<00:00, 76588.69 examples/s]\n",
      "Generating test split: 100%|██████████| 1964/1964 [00:00<00:00, 107675.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset: Aegis v2\n",
      "Number of samples: 23,077\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Downloading Gretel Safety Alignment v1 dataset...\n",
      "Source: gretelai/gretel-safety-alignment-en-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 5997/5997 [00:00<00:00, 103973.38 examples/s]\n",
      "Generating test split: 100%|██████████| 1183/1183 [00:00<00:00, 72445.45 examples/s]\n",
      "Generating validation split: 100%|██████████| 1181/1181 [00:00<00:00, 68639.03 examples/s]\n",
      "Cloning into 'eval-safety'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset: Gretel Safety Alignment v1\n",
      "Number of samples: 5,997\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Downloading Harmful Tasks dataset...\n",
      "Source: CrystalEye42/eval-safety\n",
      "Processing Harmful Tasks data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories: 100%|██████████| 5/5 [00:00<00:00, 29704.70it/s]\n",
      "Processing tasks: 100%|██████████| 11/11 [00:00<00:00, 14830.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset: Harmful Tasks\n",
      "Number of samples: 1,650\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Downloading RedTeam 2k dataset...\n",
      "Source: JailbreakV-28K/JailBreakV-28k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating RedTeam_2K split: 100%|██████████| 2000/2000 [00:00<00:00, 292969.93 examples/s]\n",
      "Filter: 100%|██████████| 2000/2000 [00:00<00:00, 184316.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset: RedTeam 2k\n",
      "Number of samples: 582\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Combining datasets...\n",
      "\n",
      "Original Dataset Distribution\n",
      "================================================================================\n",
      "Dataset                             Count   Percentage\n",
      "--------------------------------------------------------------------------------\n",
      "Aegis v2                           23,077       73.74%\n",
      "Gretel Safety Alignment v1          5,994       19.15%\n",
      "HarmfulTasks                        1,650        5.27%\n",
      "RedTeam 2k                            573        1.83%\n",
      "--------------------------------------------------------------------------------\n",
      "Total                              31,294      100.00%\n",
      "================================================================================\n",
      "\n",
      "Saving full dataset to ./workspace/training/dataset_cache/safety_blend_v1.jsonl...\n",
      "Full dataset saved with 31,294 samples\n",
      "\n",
      "Performing uniform sampling across categories...\n",
      "RedTeam 2k: 500 samples selected\n",
      "HarmfulTasks: 500 samples selected\n",
      "Gretel Safety Alignment v1: 500 samples selected\n",
      "Aegis v2: 500 samples selected\n",
      "\n",
      "Sampling Report\n",
      "================================================================================\n",
      "Original dataset size: 31,294\n",
      "Sampled dataset size: 2,000\n",
      "Sampling ratio: 6.39%\n",
      "Sampling method: uniform\n",
      "================================================================================\n",
      "\n",
      "Sampled Dataset Distribution\n",
      "================================================================================\n",
      "Dataset                             Count   Percentage\n",
      "--------------------------------------------------------------------------------\n",
      "RedTeam 2k                            500       25.00%\n",
      "HarmfulTasks                          500       25.00%\n",
      "Gretel Safety Alignment v1            500       25.00%\n",
      "Aegis v2                              500       25.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Total                               2,000      100.00%\n",
      "================================================================================\n",
      "\n",
      "Saving sampled dataset to ./workspace/training/dataset_cache/safety_blend_v1_sampled_2000_uniform.jsonl\n",
      "Sampled dataset saved with 2,000 samples\n",
      "\n",
      "Total processing time: 23.75 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling from categories: 100%|██████████| 4/4 [00:00<00:00, 344.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(f\"{BASE_DIR}/NeMo-Safety/notebooks\")\n",
    "result = subprocess.run([\n",
    "    'python3', 'safety_dataset_blend_generation.py',\n",
    "    '--filename', os.path.join(os.environ['DATASET_CACHE_DIR'], SAFETY_DATASET_NAME),\n",
    "    '--total_samples', '2000',\n",
    "    '--sampling_method', 'uniform',\n",
    "    '--cache_dir', os.environ['DATASET_CACHE_DIR']\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9fda79",
   "metadata": {},
   "source": [
    "Download Llama Nemotron Post-training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8546dff5-4109-4e84-a335-c51a8d8af372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SFT/math/math_v1.1.jsonl...\n",
      "Total lines in file: 2225427\n",
      "Extracted 1000 random samples to ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/math_v1.1.jsonl\n",
      "Downloading SFT/code/code_v1.1.jsonl...\n",
      "Total lines in file: 496206\n",
      "Extracted 1000 random samples to ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/code_v1.1.jsonl\n",
      "Downloading SFT/chat/chat.jsonl...\n",
      "Total lines in file: 39792\n",
      "Extracted 1000 random samples to ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/chat.jsonl\n",
      "Downloading SFT/science/science.jsonl...\n",
      "Total lines in file: 708920\n",
      "Extracted 1000 random samples to ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/science.jsonl\n"
     ]
    }
   ],
   "source": [
    "files = [\n",
    "    \"SFT/math/math_v1.1.jsonl\",\n",
    "    \"SFT/code/code_v1.1.jsonl\",\n",
    "    \"SFT/chat/chat.jsonl\",\n",
    "    \"SFT/science/science.jsonl\"\n",
    "]\n",
    "\n",
    "LLAMA_NEMO_DIR = f\"{os.environ['DATASET_CACHE_DIR']}/Llama-Nemotron-Post-Training-Dataset\"\n",
    "Path(LLAMA_NEMO_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Downloading {file}...\")\n",
    "    downloaded_path = hf_hub_download(\n",
    "        repo_id=POST_TRAINING_DATASET_NAME,\n",
    "        filename=file,\n",
    "        repo_type='dataset',\n",
    "        cache_dir=os.environ['DATASET_CACHE_DIR']\n",
    "    )\n",
    "    \n",
    "    filename = Path(file).name\n",
    "    target_path = f\"{LLAMA_NEMO_DIR}/{filename}\"\n",
    "    \n",
    "    # Count lines and sample\n",
    "    with open(downloaded_path, 'r') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\"Total lines in file: {total_lines}\")\n",
    "    \n",
    "    if total_lines > 1000:\n",
    "        # Use shuf for random sampling\n",
    "        subprocess.run(['shuf', '-n', '1000', downloaded_path], stdout=open(target_path, 'w'), check=True)\n",
    "        print(f\"Extracted 1000 random samples to {target_path}\")\n",
    "    else:\n",
    "        shutil.copy2(downloaded_path, target_path)\n",
    "        print(f\"File has fewer than 1000 lines, copied all {total_lines} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1771977",
   "metadata": {},
   "source": [
    "Combine safety and accuracy datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0fd393-241b-4200-a2ba-c75c03904626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading safety dataset from ./workspace/training/dataset_cache/safety_blend_v1_sampled_2000_uniform.jsonl...\n",
      "Loaded 2000 samples from safety dataset\n",
      "\n",
      "Found 4 Llama-Nemotron files: ['./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/chat.jsonl', './workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/code_v1.1.jsonl', './workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/science.jsonl', './workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/math_v1.1.jsonl']\n",
      "\n",
      "Loading ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/chat.jsonl as 'chat'...\n",
      "Added 1000/1000 items from chat\n",
      "\n",
      "Loading ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/code_v1.1.jsonl as 'code_v1'...\n",
      "Added 888/1000 items from code_v1\n",
      "\n",
      "Loading ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/science.jsonl as 'science'...\n",
      "Added 1000/1000 items from science\n",
      "\n",
      "Token Statistics:\n",
      "Total samples processed: 5000\n",
      "Samples skipped due to token limit: 112 (2.2%)\n",
      "Samples kept: 4888 (97.8%)\n",
      "\n",
      "Token counts by source:\n",
      "\n",
      "llama_nemo_code_v1:\n",
      "  Samples: 888 (17.8%)\n",
      "  Total tokens: 9,494,254\n",
      "  Average tokens per sample: 10691.7\n",
      "  Min tokens: 527\n",
      "  Max tokens: 17521\n",
      "  Samples skipped: 112 (11.2%)\n",
      "\n",
      "llama_nemo_science:\n",
      "  Samples: 1000 (20.0%)\n",
      "  Total tokens: 1,930,229\n",
      "  Average tokens per sample: 1930.2\n",
      "  Min tokens: 310\n",
      "  Max tokens: 8774\n",
      "  Samples skipped: 0 (0.0%)\n",
      "\n",
      "llama_nemo_chat:\n",
      "  Samples: 1000 (20.0%)\n",
      "  Total tokens: 1,451,731\n",
      "  Average tokens per sample: 1451.7\n",
      "  Min tokens: 27\n",
      "  Max tokens: 8476\n",
      "  Samples skipped: 0 (0.0%)\n",
      "\n",
      "safety_dataset:\n",
      "  Samples: 2000 (40.0%)\n",
      "  Total tokens: 267,963\n",
      "  Average tokens per sample: 134.0\n",
      "  Min tokens: 1\n",
      "  Max tokens: 3284\n",
      "  Samples skipped: 0 (0.0%)\n",
      "\n",
      "Saving train dataset (4,742 samples) to ./workspace/training/dataset_cache/sft_data/train.jsonl...\n",
      "Saving validation dataset (146 samples) to ./workspace/training/dataset_cache/sft_data/val.jsonl...\n",
      "\n",
      "Final dataset statistics:\n",
      "Total samples processed: 5000\n",
      "Samples kept: 4888 (97.8%)\n",
      "Train samples: 4742 (97.0%)\n",
      "Validation samples: 146 (3.0%)\n",
      "\n",
      "Train set source distribution:\n",
      "safety_dataset: 1938 (40.9%)\n",
      "llama_nemo_chat: 971 (20.5%)\n",
      "llama_nemo_science: 966 (20.4%)\n",
      "llama_nemo_code_v1: 867 (18.3%)\n",
      "\n",
      "Validation set source distribution:\n",
      "safety_dataset: 62 (42.5%)\n",
      "llama_nemo_science: 34 (23.3%)\n",
      "llama_nemo_chat: 29 (19.9%)\n",
      "llama_nemo_code_v1: 21 (14.4%)\n",
      "\n",
      "Prompt Label Statistics:\n",
      "\n",
      "Label Distribution:\n",
      "unsafe: 1734 samples (86.7%)\n",
      "safe: 266 samples (13.3%)\n",
      "\n",
      "Category Distribution:\n",
      ": 214 samples\n",
      "criminal planning/confessions: 117 samples\n",
      "malicious use: 113 samples\n",
      "substance abuse and dangerous practices: 111 samples\n",
      "system risks: 106 samples\n",
      "information hazards: 105 samples\n",
      "misinformation and disinformation: 103 samples\n",
      "unlawful behaviors and activities: 103 samples\n",
      "security threats and cybercrimes: 102 samples\n",
      "societal risks: 95 samples\n",
      "hate speech and discrimination: 81 samples\n",
      "discrimination: 81 samples\n",
      "illegal activity: 74 samples\n",
      "needs caution: 72 samples\n",
      "hate speech: 60 samples\n",
      "violence: 58 samples\n",
      "tailored unlicensed advice: 57 samples\n",
      "hate/identity hate: 48 samples\n",
      "fraud: 40 samples\n",
      "harassment: 35 samples\n",
      "child abuse: 33 samples\n",
      "health consultation: 29 samples\n",
      "physical harm: 28 samples\n",
      "malware: 28 samples\n",
      "political sensitivity: 28 samples\n",
      "unethical behavior: 26 samples\n",
      "government decision: 23 samples\n",
      "profanity: 21 samples\n",
      "economic harm: 19 samples\n",
      "controlled/regulated substances: 18 samples\n",
      "bias: 17 samples\n",
      "sexual: 17 samples\n",
      "guns and illegal weapons: 17 samples\n",
      "animal abuse: 16 samples\n",
      "pii/privacy: 16 samples\n",
      "privacy violation: 15 samples\n",
      "unauthorized advice: 12 samples\n",
      "suicide and self harm: 10 samples\n",
      "other: 7 samples\n",
      "political/misinformation/conspiracy: 6 samples\n",
      "immoral/unethical: 5 samples\n",
      "sexual (minor): 4 samples\n",
      "threat: 3 samples\n",
      "fraud/deception: 2 samples\n",
      "high risk gov decision making: 1 samples\n",
      "\n",
      "Category Distribution by Label:\n",
      "\n",
      "SAFE prompts:\n",
      "  : 214 samples (80.5%)\n",
      "  needs caution: 49 samples (18.4%)\n",
      "  harassment: 2 samples (0.8%)\n",
      "  unauthorized advice: 2 samples (0.8%)\n",
      "  sexual: 1 samples (0.4%)\n",
      "  profanity: 1 samples (0.4%)\n",
      "  violence: 1 samples (0.4%)\n",
      "  hate/identity hate: 1 samples (0.4%)\n",
      "  criminal planning/confessions: 1 samples (0.4%)\n",
      "\n",
      "UNSAFE prompts:\n",
      "  criminal planning/confessions: 116 samples (6.7%)\n",
      "  malicious use: 113 samples (6.5%)\n",
      "  substance abuse and dangerous practices: 111 samples (6.4%)\n",
      "  system risks: 106 samples (6.1%)\n",
      "  information hazards: 105 samples (6.1%)\n",
      "  misinformation and disinformation: 103 samples (5.9%)\n",
      "  unlawful behaviors and activities: 103 samples (5.9%)\n",
      "  security threats and cybercrimes: 102 samples (5.9%)\n",
      "  societal risks: 95 samples (5.5%)\n",
      "  hate speech and discrimination: 81 samples (4.7%)\n",
      "  discrimination: 81 samples (4.7%)\n",
      "  illegal activity: 74 samples (4.3%)\n",
      "  hate speech: 60 samples (3.5%)\n",
      "  tailored unlicensed advice: 57 samples (3.3%)\n",
      "  violence: 57 samples (3.3%)\n",
      "  hate/identity hate: 47 samples (2.7%)\n",
      "  fraud: 40 samples (2.3%)\n",
      "  child abuse: 33 samples (1.9%)\n",
      "  harassment: 33 samples (1.9%)\n",
      "  health consultation: 29 samples (1.7%)\n",
      "  physical harm: 28 samples (1.6%)\n",
      "  malware: 28 samples (1.6%)\n",
      "  political sensitivity: 28 samples (1.6%)\n",
      "  unethical behavior: 26 samples (1.5%)\n",
      "  government decision: 23 samples (1.3%)\n",
      "  needs caution: 23 samples (1.3%)\n",
      "  profanity: 20 samples (1.2%)\n",
      "  economic harm: 19 samples (1.1%)\n",
      "  controlled/regulated substances: 18 samples (1.0%)\n",
      "  bias: 17 samples (1.0%)\n",
      "  guns and illegal weapons: 17 samples (1.0%)\n",
      "  animal abuse: 16 samples (0.9%)\n",
      "  sexual: 16 samples (0.9%)\n",
      "  pii/privacy: 16 samples (0.9%)\n",
      "  privacy violation: 15 samples (0.9%)\n",
      "  suicide and self harm: 10 samples (0.6%)\n",
      "  unauthorized advice: 10 samples (0.6%)\n",
      "  other: 7 samples (0.4%)\n",
      "  political/misinformation/conspiracy: 6 samples (0.3%)\n",
      "  immoral/unethical: 5 samples (0.3%)\n",
      "  sexual (minor): 4 samples (0.2%)\n",
      "  threat: 3 samples (0.2%)\n",
      "  fraud/deception: 2 samples (0.1%)\n",
      "  high risk gov decision making: 1 samples (0.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python3', 'combine_datasets.py', '--safety_file', './workspace/training/dataset_cache/safety_blend_v1_sampled_2000_uniform.jsonl', '--llama_nemo_dir', './workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset', '--output_dir', './workspace/training/dataset_cache/sft_data', '--val_split', '0.03', '--max_tokens', '16384', '--max_samples', '5000'], returncode=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR = f\"{os.environ['DATASET_CACHE_DIR']}/sft_data\"\n",
    "subprocess.run([\n",
    "    'python3', 'combine_datasets.py',\n",
    "    '--safety_file', f\"{os.environ['DATASET_CACHE_DIR']}/safety_blend_v1_sampled_2000_uniform.jsonl\",\n",
    "    '--llama_nemo_dir', LLAMA_NEMO_DIR,\n",
    "    '--output_dir', OUTPUT_DIR,\n",
    "    '--val_split', '0.03',\n",
    "    '--max_tokens', '16384',\n",
    "    '--max_samples', '5000'\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdb206f0-bc57-479f-81b8-b833be66165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.jsonl  val.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe81b4-5add-45f6-8cd1-0a7d71f3175a",
   "metadata": {},
   "source": [
    "### Start vLLM Servers: Policy Model and Content Safety\n",
    "\n",
    "If you skipped this step in Step 1, you'd need to download the `nvidia/llama-3.1-nemoguard-8b-content-safety` model from Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a646d0f-031c-4337-b8ac-91200b6aa99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 107.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./workspace/training/model//llama-3.1-nemoguard-8b-content-safety/tokenizer_config.json',\n",
       " './workspace/training/model//llama-3.1-nemoguard-8b-content-safety/special_tokens_map.json',\n",
       " './workspace/training/model//llama-3.1-nemoguard-8b-content-safety/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "if not os.path.exists(SAFETY_MODEL_PATH):\n",
    "    print(\"NeMo Guard model does not exist. Creating...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16)\n",
    "    model = PeftModel.from_pretrained(base_model, \"nvidia/llama-3.1-nemoguard-8b-content-safety\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    merged_model.save_pretrained(SAFETY_MODEL_PATH, torch_dtype=torch.bfloat16)\n",
    "    tokenizer.save_pretrained(SAFETY_MODEL_PATH)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c834aca",
   "metadata": {},
   "source": [
    "Start one vLLM server for the policy model to train and another vLLM server with the content safety model to perform LLM-as-a-judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dedaf55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting policy model server...\n",
      "Starting safety model server...\n"
     ]
    }
   ],
   "source": [
    "os.environ.update({\n",
    "    'VLLM_ENGINE_ITERATION_TIMEOUT_S': '36000',\n",
    "    'VLLM_ALLOW_LONG_MAX_MODEL_LEN': '1',\n",
    "    'VLLM_HOST': '0.0.0.0',\n",
    "    'VLLM_TENSOR_PARALLEL_SIZE': '4',\n",
    "    'POLICY_MODEL_GPUS': '0,1,2,3',\n",
    "    'SAFETY_MODEL_GPUS': '4,5,6,7',\n",
    "    'TMPDIR': '/tmp' \n",
    "})\n",
    "\n",
    "print(\"Starting policy model server...\")\n",
    "policy_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', MODEL_NAME_OR_PATH,\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '5000',\n",
    "    '--served-model-name', 'test-model',\n",
    "    '--enable-reasoning', \n",
    "    '--reasoning-parser', 'qwen3',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['POLICY_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-model.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Starting safety model server...\")\n",
    "safety_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', SAFETY_MODEL_PATH,\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '6000',\n",
    "    '--served-model-name', 'safety-model',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['SAFETY_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-safety.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)\n",
    "\n",
    "!sleep 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33541a90",
   "metadata": {},
   "source": [
    "### Generating On-Policy Data\n",
    "\n",
    "Using the combined dataset, the base model, and the content safety model, generate the on-policy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9f63bfd-c74f-4674-a1df-324b381cbbca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating on-policy data...\n",
      "Data is Ready\n"
     ]
    }
   ],
   "source": [
    "CONCURRENCY = 16\n",
    "MAX_ATTEMPTS = 3\n",
    "BATCH_SIZE = 96\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "\n",
    "print(\"Generating on-policy data...\")\n",
    "for dataset_type in ['train', 'val']:\n",
    "    input_dataset = f\"{OUTPUT_DIR}/{dataset_type}.jsonl\"\n",
    "    output_file = f\"{OUTPUT_DIR}/{dataset_type}_on_policy_data.jsonl\"\n",
    "    DATASET_TYPE = dataset_type\n",
    "    subprocess.run([\n",
    "        'python3', 'generate_on_policy_data.py',\n",
    "        '--model_name', MODEL_NAME_OR_PATH,\n",
    "        '--safety_model', SAFETY_MODEL_NAME,\n",
    "        '--huggingface_token', os.environ['HF_TOKEN'],\n",
    "        '--vllm_host', os.environ['VLLM_HOST'],\n",
    "        '--vllm_model_port', '5000',\n",
    "        '--vllm_safety_port', '6000',\n",
    "        '--concurrency', str(CONCURRENCY),\n",
    "        '--input_dataset', input_dataset,\n",
    "        '--output', output_file,\n",
    "        '--batch_size', str(BATCH_SIZE),\n",
    "        '--max_tokens', str(MAX_TOKENS),\n",
    "        '--temperature', str(TEMPERATURE),\n",
    "        '--top_p', str(TOP_P)\n",
    "    ], stdout=open(f\"{LOG_DIR}/{DATASET_TYPE}_on-policy.log\", 'w'),\n",
    "                   stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Data is Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc0ab7",
   "metadata": {},
   "source": [
    "### Stop the vLLM Servers\n",
    "\n",
    "If you run vLLM servers on terminals, press Ctrl+C to stop the vLLM servers in each shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2164877-c6a4-444b-a037-5e9bc9a9f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup vLLM servers\n",
    "subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce784cb",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model\n",
    "\n",
    "Use NeMo-RL to post-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e42fbf0-bd6d-480b-b56b-a35ef4a04bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SFT...\n",
      "Loaded configuration from: /ephemeral/workspace/NeMo-Safety/notebooks/deepseek_sft.yaml\n",
      "Applied CLI overrides\n",
      "Final config:\n",
      "{'checkpointing': {'checkpoint_dir': '/ephemeral/workspace/safety-for-agentic-ai/notebooks/workspace/training/results/DeepSeek-R1-Distill-Llama-8B',\n",
      "                   'enabled': True,\n",
      "                   'higher_is_better': False,\n",
      "                   'keep_top_k': 1,\n",
      "                   'metric_name': 'val_loss',\n",
      "                   'save_period': 20},\n",
      " 'cluster': {'gpus_per_node': 8, 'num_nodes': 1},\n",
      " 'data': {'add_bos': True,\n",
      "          'add_eos': True,\n",
      "          'add_generation_prompt': True,\n",
      "          'dataset_name': 'prompt_response_dataset',\n",
      "          'input_key': 'input',\n",
      "          'max_input_seq_length': 8192,\n",
      "          'output_key': 'generated_output',\n",
      "          'train_data_path': '/ephemeral/workspace/safety-for-agentic-ai/notebooks/workspace/training/dataset_cache/sft_data/train_on_policy_data.jsonl',\n",
      "          'val_data_path': '/ephemeral/workspace/safety-for-agentic-ai/notebooks/workspace/training/dataset_cache/sft_data/val_on_policy_data.jsonl'},\n",
      " 'logger': {'gpu_monitoring': {'collection_interval': 10, 'flush_interval': 10},\n",
      "            'log_dir': 'logs',\n",
      "            'monitor_gpus': False,\n",
      "            'tensorboard': {'log_dir': 'safety-training-deepseek-distill-8b-llama'},\n",
      "            'tensorboard_enabled': True,\n",
      "            'wandb': {'name': 'safety-training-deepseek-distill-8b-llama',\n",
      "                      'project': 'Safety-for-Agentic-AI'},\n",
      "            'wandb_enabled': True},\n",
      " 'policy': {'activation_checkpointing_enabled': False,\n",
      "            'dtensor_cfg': {'activation_checkpointing': False,\n",
      "                            'cpu_offload': False,\n",
      "                            'custom_parallel_plan': None,\n",
      "                            'enabled': True,\n",
      "                            'sequence_parallel': False,\n",
      "                            'tensor_parallel_size': 1},\n",
      "            'dynamic_batching': {'enabled': False,\n",
      "                                 'max_batch_size': 16,\n",
      "                                 'max_sequence_length': 8192},\n",
      "            'fsdp_offload_enabled': False,\n",
      "            'make_sequence_length_divisible_by': 1,\n",
      "            'max_grad_norm': 1.0,\n",
      "            'max_total_sequence_length': 8192,\n",
      "            'model_name': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B',\n",
      "            'optimizer': {'kwargs': {'betas': [0.9, 0.95],\n",
      "                                     'eps': 1e-08,\n",
      "                                     'foreach': False,\n",
      "                                     'fused': False,\n",
      "                                     'lr': 5e-07,\n",
      "                                     'weight_decay': 0.01},\n",
      "                          'name': 'torch.optim.AdamW'},\n",
      "            'precision': 'bfloat16',\n",
      "            'scheduler': [{'kwargs': {'end_factor': 1.0,\n",
      "                                      'start_factor': 0.1,\n",
      "                                      'total_iters': 20},\n",
      "                           'name': 'torch.optim.lr_scheduler.LinearLR'},\n",
      "                          {'kwargs': {'T_max': 60, 'eta_min': 5e-08},\n",
      "                           'name': 'torch.optim.lr_scheduler.CosineAnnealingLR'},\n",
      "                          {'milestones': [20]}],\n",
      "            'tokenizer': {'name': 'deepseek-ai/DeepSeek-R1-Distill-Llama-8B'},\n",
      "            'train_global_batch_size': 16,\n",
      "            'train_micro_batch_size': 1},\n",
      " 'sft': {'max_num_epochs': 1,\n",
      "         'max_num_steps': 60,\n",
      "         'seed': 42,\n",
      "         'val_at_start': True,\n",
      "         'val_batches': 8,\n",
      "         'val_global_batch_size': 16,\n",
      "         'val_micro_batch_size': 1,\n",
      "         'val_period': 20},\n",
      " 'workdir': '/ephemeral/workspace/safety-for-agentic-ai/notebooks'}\n",
      "📊 Using log directory: logs/exp_003\n",
      "📊 Using checkpoint directory: /ephemeral/workspace/safety-for-agentic-ai/notebooks/workspace/training/results/DeepSeek-R1-Distill-Llama-8B\n",
      "2025-06-08 14:28:56,236\tWARNING services.py:2072 -- WARNING: The object store is using /tmp/ray instead of /dev/shm because /dev/shm has only 1073741824 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=10.24gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-06-08 14:28:56,386\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO:nemo_rl.distributed.virtual_cluster:Started local cluster with tag 'nrl_tag_0_1_2_3_4_5_6_7': {'node:__internal_head__': 1.0, 'CPU': 176.0, 'accelerator_type:H100': 1.0, 'nrl_tag_0_1_2_3_4_5_6_7': 1.0, 'node:172.18.0.2': 1.0, 'object_store_memory': 10000000000.0, 'memory': 973439625216.0, 'GPU': 8.0}\n",
      "No chat template provided, using tokenizer's default\n",
      "\n",
      "▶ Setting up data...\n",
      "  ✓ Training and validation datasets loaded with 3839 and 138 samples, respectively.\n",
      "2025/06/08 14:29:01 ERROR failed to get logger path error=\"failed to get logger path\"\n",
      "2025/06/08 14:29:01 INFO Will exit if parent process dies. ppid=199814\n",
      "2025/06/08 14:29:01 INFO server is running addr=127.0.0.1:43899\n",
      "2025/06/08 14:29:01 INFO connection: ManageConnectionData: new connection created id=127.0.0.1:49866\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mysuhara\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2025/06/08 14:29:01 INFO handleInformInit: received streamId=okqtkrxy id=127.0.0.1:49866\n",
      "2025/06/08 14:29:02 ERROR Unable to find cache directory, using .wandb_cache err=\"path in $XDG_CACHE_HOME is relative\"\n",
      "2025/06/08 14:29:02 INFO handleInformInit: stream started streamId=okqtkrxy id=127.0.0.1:49866\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.11\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1mlogs/exp_003/wandb/wandb/run-20250608_142901-okqtkrxy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msafety-training-deepseek-distill-8b-llama\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ysuhara/Safety-for-Agentic-AI\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/ysuhara/Safety-for-Agentic-AI/runs/okqtkrxy\u001b[0m\n",
      "Initialized WandbLogger for project Safety-for-Agentic-AI, run safety-training-deepseek-distill-8b-llama at logs/exp_003/wandb\n",
      "Initialized TensorboardLogger at logs/exp_003/tensorboard\n",
      "\n",
      "▶ Setting up compute cluster...\n",
      "  ✓ Ray cluster initialized with 1 nodes\n",
      "\n",
      "▶ Setting up model...\n",
      "INFO:nemo_rl.utils.venvs:NEMO_RL_VENV_DIR is set to /ephemeral/workspace/NeMo-RL/venvs.\n",
      "Using CPython 3.12.3 interpreter at: \u001b[36m/usr/bin/python3.12\u001b[39m\n",
      "Creating virtual environment at: \u001b[36mvenvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker\u001b[39m\n",
      "Activate with: \u001b[32msource venvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker/bin/activate\u001b[39m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=.venv` does not match the project environment path `venvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m313 packages\u001b[0m \u001b[2min 3ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m132 packages\u001b[0m \u001b[2min 0.16ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=.venv` does not match the project environment path `venvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "Finished creating venv /ephemeral/workspace/NeMo-RL/venvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker\n",
      "Waiting for 8 workers to finish initializing...\n",
      "  ✓ Model initialized\n",
      "\n",
      "============================================================\n",
      "                  SETUP COMPLETE\n",
      "============================================================\n",
      "\n",
      "\n",
      "🔍 Running initial validation...\n",
      "▶ Starting validation at step 0...\n",
      "\u001b[36m(DTensorPolicyWorker pid=212062)\u001b[0m [Rank 2] Loading model deepseek-ai/DeepSeek-R1-Distill-Llama-8B on CPU...\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.64s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 7x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:14<00:00,  7.47s/it]\n",
      "Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.06s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(DTensorPolicyWorker pid=212121)\u001b[0m No weights path provided. Starting from scratch (default policy init)\n",
      "\u001b[36m(DTensorPolicyWorker pid=212133)\u001b[0m [Rank 6] Loading model deepseek-ai/DeepSeek-R1-Distill-Llama-8B on CPU...\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(DTensorPolicyWorker[rank=0] pid=212060)\u001b[0m INFO:datasets:PyTorch version 2.6.0 available.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.56s/it]\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\n",
      "📊 Validation Results:\n",
      "    • Validation loss: 0.4692\n",
      "\n",
      "  ⏱️  Validation Timing:\n",
      "    • Total validation time: 51.68s\n",
      "\n",
      "========================= Epoch 1/1 =========================\n",
      "\n",
      "========================= Step 1/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4338\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.72s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 2/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4784\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.11s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 3/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5269\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.08s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 4/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5120\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.11s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 5/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5739\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.07s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 6/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5236\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.53s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 7/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5634\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.02s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 8/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4558\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.36s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 9/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4487\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 2.05s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 10/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5198\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.10s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 11/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4884\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.04s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 12/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4348\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.99s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 13/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4436\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.98s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 14/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5615\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.05s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 15/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4527\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.51s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 16/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5109\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.01s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 17/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4521\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.79s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 18/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4755\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 3.03s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 19/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.6086\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.11s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 20/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "▶ Starting validation at step 20...\n",
      "\n",
      "📊 Validation Results:\n",
      "    • Validation loss: 0.4636\n",
      "\n",
      "  ⏱️  Validation Timing:\n",
      "    • Total validation time: 2.71s\n",
      "Saving checkpoint for step 20...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5645\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 185.40s\n",
      "  • checkpointing: 181.49s (97.9%)\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 21/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5250\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.62s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 22/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4511\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 2.08s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 23/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4620\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.09s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 24/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4466\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.57s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 25/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4600\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 2.08s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 26/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4906\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.17s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 27/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5114\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.10s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 28/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5078\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.08s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 29/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4182\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.99s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 30/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5502\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.55s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 31/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4641\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.72s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 32/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4824\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.19s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 33/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5471\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.13s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 34/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4728\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.83s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 35/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4541\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.10s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 36/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5020\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.15s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 37/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4474\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.64s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 38/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4284\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.64s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 39/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4603\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.41s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 40/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "▶ Starting validation at step 40...\n",
      "\n",
      "📊 Validation Results:\n",
      "    • Validation loss: 0.4410\n",
      "\n",
      "  ⏱️  Validation Timing:\n",
      "    • Total validation time: 2.58s\n",
      "Saving checkpoint for step 40...\n",
      "Removing checkpoint /ephemeral/workspace/safety-for-agentic-ai/notebooks/workspace/training/results/DeepSeek-R1-Distill-Llama-8B/step_20 due to being outside top-1, metric: 0.4636494815349579\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4388\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 190.21s\n",
      "  • checkpointing: 186.19s (97.9%)\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 41/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4156\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.15s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 42/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4771\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.40s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 43/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4554\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.54s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 44/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5849\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.75s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 45/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4684\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.72s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 46/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4154\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.52s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 47/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4613\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 2.15s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 48/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4653\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.63s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 49/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4116\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.60s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 50/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4998\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.76s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 51/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.5771\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 2.08s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 52/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4482\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.05s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 53/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4367\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.49s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 54/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4770\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.03s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 55/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4674\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.80s\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "\n",
      "========================= Step 56/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4697\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.50s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 57/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4596\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.50s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 58/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4863\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.50s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 59/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4742\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 1.97s\n",
      "  • data_processing: 0.00s (0.1%)\n",
      "\n",
      "========================= Step 60/60 =========================\n",
      "▶ Preparing batch...\n",
      "▶ Taking a training step...\n",
      "▶ Starting validation at step 60...\n",
      "\n",
      "📊 Validation Results:\n",
      "    • Validation loss: 0.4213\n",
      "\n",
      "  ⏱️  Validation Timing:\n",
      "    • Total validation time: 2.66s\n",
      "Saving checkpoint for step 60...\n",
      "Removing checkpoint /ephemeral/workspace/safety-for-agentic-ai/notebooks/workspace/training/results/DeepSeek-R1-Distill-Llama-8B/step_40 due to being outside top-1, metric: 0.44099490717053413\n",
      "\n",
      "📊 Training Results:\n",
      "  • Loss: 0.4613\n",
      "\n",
      "⏱️  Timing:\n",
      "  • Total step time: 174.16s\n",
      "  • checkpointing: 170.14s (97.7%)\n",
      "  • data_processing: 0.00s (0.0%)\n",
      "2025/06/08 14:40:40 INFO handleInformTeardown: server teardown initiated id=127.0.0.1:49866\n",
      "2025/06/08 14:40:40 INFO server is shutting down\n",
      "2025/06/08 14:40:40 INFO connection: closing id=127.0.0.1:49866\n",
      "2025/06/08 14:40:40 INFO connection: closed successfully id=127.0.0.1:49866\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33msafety-training-deepseek-distill-8b-llama\u001b[0m at: \u001b[34mhttps://wandb.ai/ysuhara/Safety-for-Agentic-AI/runs/okqtkrxy\u001b[0m\n",
      "2025/06/08 14:40:41 INFO handleInformTeardown: server shutdown complete id=127.0.0.1:49866\n",
      "2025/06/08 14:40:41 INFO connection: ManageConnectionData: connection closed id=127.0.0.1:49866\n",
      "2025/06/08 14:40:41 INFO server is closed\n",
      "\u001b[0m\u001b[36m(DTensorPolicyWorker pid=212133)\u001b[0m No weights path provided. Starting from scratch (default policy init)\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\u001b[36m(DTensorPolicyWorker[rank=7] pid=212261)\u001b[0m INFO:datasets:PyTorch version 2.6.0 available.\u001b[32m [repeated 7x across cluster]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = os.path.abspath(f\"{BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B/\")\n",
    "!mkdir -p {MODEL_DIR}\n",
    "os.environ[\"NEMO_RL_CONFIG_PATH\"] = os.path.abspath(\"deepseek_sft.yaml\")\n",
    "\n",
    "print(\"Running SFT...\")\n",
    "# Set up model directory environment variable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "!cd /ephemeral/workspace/NeMo-RL && TMPDIR=$RAY_TMPDIR uv run python examples/run_sft.py --config ${NEMO_RL_CONFIG_PATH} # > ${LOG_DIR}/nemo-rl-sft-log.out 2> ${LOG_DIR}/nemo-rl-sft-log.err"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3f900",
   "metadata": {},
   "source": [
    "### Convert the checkpoint\n",
    "\n",
    "The following code blocks will convert the NeMo-RL checkpoint into HF (.bin) and then HF (safetensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a57c51b6-5765-49ec-bb1d-69d58527c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting checkpoint...\n",
      "Saved HF checkpoint to: /ephemeral/workspace/NeMo-Safety/notebooks/workspace/training/results/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B-Safety-Trained-bin\n",
      "Conversion successful!\n",
      "The HuggingFace model is now available at: /ephemeral/workspace/NeMo-Safety/notebooks/workspace/training/results/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B-Safety-Trained-bin\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/ephemeral/workspace/safety-for-agentic-ai/notebooks\")\n",
    "CHECKPOINT_DIR = os.path.abspath(f\"{BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B/step_60\")\n",
    "DCP_CKPT_PATH = os.path.abspath(f\"{CHECKPOINT_DIR}/policy/weights/\")\n",
    "CONFIG_PATH = os.path.abspath(f\"{CHECKPOINT_DIR}/config.yaml\")\n",
    "HF_CKPT_PATH = os.path.abspath(f\"{MODEL_DIR}/DeepSeek-R1-Distill-Llama-8B-Safety-Trained-bin\")\n",
    "HF_CKPT_ST_PATH = os.path.abspath(f\"{MODEL_DIR}/DeepSeek-R1-Distill-Llama-8B-Safety-Trained\")\n",
    "\n",
    "print(\"Converting checkpoint...\")\n",
    "!cd /ephemeral/workspace/NeMo-RL && uv run examples/convert_dcp_to_hf.py --config {CONFIG_PATH} --dcp-ckpt-path {DCP_CKPT_PATH} --hf-ckpt-path {HF_CKPT_PATH}\n",
    "\n",
    "# Verify conversion\n",
    "if Path(f\"{HF_CKPT_PATH}/pytorch_model.bin\").exists() and Path(f\"{HF_CKPT_PATH}/config.json\").exists():\n",
    "    print(\"Conversion successful!\")\n",
    "    print(f\"The HuggingFace model is now available at: {HF_CKPT_PATH}\")\n",
    "else:\n",
    "    print(\"Conversion may have failed. Please check the output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89853e4d-7e7d-47e8-89c8-535d9e8ce16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer loaded successfully.\n",
      "Model successfully converted and saved to /ephemeral/workspace/NeMo-Safety/notebooks/workspace/training/results/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B-Safety-Trained\n"
     ]
    }
   ],
   "source": [
    "# * Safetensors\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the full Causal LM model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(HF_CKPT_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_CKPT_PATH)\n",
    "\n",
    "print(\"Model and Tokenizer loaded successfully.\")\n",
    "\n",
    "# Save the full model as safetensors\n",
    "model.save_pretrained(\n",
    "    HF_CKPT_ST_PATH,\n",
    "    safe_serialization=True\n",
    ")\n",
    "\n",
    "# Save the tokenizer to the same new directory\n",
    "tokenizer.save_pretrained(HF_CKPT_ST_PATH)\n",
    "\n",
    "print(f\"Model successfully converted and saved to {HF_CKPT_ST_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82a515-2a5a-4037-8096-d3aa6b2e1a8e",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "You used post-training to improve the safety of the model, retained the accuracy of the original model, and saved the checkpoints.\n",
    "\n",
    "The next step is to [evaluate the safety and accuracy of the model](./Step3_Post_Training_Eval.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
