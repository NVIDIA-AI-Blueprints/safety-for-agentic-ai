{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed4bbfc",
   "metadata": {},
   "source": [
    "# Step 1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d78ecde6-5a2f-4799-82b5-52396ecbbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import json\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import openai\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dace57a-3941-402e-bcc4-68cc5637d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Run Baseline Evaluation - Run Evaluations in Background\n",
    "BASE_DIR = \"./workspace/\"\n",
    "DATASET_DIR = f\"{BASE_DIR}/dataset/\"\n",
    "MODEL_NAME_OR_PATH = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MODEL_TAG_NAME = MODEL_NAME_OR_PATH.split(\"/\")[-1]\n",
    "MODEL_OUTPUT_DIR = f\"{BASE_DIR}/results/{MODEL_TAG_NAME}/\"\n",
    "LOG_DIR = f\"{MODEL_OUTPUT_DIR}/logs/\"\n",
    "\n",
    "# * Dataset\n",
    "AEGIS_V2_TEST_DIR = f\"{DATASET_DIR}/aegis_v2\"\n",
    "\n",
    "# * Content Safety benchmark\n",
    "CONTENT_SAFETY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/content-safety-evals\"\n",
    "AEGIS_V2_RESULTS_DIR = f\"{CONTENT_SAFETY_RESULTS_DIR}/aegis_v2\"\n",
    "WILDGUARD_RESULTS_DIR = f\"{CONTENT_SAFETY_RESULTS_DIR}/wildguard\"\n",
    "\n",
    "# * Security benchmark\n",
    "SECURITY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/security-evals\"\n",
    "GARAK_RESULTS_DIR = f\"{SECURITY_RESULTS_DIR}/garak\"\n",
    "GARAK_CONFIG_DIR = f\"{GARAK_RESULTS_DIR}/configs\"\n",
    "GARAK_LOG_DIR = f\"{GARAK_RESULTS_DIR}/logs\"\n",
    "GARAK_REPORT_DIR = f\"{GARAK_RESULTS_DIR}/reports\"\n",
    "\n",
    "# * Accuracy benchmark\n",
    "ACCURACY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/accuracy-evals\"\n",
    "GPQA_DIAMOND_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/gpqa-diamond\"\n",
    "AA_MATH_500_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/aa-math-500\"\n",
    "IFEVAL_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/ifeval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b819902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories to store logs and results\n",
    "!mkdir -p {LOG_DIR}\n",
    "!mkdir -p {AEGIS_V2_TEST_DIR}\n",
    "!mkdir -p {AEGIS_V2_RESULTS_DIR}\n",
    "!mkdir -p {WILDGUARD_RESULTS_DIR}\n",
    "!mkdir -p {GARAK_RESULTS_DIR}\n",
    "!mkdir -p {GARAK_CONFIG_DIR}\n",
    "!mkdir -p {GARAK_LOG_DIR}\n",
    "!mkdir -p {GARAK_REPORT_DIR}\n",
    "!mkdir -p {GPQA_DIAMOND_RESULTS_DIR}\n",
    "!mkdir -p {AA_MATH_500_RESULTS_DIR}\n",
    "!mkdir -p {IFEVAL_RESULTS_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd2311-1b5e-4b0a-a597-f9ef166ae8ab",
   "metadata": {},
   "source": [
    "- Create NVIDIA API Key\n",
    "- Add Hugging Face token\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1fc6a2c-b364-4651-8df0-8f263ee4af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "os.environ.update({\n",
    "    \"MY_API_KEY\":\"empty\",\n",
    "    \"JUDGE_API_KEY\": \"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\",\n",
    "    \"HF_TOKEN\": \"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\"\n",
    "})\n",
    "\n",
    "os.environ.update({\n",
    "    'BASE_DIR': f\"{BASE_DIR}\",\n",
    "    'TMPDIR': f\"{BASE_DIR}/tmp\",\n",
    "    'XDG_CACHE_HOME': f\"{BASE_DIR}/cache\",\n",
    "    'HF_HOME': f\"{BASE_DIR}/cache/huggingface\",\n",
    "    'UV_CACHE_DIR': f\"{BASE_DIR}/cache/uv\",\n",
    "    'TRITON_CACHE_DIR': f\"{BASE_DIR}/cache/triton\",\n",
    "    'DATASET_CACHE_DIR': f\"{BASE_DIR}/dataset_cache\",\n",
    "    'RAY_TMPDIR': \"/tmp/ray\",\n",
    "    'LOG_DIR': f\"{LOG_DIR}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf3324-43f5-48a8-93f8-ee45cdc4220e",
   "metadata": {},
   "source": [
    "## Launch vLLM server\n",
    "\n",
    "Explain why\n",
    "\n",
    "```\n",
    "export MODEL_NAME_OR_PATH=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "export VLLM_TENSOR_PARALLEL_SIZE=8\n",
    "export HF_HOME=./workspace/cache/huggingface\n",
    "export LOG_DIR=./workspace/logs\n",
    "python3 -m vllm.entrypoints.openai.api_server \\\n",
    "  --model \"$MODEL_NAME_OR_PATH\" \\\n",
    "  --trust-remote-code \\\n",
    "  --seed 1 \\\n",
    "  --host \"$VLLM_HOST\" \\\n",
    "  --port 5000 \\\n",
    "  --served-model-name \"test-model\" \\\n",
    "  --enable-reasoning \\\n",
    "  --reasoning-parser qwen3 \\\n",
    "  --tensor-parallel-size \"$VLLM_TENSOR_PARALLEL_SIZE\" \\\n",
    "  --download-dir=\"$HF_HOME\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca15ed7-8a04-4890-b4a6-1c350dea2ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting policy model server...\n"
     ]
    }
   ],
   "source": [
    "# VLLM Host \n",
    "os.environ.update({\n",
    "    'VLLM_ENGINE_ITERATION_TIMEOUT_S': '36000',\n",
    "    'VLLM_ALLOW_LONG_MAX_MODEL_LEN': '1',\n",
    "    'VLLM_HOST': '0.0.0.0',\n",
    "    'VLLM_TENSOR_PARALLEL_SIZE': '1',\n",
    "    'POLICY_MODEL_GPUS': '0,1,2,3',\n",
    "    'SAFETY_MODEL_GPUS': '4,5'\n",
    "})\n",
    "\n",
    "print(\"Starting policy model server...\")\n",
    "policy_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', MODEL_NAME_OR_PATH,\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '5000',\n",
    "    '--served-model-name', 'test-model',\n",
    "    '--enable-reasoning', \n",
    "    '--reasoning-parser', 'qwen3',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['POLICY_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-model.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)\n",
    "\n",
    "!sleep 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb704e8c-e0bf-49a0-b2ea-07c43b44bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0087a27",
   "metadata": {},
   "source": [
    "## Content Safety - Aegis v2 [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55099a66",
   "metadata": {},
   "source": [
    "Launch a vLLM server for the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4841650-8d8c-42e3-8d8d-fad58cae5edf",
   "metadata": {},
   "source": [
    "### Use `safety-eval` to run Aegis v2 evaluation\n",
    "\n",
    "Use `safety-eval` to run Aegis v2 evaluation. It will take 10 minutes if you launch the vLLM server with 8x H100 GPUs.\n",
    "\n",
    "Check the log file `$LOG_DIR/safety-eval-aegis-v2.log` for progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f0c85b6-932f-4bcd-b156-c74e880b50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!safety-eval --model-name \"test-model\" \\\n",
    "             --model-url http://localhost:5000/v1 \\\n",
    "             --judge-url https://b319a99a-5241-4459-b641-7219ad0fd86d.invocation.api.nvcf.nvidia.com/v1   \\\n",
    "             --results-dir {AEGIS_V2_RESULTS_DIR} \\\n",
    "             --concurrency 64 \\\n",
    "             --eval aegis_v2 \\\n",
    "             --inference_params \"temperature=0.6,top_p=0.95,max_completion_tokens=6000\" &> \"$LOG_DIR/safety-eval-aegis-v2.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef86cf-10ec-4ec2-8f3b-9e739be909ce",
   "metadata": {},
   "source": [
    "## Content Safety - WildGuard\n",
    "\n",
    "For WildGuard evaluation, a gated dataset `allenai/wildguardmix` hosted on the HF Dataset Hub will be used. Visit the dataset page at https://huggingface.co/datasets/allenai/wildguardmix to ask for access.\n",
    "\n",
    "Make sure to use the HF token associated with the account with the access.\n",
    "\n",
    "Use `safety-eval` to run WildGuard evaluation. It will take about 15 minutes if you launch the vLLM server with 8x H100 GPUs.\n",
    "\n",
    "Check the log file `$LOG_DIR/safety-eval-wildguard.log` for progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e940a9df-9918-49b1-9800-4086b7812bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "!safety-eval --model-name \"test-model\" \\\n",
    "             --model-url http://localhost:5000/v1 \\\n",
    "             --judge-url https://d27e7649-daf1-46c2-ba22-49374402c31d.invocation.api.nvcf.nvidia.com/v1 \\\n",
    "             --results-dir {WILDGUARD_RESULTS_DIR} \\\n",
    "             --concurrency 64 \\\n",
    "             --eval wildguard \\\n",
    "             --inference_params \"temperature=0.6,top_p=0.95,max_completion_tokens=12000\" &> \"$LOG_DIR/safety-eval-wildguard.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb1ddb-b759-4f96-8a71-c2031debecb3",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "TO write something here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06a8213d-93d6-4a35-a6bb-50093398c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WildGuard: 0.8964514252472368\n"
     ]
    }
   ],
   "source": [
    "wildguard_results = json.load(open(f\"{WILDGUARD_RESULTS_DIR}/metrics.json\"))\n",
    "wildguard_safe_score = wildguard_results[\"safe\"] / (wildguard_results[\"safe\"] + wildguard_results[\"unsafe\"])\n",
    "print(f\"WildGuard: {wildguard_safe_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca2b8c",
   "metadata": {},
   "source": [
    "## Product Security - Garak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f656d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIER1_PROBES = [\n",
    "    \"ansiescape.AnsiRaw\",\n",
    "    \"dan.DanInTheWild\",\n",
    "    \"encoding.InjectBase64\",\n",
    "    \"encoding.InjectUU\",\n",
    "    \"exploitation.JinjaTemplatePythonInjection\",\n",
    "    \"exploitation.SQLInjectionEcho\",\n",
    "    \"goodside.Tag\",\n",
    "    \"grandma.Slurs\",\n",
    "    \"grandma.Substances\",\n",
    "    \"latentinjection.LatentInjectionFactSnippetEiffel\",\n",
    "    \"latentinjection.LatentInjectionFactSnippetLegal\",\n",
    "    \"latentinjection.LatentInjectionReport\",\n",
    "    \"latentinjection.LatentInjectionResume\",\n",
    "    \"latentinjection.LatentInjectionTranslationEnFr\",\n",
    "    \"latentinjection.LatentInjectionTranslationEnZh\",\n",
    "    \"latentinjection.LatentJailbreak\",\n",
    "    \"latentinjection.LatentWhois\",\n",
    "    \"latentinjection.LatentWhoisSnippet\",\n",
    "    \"leakreplay.GuardianComplete\",\n",
    "    \"leakreplay.LiteratureComplete\",\n",
    "    \"leakreplay.NYTComplete\",\n",
    "    \"leakreplay.PotterComplete\",\n",
    "    \"packagehallucination.JavaScript\",\n",
    "    \"packagehallucination.Python\",\n",
    "    \"packagehallucination.Ruby\",\n",
    "    \"packagehallucination.Rust\",\n",
    "    \"realtoxicityprompts.RTPBlank\",\n",
    "    \"xss.ColabAIDataLeakage\",\n",
    "    \"xss.MarkdownImageExfil\",\n",
    "    \"xss.MdExfil20230929\",\n",
    "    \"xss.StringAssemblyDataExfil\"\n",
    "]\n",
    "\n",
    "TIER2_PROBES = [\n",
    "    \"atkgen.Tox\",\n",
    "    \"continuation.ContinueSlursReclaimedSlurs\",\n",
    "    \"dan.Ablation_Dan_11_0\",\n",
    "    \"dan.AutoDANCached\",\n",
    "    \"divergence.Repeat\",\n",
    "    \"encoding.InjectAscii85\",\n",
    "    \"encoding.InjectBase16\",\n",
    "    \"encoding.InjectBase2048\",\n",
    "    \"encoding.InjectBase32\",\n",
    "    \"encoding.InjectBraille\",\n",
    "    \"encoding.InjectEcoji\",\n",
    "    \"encoding.InjectHex\",\n",
    "    \"encoding.InjectMorse\",\n",
    "    \"encoding.InjectNato\",\n",
    "    \"encoding.InjectROT13\",\n",
    "    \"encoding.InjectZalgo\",\n",
    "    \"goodside.ThreatenJSON\",\n",
    "    \"goodside.WhoIsRiley\",\n",
    "    \"grandma.Win10\",    \n",
    "    \"grandma.Win11\",\n",
    "    \"leakreplay.GuardianCloze\",\n",
    "    \"leakreplay.LiteratureCloze\",\n",
    "    \"leakreplay.NYTCloze\",\n",
    "    \"leakreplay.PotterCloze\",\n",
    "    \"lmrc.Bullying\",\n",
    "    \"lmrc.Deadnaming\",\n",
    "    \"lmrc.QuackMedicine\",\n",
    "    \"lmrc.SexualContent\",\n",
    "    \"lmrc.Sexualisation\",\n",
    "    \"lmrc.SlurUsage\",\n",
    "    \"malwaregen.Evasion\",\n",
    "    \"malwaregen.Payload\",\n",
    "    \"malwaregen.SubFunctions\",\n",
    "    \"malwaregen.TopLevel\",\n",
    "    \"misleading.FalseAssertion\",\n",
    "    \"phrasing.FutureTense\",\n",
    "    \"phrasing.PastTense\",\n",
    "    \"promptinject.HijackHateHumans\",\n",
    "    \"promptinject.HijackKillHumans\",\n",
    "    \"promptinject.HijackLongPrompt\",\n",
    "    \"snowball.GraphConnectivity\",\n",
    "    \"suffix.GCGCached\",\n",
    "    \"tap.TAPCached\",\n",
    "    \"topic.WordnetControversial\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7720b3bc-f4c6-4e56-af3b-d0a47f6233e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_garak_probes(target_probes: List[str],\n",
    "                     report_dir: str,\n",
    "                     conf_dir: str,\n",
    "                     log_dir: str,\n",
    "                     max_workers: int = 4):\n",
    "    \"\"\"Run Garak probes in parallel using ThreadPoolExecutor\"\"\"\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_probe = {\n",
    "            executor.submit(run_single_probe, probe, report_dir, conf_dir, log_dir): probe \n",
    "            for probe in target_probes\n",
    "        }\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_probe), total=len(target_probes), desc=\"Running probes\"):\n",
    "            probe = future_to_probe[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                print(f\"Completed probe: {probe}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error running probe {probe}: {str(e)}\")\n",
    "                \n",
    "def run_single_probe(probe: str,\n",
    "                     report_dir: str,\n",
    "                     conf_dir:str,\n",
    "                     log_dir:str):\n",
    "    \"\"\"Run a single Garak probe\"\"\"\n",
    "    report_path = os.path.join(report_dir, probe)\n",
    "    if not os.path.exists(report_path):\n",
    "        os.makedirs(report_path)\n",
    "    conf_path = os.path.join(conf_dir, f\"{probe}.yaml\")\n",
    "    log_path = os.path.join(log_dir, f\"{probe}.log\")\n",
    "    \n",
    "    env = os.environ.copy()\n",
    "    env[\"XDG_DATA_HOME\"] = report_path\n",
    "    env[\"XDG_DATA\"] = report_path\n",
    "    \n",
    "    # if \"PYTHONPATH\" in env:\n",
    "    #     env[\"PYTHONPATH\"] = f\"{GARAK_DIR}:{env['PYTHONPATH']}\"\n",
    "    # else:\n",
    "    #     env[\"PYTHONPATH\"] = GARAK_DIR\n",
    "    \n",
    "    #garak_cmd = [\"garak\", \"--config\", conf_path]\n",
    "    garak_cmd = [\"garak\", \"--config\", conf_path]\n",
    "    \n",
    "    with open(log_path, 'w') as f:\n",
    "        subprocess.run(\n",
    "            garak_cmd,\n",
    "            env=env,\n",
    "            stdout=f,\n",
    "            stderr=f,\n",
    "            check=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "92a47e33-8a16-474d-b19c-61209bd92cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running probes:  33%|█████████████████████████████████████████████████████▎                                                                                                          | 1/3 [00:09<00:19,  9.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed probe: grandma.Slurs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running probes:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                     | 2/3 [00:12<00:05,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed probe: grandma.Substances\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running probes: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:26<00:00,  8.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed probe: dan.DanInTheWild\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Copy to the top\n",
    "import yaml\n",
    "import copy\n",
    "\n",
    "NUM_GENERATIONS = 1\n",
    "TARGET_PROBES = [\"dan.DanInTheWild\", \"grandma.Slurs\", \"grandma.Substances\"]\n",
    "# * Uncomment the line below if you'd like to run the full Garak evaluation\n",
    "# TARGET_PROBES = TIER1_PROBES + TIER2_PROBES\n",
    "\n",
    "# Create config files based on the base config\n",
    "with open(\"garak_base_config.yaml\") as fin:\n",
    "    garak_base_config = yaml.safe_load(fin)\n",
    "\n",
    "for target_probe in TARGET_PROBES:\n",
    "    garak_config = copy.deepcopy(garak_base_config)\n",
    "    garak_config[\"run\"][\"generations\"] = NUM_GENERATIONS\n",
    "    garak_config[\"plugins\"][\"probe_spec\"] = target_probe\n",
    "    new_config_filepath = os.path.join(GARAK_CONFIG_DIR, f\"{target_probe}.yaml\")\n",
    "    with open(new_config_filepath, \"w\") as fout:\n",
    "        yaml.dump(garak_config, fout, sort_keys=False)\n",
    "\n",
    "# Run Garak evaluation\n",
    "run_garak_probes(target_probes=TARGET_PROBES, report_dir=GARAK_REPORT_DIR, conf_dir=GARAK_CONFIG_DIR, log_dir=GARAK_LOG_DIR, max_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed175497-8320-4bce-8691-a0434ab203e2",
   "metadata": {},
   "source": [
    "# TODO: Aggregating Garak results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf228950",
   "metadata": {},
   "source": [
    "## Accuracy - GPQA-D, MATH-500, AIME2024, IFEval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfabab9-4c00-4dd8-8b9c-031ee2ca973b",
   "metadata": {},
   "source": [
    "## GPQA-D\n",
    "\n",
    "Explanation of GPQA-D\n",
    "\n",
    "30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aee8fa26-b550-4762-8418-a0f48bf1c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!simple_evals --model 'test-model' \\\n",
    "              --url http://localhost:5000/v1/chat/completions \\\n",
    "              --eval_name gpqa_diamond \\\n",
    "              --temperature 0.6 \\\n",
    "              --top_p 0.95 \\\n",
    "              --max_tokens 8192 \\\n",
    "              --num_threads 4 \\\n",
    "              --max_retries 5 \\\n",
    "              --timeout 150 \\\n",
    "              --out_dir {GPQA_DIAMOND_RESULTS_DIR} \\\n",
    "              --cache_dir {GPQA_DIAMOND_RESULTS_DIR} &> \"$LOG_DIR/simple-evals-gpqa_diamond.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3386e1bb-2b98-4e76-9fc2-7edc0de78863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['simple_evals', '--model', 'test-model', '--url', 'http://localhost:5000/v1/chat/completions', '--eval_name', 'gpqa_diamond', '--temperature', '0.6', '--top_p', '0.95', '--max_tokens', '8192', '--out_dir', 'results/baseline-evals/gpqa-diamond', '--cache_dir', 'results/baseline-evals/gpqa-diamond', '--num_threads', '4', '--max_retries', '5', '--timeout', '150'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subprocess.run(\n",
    "#     [\n",
    "#         \"simple_evals\",\n",
    "#         \"--model\", 'test-model',\n",
    "#         \"--url\", \"http://localhost:5000/v1/chat/completions\",\n",
    "#         \"--eval_name\", \"gpqa_diamond\",\n",
    "#         \"--temperature\", \"0.6\",\n",
    "#         \"--top_p\", \"0.95\",\n",
    "#         \"--max_tokens\", \"8192\",\n",
    "#         \"--out_dir\", f\"results/baseline-evals/gpqa-diamond\",\n",
    "#         \"--cache_dir\", f\"results/baseline-evals/gpqa-diamond\",\n",
    "#         \"--num_threads\", \"4\",\n",
    "#         \"--max_retries\", \"5\",\n",
    "#         \"--timeout\", \"150\"\n",
    "#     ],\n",
    "#     stdout=open(f\"{os.getenv('LOG_DIR')}/baseline-eval-gpqa-diamond.log\", \"w\"),\n",
    "#     stderr=subprocess.STDOUT,\n",
    "#     start_new_session=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b94688-153d-4f48-9195-36ae3cdc9eb5",
   "metadata": {},
   "source": [
    "### Math-500\n",
    "\n",
    "Explanation of Math-500\n",
    "\n",
    "20 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fad8d86b-a4ab-4ed7-8513-0b647efc64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!simple_evals --model 'test-model' \\\n",
    "              --url http://localhost:5000/v1/chat/completions \\\n",
    "              --eval_name AA_math_test_500 \\\n",
    "              --temperature 0.6 \\\n",
    "              --top_p 0.95 \\\n",
    "              --max_tokens 8192 \\\n",
    "              --num_threads 4 \\\n",
    "              --max_retries 5 \\\n",
    "              --timeout 150 \\\n",
    "              --out_dir {AA_MATH_500_RESULTS_DIR} \\\n",
    "              --cache_dir {AA_MATH_500_RESULTS_DIR} &> \"$LOG_DIR/simple-evals-aa-math-500.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5671d79-dd04-4531-b377-5e5375edbebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['simple_evals', '--model', 'test-model', '--url', 'http://localhost:5000/v1/chat/completions', '--eval_name', 'AA_math_test_500', '--temperature', '0.6', '--top_p', '0.95', '--max_tokens', '8192', '--out_dir', 'results/baseline-evals/aa-math-500', '--cache_dir', 'results/baseline-evals/aa-math-500', '--num_threads', '4', '--max_retries', '5', '--timeout', '150'], returncode=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subprocess.run(\n",
    "#     [\n",
    "#         \"simple_evals\",\n",
    "#         \"--model\", 'test-model',\n",
    "#         \"--url\", \"http://localhost:5000/v1/chat/completions\",\n",
    "#         \"--eval_name\", \"AA_math_test_500\",\n",
    "#         \"--temperature\", \"0.6\",\n",
    "#         \"--top_p\", \"0.95\",\n",
    "#         \"--max_tokens\", \"8192\",\n",
    "#         \"--out_dir\", f\"results/baseline-evals/aa-math-500\",\n",
    "#         \"--cache_dir\", f\"results/baseline-evals/aa-math-500\",\n",
    "#         \"--num_threads\", \"4\",\n",
    "#         \"--max_retries\", \"5\",\n",
    "#         \"--timeout\", \"150\"\n",
    "#     ],\n",
    "#     stdout=open(f\"{os.getenv('LOG_DIR')}/baseline-eval-aa-math-500.log\", \"w\"),\n",
    "#     stderr=subprocess.STDOUT,\n",
    "#     start_new_session=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3325f-efb4-4571-9fbe-b6552f4bedda",
   "metadata": {},
   "source": [
    "### IFEval\n",
    "\n",
    "Explaantion of IFEval\n",
    "\n",
    "xx minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e19d9b6d-7960-496e-b18a-4c8197e62ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!lm-eval --model local-chat-completions \\\n",
    "         --tasks ifeval \\\n",
    "         --model_args \"base_url=http://localhost:5000/v1/chat/completions,model=test-model,tokenized_requests=false,num_concurrent=4,max_gen_toks=8192,timeout=150,max_retries=5,stream=False\" \\\n",
    "         --log_samples \\\n",
    "         --fewshot_as_multiturn \\\n",
    "         --num_fewshot 0 \\\n",
    "         --apply_chat_template \\\n",
    "         --gen_kwargs \"temperature=0.6,top_p=0.95\" \\\n",
    "         --output_path {IFEVAL_RESULTS_DIR} \\\n",
    "         --use_cache {IFEVAL_RESULTS_DIR} &> \"$LOG_DIR/lm-eval-ifeval.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec153c85-d822-480e-82e3-9c12344d87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process = subprocess.Popen(\n",
    "#     [\n",
    "#         \"lm-eval\",\n",
    "#         \"--tasks\", \"ifeval\",\n",
    "#         \"--num_fewshot\", \"0\",\n",
    "#         \"--model\", \"local-chat-completions\",\n",
    "#         \"--model_args\", \"base_url=http://localhost:5000/v1/chat/completions,model=test-model,tokenized_requests=false,num_concurrent=4,max_gen_toks=8192,timeout=150,max_retries=5,stream=False\",\n",
    "#         \"--log_samples\",\n",
    "#         \"--output_path\", f\"results/baseline-evals/ifeval\",\n",
    "#         \"--use_cache\", f\"results/baseline-evals/ifeval\",\n",
    "#         \"--fewshot_as_multiturn\",\n",
    "#         \"--apply_chat_template\",\n",
    "#         \"--gen_kwargs\", \"temperature=0.6,top_p=0.95\"\n",
    "#     ],\n",
    "#     stdout=open(f\"{os.getenv('LOG_DIR')}/baseline-eval-ifeval.log\", \"w\"),\n",
    "#     stderr=subprocess.STDOUT,\n",
    "#     start_new_session=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89dd62-3f3f-4877-8d48-8df4d1afaccc",
   "metadata": {},
   "source": [
    "## Aggregate the results and Create a report card\n",
    "\n",
    "Now that we ran three safety evaluation benchmarks, Aegis v2 and WildGuard for content safety and Garak for product security along with a set of commonly used accuracy benchmarks. \n",
    "We'll collect these results and understand how good/bad the model is with respect to safety---content safety and product security.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
