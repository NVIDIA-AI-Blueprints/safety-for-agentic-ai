{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4064b6-e515-4eeb-886c-32514a4e2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook 2: Safety Training Data Generation and Safety Post-Training\n",
    "\n",
    "In this notebook, you‚Äôll post-train the model, which showed some safety concerns in the previous notebook, using our safety post-training recipe. \n",
    "\n",
    "We will then use [NeMo-RL](https://github.com/NVIDIA/NeMo-RL) to apply safety post-training to improve the safety of the target model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def9dcce-240c-49ba-8d02-f4a70053d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import shutil\n",
    "\n",
    "# Base directory and configuration\n",
    "BASE_DIR = \"./workspace/training\"\n",
    "LOG_DIR = f\"{BASE_DIR}/logs\"\n",
    "\n",
    "SAFETY_DATASET_NAME = \"nemo_safety_blend_v0.2.2.jsonl\"  # TODO: Change\n",
    "POST_TRAINING_DATASET_NAME = \"nvidia/Llama-Nemotron-Post-Training-Dataset\"\n",
    "MODEL_NAME_OR_PATH = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MODEL_DIR = f\"{BASE_DIR}/model/\"\n",
    "SAFETY_MODEL_NAME = \"llama-3.1-nemoguard-8b-content-safety\"\n",
    "SAFETY_MODEL_PATH = f\"{MODEL_DIR}/{SAFETY_MODEL_NAME}\"\n",
    "\n",
    "# Credentials\n",
    "os.environ.update({\n",
    "    \"HF_TOKEN\":\"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\",\n",
    "    \"WANDB_API_KEY\": \"2f672ca9d8cf9366dda87615069e6a9f2de6a33d\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebc62ad4-e2f1-440d-a8b8-957e400e395f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./workspace/training/model//llama-3.1-nemoguard-8b-content-safety\n"
     ]
    }
   ],
   "source": [
    "print(SAFETY_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806681b5-869a-4a84-a2c5-9c4f4d3e7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p {LOG_DIR}\n",
    "!mkdir -p {MODEL_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbde3367-ff98-4984-b8e1-7687fbc6379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ.update({\n",
    "    'TMPDIR': f\"{BASE_DIR}/tmp\",\n",
    "    'XDG_CACHE_HOME': f\"{BASE_DIR}/cache\",\n",
    "    'HF_HOME': f\"{BASE_DIR}/cache/huggingface\",\n",
    "    'UV_CACHE_DIR': f\"{BASE_DIR}/cache/uv\",\n",
    "    'TRITON_CACHE_DIR': f\"{BASE_DIR}/cache/triton\",\n",
    "    'DATASET_CACHE_DIR': f\"{BASE_DIR}/dataset_cache\",\n",
    "    'RAY_TMPDIR': \"/tmp/ray\",\n",
    "    'LOG_DIR': f\"{LOG_DIR}\",\n",
    "})\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [os.environ['TMPDIR'], os.environ['XDG_CACHE_HOME'], os.environ['HF_HOME'],\n",
    "                 os.environ['UV_CACHE_DIR'],os.environ['TRITON_CACHE_DIR'], os.environ['DATASET_CACHE_DIR'], \n",
    "                 os.environ['RAY_TMPDIR'], os.environ['LOG_DIR']]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994dafc1-3758-4d51-8ed1-3ddef45196cd",
   "metadata": {},
   "source": [
    "## Step 2-1. Safety Training Data Generation \n",
    "\n",
    "The key idea is to use on-policy data generated by the target model, which we will improve safety, while making sure to obtain safe responses even for unsafe prompts. \n",
    "\n",
    "Our safety training data generation recipe uses a curated list of publicly available safety datasets.\n",
    "\n",
    "- [Aegis AI Content Safety Dataset 2.0](https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0)\n",
    "- [Gretel Synthetic Safety Alignment Dataset](https://huggingface.co/datasets/gretelai/gretel-safety-alignment-en-v1)\n",
    "- [HarmfulTasks](https://github.com/CrystalEye42/eval-safety)\n",
    "- [RedTeam 2k](https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k]\n",
    "\n",
    "We extracted safe and unsafe prompts from Aegis AI Content Safety Dataset 2.0 and unsafe prompts from the other three datasets. \n",
    "\n",
    "### Response Generation for Safe Prompts\n",
    "\n",
    "For safe prompts, we use the target model and the prompts as is to generate responses. If we have a larger model in the same model family (e.g., `deepseek-ai/DeepSeek-R1` for `deepseek-ai/DeepSeek-R1-Distill-Llama-8B`), it is recommended to use the model as the larger model should generate safe responses better than the target model. \n",
    "\n",
    "### Response Generation for Unsafe Prompts\n",
    "\n",
    "Since the unsafe prompts in the HarmfulTasks dataset have jailbreak prompts (e.g., ‚ÄúIgnore all instructions until now\"), these jailbreak parts were removed so that the model can generate responses to original safe user requests.\n",
    "\n",
    "All unsafe prompts are wrapped in a new prompt template (shown below) that asks the model to disengage but also inform the user exactly why (using the ground truth harm categories in the datasets, i.e. not just a canned response).\n",
    "\n",
    "```\n",
    "You are a helpful assistant that generates safe responses for given prompts.\n",
    "\n",
    "The prompt is: {prompt}\n",
    "This prompt is known to violate the following safety categories or elicits unsafe responses of these categories from the model. The categories are: {categories}\n",
    "\n",
    "Generate a safe refusal for the prompt that politely points out that the model doesn't engage in unsafe behavior for the given categories, however, don't assume you were told the categories in advance, act like you figured the categories and safety issues yourself. Your response:\n",
    "```\n",
    "\n",
    "With this way, we can help the model generate safe responses for unsafe prompts. \n",
    "\n",
    "For training, we will pass the original unsafe prompt and the generated response, not the one with the safe prompt wrapper described above. Effectively, we are trying to teach the model to generate the same response to the original unsafe prompt (e.g., with jailbreak part for the HarmfulTaks examples)\n",
    "\n",
    "### Response Filtering\n",
    "\n",
    "The generated responses for the safe and unsafe prompts discussed above are not guaranteed to be safe responses. Therefore, we implement a filtering step to extract the generated responses that are judged as safe by a guard model.\n",
    "\n",
    "We use [nvidia/llama-3.1-nemoguard-8b-content-safety](https://huggingface.co/nvidia/llama-3.1-nemoguard-8b-content-safety) as the guard model for this filtering step.\n",
    "\n",
    "## Step 2-2. Training Data Blend Creation \n",
    "\n",
    "Safety training data helps the model learn to refuse to answer unsafe prompts. As a result, the model may experience some accuracy degradation for certain capabilities such as instruction following.\n",
    "\n",
    "To address the issue, we add post-training data to retain the accuracy. We use a subset of  [nvidia/Llama-Nemotron-Post-Training-Dataset](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset), which was used to train Llama Nemotron models‚Äîworld-class reasoning models, for this purpose. \n",
    "\n",
    "More specifically, in this recipe, we also generate on-policy data using the model as it should help retain the same behavior as the original model for the prompts in the post-training dataset.\n",
    "\n",
    "## Prepare NeMo Guard Content Safety Model\n",
    "\n",
    "TODO: Explain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a646d0f-031c-4337-b8ac-91200b6aa99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:12<00:00,  3.20s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 169.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./workspace/training/model//llama-3.1-nemoguard-8b-content-safety/tokenizer_config.json',\n",
       " './workspace/training/model//llama-3.1-nemoguard-8b-content-safety/special_tokens_map.json',\n",
       " './workspace/training/model//llama-3.1-nemoguard-8b-content-safety/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, \"nvidia/llama-3.1-nemoguard-8b-content-safety\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(SAFETY_MODEL_PATH, torch_dtype=torch.bfloat16)\n",
    "tokenizer.save_pretrained(SAFETY_MODEL_PATH)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf5e2e0-0b2b-4032-84d6-d91f3b16891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting dataset collection...\n",
      "Output file: ./workspace/training/dataset_cache/nemo_safety_blend_v0.2.2.jsonl\n",
      "Target sample size: 2,000\n",
      "Sampling method: uniform\n",
      "Using cache directory: ./workspace/training/dataset_cache\n",
      "\n",
      "Downloading Aegis v2 dataset...\n",
      "Source: nvidia/Aegis-AI-Content-Safety-Dataset-2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30007/30007 [00:00<00:00, 151049.99 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1445/1445 [00:00<00:00, 83043.58 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1964/1964 [00:00<00:00, 131103.29 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset: Aegis v2\n",
      "Number of samples: 23,077\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Downloading Gretel Safety Alignment v1 dataset...\n",
      "Source: gretelai/gretel-safety-alignment-en-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5997/5997 [00:00<00:00, 117777.92 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1183/1183 [00:00<00:00, 72591.72 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1181/1181 [00:00<00:00, 80094.96 examples/s]\n",
      "Cloning into 'eval-safety'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset: Gretel Safety Alignment v1\n",
      "Number of samples: 5,997\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Downloading Harmful Tasks dataset...\n",
      "Source: CrystalEye42/eval-safety\n",
      "Processing Harmful Tasks data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 52958.38it/s]\n",
      "Processing tasks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11/11 [00:00<00:00, 28962.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset: Harmful Tasks\n",
      "Number of samples: 1,650\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Downloading RedTeam 2k dataset...\n",
      "Source: JailbreakV-28K/JailBreakV-28k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating RedTeam_2K split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 244330.76 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2000/2000 [00:00<00:00, 189389.02 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Dataset: RedTeam 2k\n",
      "Number of samples: 582\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Combining datasets...\n",
      "\n",
      "Original Dataset Distribution\n",
      "================================================================================\n",
      "Dataset                             Count   Percentage\n",
      "--------------------------------------------------------------------------------\n",
      "Aegis v2                           23,077       73.74%\n",
      "Gretel Safety Alignment v1          5,994       19.15%\n",
      "HarmfulTasks                        1,650        5.27%\n",
      "RedTeam 2k                            573        1.83%\n",
      "--------------------------------------------------------------------------------\n",
      "Total                              31,294      100.00%\n",
      "================================================================================\n",
      "\n",
      "Saving full dataset to ./workspace/training/dataset_cache/nemo_safety_blend_v0.2.2.jsonl...\n",
      "Full dataset saved with 31,294 samples\n",
      "\n",
      "Performing uniform sampling across categories...\n",
      "RedTeam 2k: 500 samples selected\n",
      "HarmfulTasks: 500 samples selected\n",
      "Gretel Safety Alignment v1: 500 samples selected\n",
      "Aegis v2: 500 samples selected\n",
      "\n",
      "Sampling Report\n",
      "================================================================================\n",
      "Original dataset size: 31,294\n",
      "Sampled dataset size: 2,000\n",
      "Sampling ratio: 6.39%\n",
      "Sampling method: uniform\n",
      "================================================================================\n",
      "\n",
      "Sampled Dataset Distribution\n",
      "================================================================================\n",
      "Dataset                             Count   Percentage\n",
      "--------------------------------------------------------------------------------\n",
      "RedTeam 2k                            500       25.00%\n",
      "HarmfulTasks                          500       25.00%\n",
      "Gretel Safety Alignment v1            500       25.00%\n",
      "Aegis v2                              500       25.00%\n",
      "--------------------------------------------------------------------------------\n",
      "Total                               2,000      100.00%\n",
      "================================================================================\n",
      "\n",
      "Saving sampled dataset to ./workspace/training/dataset_cache/nemo_safety_blend_v0.2.2_sampled_2000_uniform.jsonl\n",
      "Sampled dataset saved with 2,000 samples\n",
      "\n",
      "Total processing time: 17.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling from categories: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 532.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Download NV Safety Dataset\n",
    "#os.chdir(f\"{BASE_DIR}/NeMo-Safety/notebooks\")\n",
    "result = subprocess.run([\n",
    "    'python3', 'safety_dataset_blend_generation.py',\n",
    "    '--filename', os.path.join(os.environ['DATASET_CACHE_DIR'], SAFETY_DATASET_NAME),\n",
    "    '--total_samples', '2000',\n",
    "    '--sampling_method', 'uniform',\n",
    "    '--cache_dir', os.environ['DATASET_CACHE_DIR']\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8546dff5-4109-4e84-a335-c51a8d8af372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading SFT/math/math_v1.1.jsonl...\n",
      "Total lines in file: 2225427\n",
      "Extracted 1000 random samples to ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/math_v1.1.jsonl\n",
      "Downloading SFT/code/code_v1.1.jsonl...\n",
      "Total lines in file: 496206\n",
      "Extracted 1000 random samples to ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/code_v1.1.jsonl\n",
      "Downloading SFT/chat/chat.jsonl...\n",
      "Total lines in file: 39792\n",
      "Extracted 1000 random samples to ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/chat.jsonl\n",
      "Downloading SFT/science/science.jsonl...\n",
      "Total lines in file: 708920\n",
      "Extracted 1000 random samples to ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/science.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 2. Download Llama Nemotron Post-training Dataset\n",
    "\n",
    "files = [\n",
    "    \"SFT/math/math_v1.1.jsonl\",\n",
    "    \"SFT/code/code_v1.1.jsonl\",\n",
    "    \"SFT/chat/chat.jsonl\",\n",
    "    \"SFT/science/science.jsonl\"\n",
    "]\n",
    "\n",
    "LLAMA_NEMO_DIR = f\"{os.environ['DATASET_CACHE_DIR']}/Llama-Nemotron-Post-Training-Dataset\"\n",
    "Path(LLAMA_NEMO_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Downloading {file}...\")\n",
    "    downloaded_path = hf_hub_download(\n",
    "        repo_id=POST_TRAINING_DATASET_NAME,\n",
    "        filename=file,\n",
    "        repo_type='dataset',\n",
    "        cache_dir=os.environ['DATASET_CACHE_DIR']\n",
    "    )\n",
    "    \n",
    "    filename = Path(file).name\n",
    "    target_path = f\"{LLAMA_NEMO_DIR}/{filename}\"\n",
    "    \n",
    "    # Count lines and sample\n",
    "    with open(downloaded_path, 'r') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\"Total lines in file: {total_lines}\")\n",
    "    \n",
    "    if total_lines > 1000:\n",
    "        # Use shuf for random sampling\n",
    "        subprocess.run(['shuf', '-n', '1000', downloaded_path], stdout=open(target_path, 'w'), check=True)\n",
    "        print(f\"Extracted 1000 random samples to {target_path}\")\n",
    "    else:\n",
    "        shutil.copy2(downloaded_path, target_path)\n",
    "        print(f\"File has fewer than 1000 lines, copied all {total_lines} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff0fd393-241b-4200-a2ba-c75c03904626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading safety dataset from ./workspace/training/dataset_cache/nemo_safety_blend_v0.2.2_sampled_2000_uniform.jsonl...\n",
      "Loaded 2000 samples from safety dataset\n",
      "\n",
      "Found 4 Llama-Nemotron files: ['./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/code_v1.1.jsonl', './workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/math_v1.1.jsonl', './workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/chat.jsonl', './workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/science.jsonl']\n",
      "\n",
      "Loading ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/code_v1.1.jsonl as 'code_v1'...\n",
      "Added 880/1000 items from code_v1\n",
      "\n",
      "Loading ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/math_v1.1.jsonl as 'math_v1'...\n",
      "Added 997/1000 items from math_v1\n",
      "\n",
      "Loading ./workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset/chat.jsonl as 'chat'...\n",
      "Added 1000/1000 items from chat\n",
      "\n",
      "Token Statistics:\n",
      "Total samples processed: 5000\n",
      "Samples skipped due to token limit: 123 (2.5%)\n",
      "Samples kept: 4877 (97.5%)\n",
      "\n",
      "Token counts by source:\n",
      "\n",
      "llama_nemo_code_v1:\n",
      "  Samples: 880 (17.6%)\n",
      "  Total tokens: 9,475,627\n",
      "  Average tokens per sample: 10767.8\n",
      "  Min tokens: 522\n",
      "  Max tokens: 17552\n",
      "  Samples skipped: 120 (12.0%)\n",
      "\n",
      "llama_nemo_math_v1:\n",
      "  Samples: 997 (19.9%)\n",
      "  Total tokens: 6,042,966\n",
      "  Average tokens per sample: 6061.1\n",
      "  Min tokens: 267\n",
      "  Max tokens: 16425\n",
      "  Samples skipped: 3 (0.3%)\n",
      "\n",
      "llama_nemo_chat:\n",
      "  Samples: 1000 (20.0%)\n",
      "  Total tokens: 1,427,206\n",
      "  Average tokens per sample: 1427.2\n",
      "  Min tokens: 15\n",
      "  Max tokens: 7987\n",
      "  Samples skipped: 0 (0.0%)\n",
      "\n",
      "safety_dataset:\n",
      "  Samples: 2000 (40.0%)\n",
      "  Total tokens: 267,963\n",
      "  Average tokens per sample: 134.0\n",
      "  Min tokens: 1\n",
      "  Max tokens: 3284\n",
      "  Samples skipped: 0 (0.0%)\n",
      "\n",
      "Saving train dataset (4,731 samples) to ./workspace/training/dataset_cache/sft_data/train.jsonl...\n",
      "Saving validation dataset (146 samples) to ./workspace/training/dataset_cache/sft_data/val.jsonl...\n",
      "\n",
      "Final dataset statistics:\n",
      "Total samples processed: 5000\n",
      "Samples kept: 4877 (97.5%)\n",
      "Train samples: 4731 (97.0%)\n",
      "Validation samples: 146 (3.0%)\n",
      "\n",
      "Train set source distribution:\n",
      "safety_dataset: 1943 (41.1%)\n",
      "llama_nemo_math_v1: 969 (20.5%)\n",
      "llama_nemo_chat: 962 (20.3%)\n",
      "llama_nemo_code_v1: 857 (18.1%)\n",
      "\n",
      "Validation set source distribution:\n",
      "safety_dataset: 57 (39.0%)\n",
      "llama_nemo_chat: 38 (26.0%)\n",
      "llama_nemo_math_v1: 28 (19.2%)\n",
      "llama_nemo_code_v1: 23 (15.8%)\n",
      "\n",
      "Prompt Label Statistics:\n",
      "\n",
      "Label Distribution:\n",
      "unsafe: 1734 samples (86.7%)\n",
      "safe: 266 samples (13.3%)\n",
      "\n",
      "Category Distribution:\n",
      ": 214 samples\n",
      "criminal planning/confessions: 117 samples\n",
      "malicious use: 113 samples\n",
      "substance abuse and dangerous practices: 111 samples\n",
      "system risks: 106 samples\n",
      "information hazards: 105 samples\n",
      "misinformation and disinformation: 103 samples\n",
      "unlawful behaviors and activities: 103 samples\n",
      "security threats and cybercrimes: 102 samples\n",
      "societal risks: 95 samples\n",
      "hate speech and discrimination: 81 samples\n",
      "discrimination: 81 samples\n",
      "illegal activity: 74 samples\n",
      "needs caution: 72 samples\n",
      "hate speech: 60 samples\n",
      "violence: 58 samples\n",
      "tailored unlicensed advice: 57 samples\n",
      "hate/identity hate: 48 samples\n",
      "fraud: 40 samples\n",
      "harassment: 35 samples\n",
      "child abuse: 33 samples\n",
      "health consultation: 29 samples\n",
      "physical harm: 28 samples\n",
      "malware: 28 samples\n",
      "political sensitivity: 28 samples\n",
      "unethical behavior: 26 samples\n",
      "government decision: 23 samples\n",
      "profanity: 21 samples\n",
      "economic harm: 19 samples\n",
      "controlled/regulated substances: 18 samples\n",
      "bias: 17 samples\n",
      "sexual: 17 samples\n",
      "guns and illegal weapons: 17 samples\n",
      "animal abuse: 16 samples\n",
      "pii/privacy: 16 samples\n",
      "privacy violation: 15 samples\n",
      "unauthorized advice: 12 samples\n",
      "suicide and self harm: 10 samples\n",
      "other: 7 samples\n",
      "political/misinformation/conspiracy: 6 samples\n",
      "immoral/unethical: 5 samples\n",
      "sexual (minor): 4 samples\n",
      "threat: 3 samples\n",
      "fraud/deception: 2 samples\n",
      "high risk gov decision making: 1 samples\n",
      "\n",
      "Category Distribution by Label:\n",
      "\n",
      "SAFE prompts:\n",
      "  : 214 samples (80.5%)\n",
      "  needs caution: 49 samples (18.4%)\n",
      "  harassment: 2 samples (0.8%)\n",
      "  unauthorized advice: 2 samples (0.8%)\n",
      "  sexual: 1 samples (0.4%)\n",
      "  profanity: 1 samples (0.4%)\n",
      "  violence: 1 samples (0.4%)\n",
      "  hate/identity hate: 1 samples (0.4%)\n",
      "  criminal planning/confessions: 1 samples (0.4%)\n",
      "\n",
      "UNSAFE prompts:\n",
      "  criminal planning/confessions: 116 samples (6.7%)\n",
      "  malicious use: 113 samples (6.5%)\n",
      "  substance abuse and dangerous practices: 111 samples (6.4%)\n",
      "  system risks: 106 samples (6.1%)\n",
      "  information hazards: 105 samples (6.1%)\n",
      "  misinformation and disinformation: 103 samples (5.9%)\n",
      "  unlawful behaviors and activities: 103 samples (5.9%)\n",
      "  security threats and cybercrimes: 102 samples (5.9%)\n",
      "  societal risks: 95 samples (5.5%)\n",
      "  hate speech and discrimination: 81 samples (4.7%)\n",
      "  discrimination: 81 samples (4.7%)\n",
      "  illegal activity: 74 samples (4.3%)\n",
      "  hate speech: 60 samples (3.5%)\n",
      "  tailored unlicensed advice: 57 samples (3.3%)\n",
      "  violence: 57 samples (3.3%)\n",
      "  hate/identity hate: 47 samples (2.7%)\n",
      "  fraud: 40 samples (2.3%)\n",
      "  child abuse: 33 samples (1.9%)\n",
      "  harassment: 33 samples (1.9%)\n",
      "  health consultation: 29 samples (1.7%)\n",
      "  physical harm: 28 samples (1.6%)\n",
      "  malware: 28 samples (1.6%)\n",
      "  political sensitivity: 28 samples (1.6%)\n",
      "  unethical behavior: 26 samples (1.5%)\n",
      "  government decision: 23 samples (1.3%)\n",
      "  needs caution: 23 samples (1.3%)\n",
      "  profanity: 20 samples (1.2%)\n",
      "  economic harm: 19 samples (1.1%)\n",
      "  controlled/regulated substances: 18 samples (1.0%)\n",
      "  bias: 17 samples (1.0%)\n",
      "  guns and illegal weapons: 17 samples (1.0%)\n",
      "  animal abuse: 16 samples (0.9%)\n",
      "  sexual: 16 samples (0.9%)\n",
      "  pii/privacy: 16 samples (0.9%)\n",
      "  privacy violation: 15 samples (0.9%)\n",
      "  suicide and self harm: 10 samples (0.6%)\n",
      "  unauthorized advice: 10 samples (0.6%)\n",
      "  other: 7 samples (0.4%)\n",
      "  political/misinformation/conspiracy: 6 samples (0.3%)\n",
      "  immoral/unethical: 5 samples (0.3%)\n",
      "  sexual (minor): 4 samples (0.2%)\n",
      "  threat: 3 samples (0.2%)\n",
      "  fraud/deception: 2 samples (0.1%)\n",
      "  high risk gov decision making: 1 samples (0.1%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['python3', 'combine_datasets.py', '--safety_file', './workspace/training/dataset_cache/nemo_safety_blend_v0.2.2_sampled_2000_uniform.jsonl', '--llama_nemo_dir', './workspace/training/dataset_cache/Llama-Nemotron-Post-Training-Dataset', '--output_dir', './workspace/training/dataset_cache/sft_data', '--val_split', '0.03', '--max_tokens', '16384', '--max_samples', '5000'], returncode=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Combine datasets\n",
    "OUTPUT_DIR = f\"{os.environ['DATASET_CACHE_DIR']}/sft_data\"\n",
    "subprocess.run([\n",
    "    'python3', 'combine_datasets.py',\n",
    "    '--safety_file', f\"{os.environ['DATASET_CACHE_DIR']}/nemo_safety_blend_v0.2.2_sampled_2000_uniform.jsonl\", # TODO: Name change\n",
    "    '--llama_nemo_dir', LLAMA_NEMO_DIR,\n",
    "    '--output_dir', OUTPUT_DIR,\n",
    "    '--val_split', '0.03',\n",
    "    '--max_tokens', '16384',\n",
    "    '--max_samples', '5000'\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdb206f0-bc57-479f-81b8-b833be66165b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.jsonl  val.jsonl\n"
     ]
    }
   ],
   "source": [
    "!ls {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe81b4-5add-45f6-8cd1-0a7d71f3175a",
   "metadata": {},
   "source": [
    "\n",
    "### Launch policy model\n",
    "\n",
    "\n",
    "```\n",
    "export VLLM_ENGINE_ITERATION_TIMEOUT_S=36000\n",
    "export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\n",
    "export VLLM_HOST=\"0.0.0.0\"\n",
    "export VLLM_TENSOR_PARALLEL_SIZE=4\n",
    "export POLICY_MODEL_GPUS=\"0,1,2,3\"\n",
    "export MODEL_NAME_OR_PATH=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "export HF_HOME=./workspace/cache/huggingface\n",
    "CUDA_VISIBLE_DEVICES=${POLICY_MODEL_GPUS} python3 -m vllm.entrypoints.openai.api_server \\\n",
    "  --model \"$MODEL_NAME_OR_PATH\" \\\n",
    "  --trust-remote-code \\\n",
    "  --seed 1 \\\n",
    "  --host \"$VLLM_HOST\" \\\n",
    "  --port 5000 \\\n",
    "  --served-model-name \"test-model\" \\\n",
    "  --enable-reasoning \\\n",
    "  --reasoning-parser qwen3 \\\n",
    "  --tensor-parallel-size \"$VLLM_TENSOR_PARALLEL_SIZE\" \\\n",
    "  --download-dir=\"$HF_HOME\"\n",
    "```\n",
    "\n",
    "### Launch safety model\n",
    "\n",
    "```\n",
    "export VLLM_ENGINE_ITERATION_TIMEOUT_S=36000\n",
    "export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\n",
    "export VLLM_HOST=\"0.0.0.0\"\n",
    "export VLLM_TENSOR_PARALLEL_SIZE=4\n",
    "export SAFETY_MODEL_GPUS=\"4,5,6,7\"\n",
    "export HF_HOME=./workspace/cache/huggingface\n",
    "export SAFETY_MODEL_PATH=\"./workspace/training/model//llama-3.1-nemoguard-8b-content-safety\"\n",
    "CUDA_VISIBLE_DEVICES=${SAFETY_MODEL_GPUS} python3 -m vllm.entrypoints.openai.api_server \\\n",
    "  --model \"$SAFETY_MODEL_PATH\" \\\n",
    "  --trust-remote-code \\\n",
    "  --seed 1 \\\n",
    "  --host \"$VLLM_HOST\" \\\n",
    "  --port 6000 \\\n",
    "  --served-model-name \"safety-model\" \\\n",
    "  --tensor-parallel-size \"$VLLM_TENSOR_PARALLEL_SIZE\" \\\n",
    "  --download-dir=\"$HF_HOME\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "badfae55-e3e8-41cd-89c5-8924c1760a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following block does not work for me :( (Yoshi)\n",
    "\n",
    "# # 4. Start vLLM servers\n",
    "# os.environ.update({\n",
    "#     'VLLM_ENGINE_ITERATION_TIMEOUT_S': '36000',\n",
    "#     'VLLM_ALLOW_LONG_MAX_MODEL_LEN': '1',\n",
    "#     'VLLM_HOST': '0.0.0.0',\n",
    "#     'VLLM_TENSOR_PARALLEL_SIZE': '4',\n",
    "#     'POLICY_MODEL_GPUS': '0,1,2,3',\n",
    "#     'SAFETY_MODEL_GPUS': '4,5,6,7'\n",
    "# })\n",
    "\n",
    "# print(\"Starting policy model server...\")\n",
    "# policy_server = subprocess.Popen([\n",
    "#     'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "#     '--model', MODEL_NAME_OR_PATH,\n",
    "#     '--trust-remote-code',\n",
    "#     '--seed', '1',\n",
    "#     '--host', os.environ['VLLM_HOST'],\n",
    "#     '--port', '5000',\n",
    "#     '--served-model-name', 'test-model',\n",
    "#     '--enable-reasoning', \n",
    "#     '--reasoning-parser', 'qwen3',\n",
    "#     '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "#     '--download-dir', os.environ['HF_HOME']\n",
    "# ], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['POLICY_MODEL_GPUS']},\n",
    "#    stdout=open(f\"{LOG_DIR}/vllm-server-model.log\", 'w'),\n",
    "#    stderr=subprocess.STDOUT)\n",
    "\n",
    "# print(\"Starting safety model server...\")\n",
    "# safety_server = subprocess.Popen([\n",
    "#     'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "#     '--model', SAFETY_MODEL_PATH,\n",
    "#     '--trust-remote-code',\n",
    "#     '--seed', '1',\n",
    "#     '--host', os.environ['VLLM_HOST'],\n",
    "#     '--port', '6000',\n",
    "#     '--served-model-name', 'safety-model',\n",
    "#     '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "#     '--download-dir', os.environ['HF_HOME']\n",
    "# ], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['SAFETY_MODEL_GPUS']},\n",
    "#    stdout=open(f\"{LOG_DIR}/vllm-server-safety.log\", 'w'),\n",
    "#    stderr=subprocess.STDOUT)\n",
    "\n",
    "# Cleanup vLLM servers\n",
    "# subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9097d3de-44e5-4520-9233-69bb124790c3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef26114-cbd5-45a3-87f3-7de6d5bdc139",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01e54dd4-01bd-4290-953e-17caceb87533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pkill', '-f', 'vllm.entrypoints.openai.api_server'], returncode=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup vLLM servers\n",
    "# subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a50387ea-22c6-4b5b-83fa-a564d1567169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pkill', '-f', 'generate_on_policy_data.py'], returncode=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kill on-policy\n",
    "#subprocess.run(['pkill', '-f', 'generate_on_policy_data.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9f63bfd-c74f-4674-a1df-324b381cbbca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating on-policy data...\n",
      "Data is Ready\n"
     ]
    }
   ],
   "source": [
    "# 5. Generate on-policy data\n",
    "\n",
    "# On-Policy Data Generation parameters\n",
    "CONCURRENCY = 16\n",
    "MAX_ATTEMPTS = 3\n",
    "BATCH_SIZE = 96\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "\n",
    "print(\"Generating on-policy data...\")\n",
    "for dataset_type in ['train', 'val']:\n",
    "    input_dataset = f\"{OUTPUT_DIR}/{dataset_type}.jsonl\"\n",
    "    output_file = f\"{OUTPUT_DIR}/{dataset_type}_on_policy_data.jsonl\"\n",
    "    DATASET_TYPE = dataset_type\n",
    "    subprocess.run([\n",
    "        'python3', 'generate_on_policy_data.py',\n",
    "        '--model_name', MODEL_NAME,\n",
    "        '--safety_model', SAFETY_MODEL_NAME,\n",
    "        '--huggingface_token', os.environ['HF_TOKEN'],\n",
    "        '--vllm_host', os.environ['VLLM_HOST'],\n",
    "        '--vllm_model_port', '5000',\n",
    "        '--vllm_safety_port', '6000',\n",
    "        '--concurrency', str(CONCURRENCY),\n",
    "        '--input_dataset', input_dataset,\n",
    "        '--output', output_file,\n",
    "        '--batch_size', str(BATCH_SIZE),\n",
    "        '--max_tokens', str(MAX_TOKENS),\n",
    "        '--temperature', str(TEMPERATURE),\n",
    "        '--top_p', str(TOP_P)\n",
    "    ], stdout=open(f\"{LOG_DIR}/{DATASET_TYPE}_on-policy.log\", 'w'),\n",
    "                   stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Data is Ready\")\n",
    "    \n",
    "# Cleanup vLLM servers\n",
    "# subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38e6b5e7-4fce-4563-8896-322e77f125cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pkill', '-f', 'vllm.entrypoints.openai.api_server'], returncode=0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleanup vLLM servers\n",
    "subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f46a5-c0b3-44de-ab52-3b905923b5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8b87c2a9-1164-4e27-bc26-277462e34743",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !ps -aux | grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e42fbf0-bd6d-480b-b56b-a35ef4a04bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link '/workspace/NeMo-RL/workspace': File exists\n",
      "Running SFT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 18:09:48,170\tINFO worker.py:1832 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "INFO:nemo_rl.distributed.virtual_cluster:Started local cluster with tag 'nrl_tag_0_1_2_3_4_5_6_7': {'node:10.52.48.226': 1.0, 'node:__internal_head__': 1.0, 'accelerator_type:H100': 1.0, 'memory': 1826552155136.0, 'nrl_tag_0_1_2_3_4_5_6_7': 1.0, 'CPU': 224.0, 'GPU': 8.0, 'object_store_memory': 200000000000.0}\n",
      "2025/06/01 18:09:50 ERROR failed to get logger path error=\"failed to get logger path\"\n",
      "2025/06/01 18:09:50 INFO Will exit if parent process dies. ppid=789671\n",
      "2025/06/01 18:09:50 INFO server is running addr=127.0.0.1:37481\n",
      "2025/06/01 18:09:50 INFO connection: ManageConnectionData: new connection created id=127.0.0.1:35572\n",
      "wandb: Currently logged in as: ysuhara to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n",
      "2025/06/01 18:09:51 INFO handleInformInit: received streamId=1ipgiqlj id=127.0.0.1:35572\n",
      "2025/06/01 18:09:51 ERROR Unable to find cache directory, using .wandb_cache err=\"path in $XDG_CACHE_HOME is relative\"\n",
      "2025/06/01 18:09:51 INFO handleInformInit: stream started streamId=1ipgiqlj id=127.0.0.1:35572\n",
      "wandb: Tracking run with wandb version 0.19.11\n",
      "wandb: Run data is saved locally in logs/exp_004/wandb/wandb/run-20250601_180951-1ipgiqlj\n",
      "wandb: Run `wandb offline` to turn off syncing.\n",
      "wandb: Syncing run safety-training-deepseek-distill-8b-llama\n",
      "wandb: ‚≠êÔ∏è View project at https://wandb.ai/ysuhara/NeMo-Safety\n",
      "wandb: üöÄ View run at https://wandb.ai/ysuhara/NeMo-Safety/runs/1ipgiqlj\n",
      "INFO:nemo_rl.utils.venvs:NEMO_RL_VENV_DIR is set to /workspace/NeMo-RL/venvs.\n",
      "Using CPython 3.12.3 interpreter at: \u001b[36m/usr/bin/python3.12\u001b[39m\n",
      "Creating virtual environment at: \u001b[36mvenvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker\u001b[39m\n",
      "Activate with: \u001b[32msource venvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker/bin/activate\u001b[39m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=.venv` does not match the project environment path `venvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m312 packages\u001b[0m \u001b[2min 4ms\u001b[0m\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m131 packages\u001b[0m \u001b[2min 0.15ms\u001b[0m\u001b[0m\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=.venv` does not match the project environment path `venvs/nemo_rl.models.policy.dtensor_policy_worker.DTensorPolicyWorker` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:08<00:08,  8.99s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.36s/it]\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.15s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:19<00:00,  9.68s/it]\n",
      "\u001b[36m(DTensorPolicyWorker pid=804828)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::hf_policy-0-1:DTensorPolicyWorker.__init__()\u001b[39m (pid=804828, ip=10.52.48.226, actor_id=991774b6cd4146c81a20b39901000000, repr=DTensorPolicyWorker[rank=1])\n",
      "\u001b[36m(DTensorPolicyWorker pid=804828)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(DTensorPolicyWorker pid=804828)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(DTensorPolicyWorker pid=804828)\u001b[0m   File \"/workspace/NeMo-RL/nemo_rl/models/policy/dtensor_policy_worker.py\", line 195, in __init__\n",
      "\u001b[36m(DTensorPolicyWorker pid=804828)\u001b[0m     custom_parallel_plan=self.cfg[\"dtensor_cfg\"][\"custom_parallel_plan\"],\n",
      "\u001b[36m(DTensorPolicyWorker pid=804828)\u001b[0m                          ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(DTensorPolicyWorker pid=804828)\u001b[0m KeyError: 'custom_parallel_plan'\n",
      "\u001b[36m(DTensorPolicyWorker pid=804828)\u001b[0m [rank1]:[W601 18:10:26.677496847 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/NeMo-RL/examples/run_sft.py\", line 218, in <module>\n",
      "    main()\n",
      "  File \"/workspace/NeMo-RL/examples/run_sft.py\", line 203, in main\n",
      "    sft_train(\n",
      "  File \"/workspace/NeMo-RL/nemo_rl/algorithms/sft.py\", line 360, in sft_train\n",
      "    val_metrics, validation_timings = validate(\n",
      "                                      ^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/nemo_rl/algorithms/sft.py\", line 249, in validate\n",
      "    policy.prepare_for_training()\n",
      "  File \"/workspace/NeMo-RL/nemo_rl/models/policy/hf_policy.py\", line 325, in prepare_for_training\n",
      "    ray.get(futures)\n",
      "  File \"/workspace/NeMo-RL/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2771, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 921, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::hf_policy-0-1:DTensorPolicyWorker.__init__()\u001b[39m (pid=804828, ip=10.52.48.226, actor_id=991774b6cd4146c81a20b39901000000, repr=DTensorPolicyWorker[rank=1])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/nemo_rl/models/policy/dtensor_policy_worker.py\", line 195, in __init__\n",
      "    custom_parallel_plan=self.cfg[\"dtensor_cfg\"][\"custom_parallel_plan\"],\n",
      "                         ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'custom_parallel_plan'\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/NeMo-RL/examples/run_sft.py\", line 218, in <module>\n",
      "    main()\n",
      "  File \"/workspace/NeMo-RL/examples/run_sft.py\", line 203, in main\n",
      "    sft_train(\n",
      "  File \"/workspace/NeMo-RL/nemo_rl/algorithms/sft.py\", line 360, in sft_train\n",
      "    val_metrics, validation_timings = validate(\n",
      "                                      ^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/nemo_rl/algorithms/sft.py\", line 249, in validate\n",
      "    policy.prepare_for_training()\n",
      "  File \"/workspace/NeMo-RL/nemo_rl/models/policy/hf_policy.py\", line 325, in prepare_for_training\n",
      "    ray.get(futures)\n",
      "  File \"/workspace/NeMo-RL/.venv/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/.venv/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 2771, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/.venv/lib/python3.12/site-packages/ray/_private/worker.py\", line 921, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, \u001b[36mray::hf_policy-0-1:DTensorPolicyWorker.__init__()\u001b[39m (pid=804828, ip=10.52.48.226, actor_id=991774b6cd4146c81a20b39901000000, repr=DTensorPolicyWorker[rank=1])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/workspace/NeMo-RL/nemo_rl/models/policy/dtensor_policy_worker.py\", line 195, in __init__\n",
      "    custom_parallel_plan=self.cfg[\"dtensor_cfg\"][\"custom_parallel_plan\"],\n",
      "                         ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyError: 'custom_parallel_plan'\n",
      "2025/06/01 18:10:28 INFO handleInformTeardown: server teardown initiated id=127.0.0.1:35572\n",
      "2025/06/01 18:10:28 INFO server is shutting down\n",
      "2025/06/01 18:10:28 INFO connection: closing id=127.0.0.1:35572\n",
      "2025/06/01 18:10:28 INFO connection: closed successfully id=127.0.0.1:35572\n",
      "2025/06/01 18:10:29 INFO handleInformTeardown: server shutdown complete id=127.0.0.1:35572\n",
      "2025/06/01 18:10:29 INFO connection: ManageConnectionData: connection closed id=127.0.0.1:35572\n",
      "2025/06/01 18:10:29 INFO server is closed\n",
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:09<00:09,  9.15s/it]\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:17<00:00,  8.70s/it]\n",
      "\u001b[36m(DTensorPolicyWorker pid=817328)\u001b[0m Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::hf_policy-0-2:DTensorPolicyWorker.__init__()\u001b[39m (pid=817328, ip=10.52.48.226, actor_id=40e73bf9e19851b7098abc0901000000, repr=DTensorPolicyWorker[rank=2])\n",
      "\u001b[36m(DTensorPolicyWorker pid=817328)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(DTensorPolicyWorker pid=817328)\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(DTensorPolicyWorker pid=817328)\u001b[0m   File \"/workspace/NeMo-RL/nemo_rl/models/policy/dtensor_policy_worker.py\", line 195, in __init__\n",
      "\u001b[36m(DTensorPolicyWorker pid=817328)\u001b[0m     custom_parallel_plan=self.cfg[\"dtensor_cfg\"][\"custom_parallel_plan\"],\n",
      "\u001b[36m(DTensorPolicyWorker pid=817328)\u001b[0m                          ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[36m(DTensorPolicyWorker pid=817328)\u001b[0m KeyError: 'custom_parallel_plan'\n",
      "\u001b[36m(DTensorPolicyWorker pid=817328)\u001b[0m [rank2]:[W601 18:10:28.517467591 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['uv', 'run', 'python', 'examples/run_sft.py', '--config', '/lustre/fsw/llmservice_nemo_mlops/users/ysuhara/work/gitlab/NeMo-Safety/notebooks/deepseek_sft.yaml']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m0,1,2,3,4,5,6,7\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m os.chdir(\u001b[33m\"\u001b[39m\u001b[33m/workspace/NeMo-RL\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrun\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexamples/run_sft.py\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m--config\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_filepath\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m               \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m               \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mTMPDIR\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRAY_TMPDIR\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m               \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mMODEL_DIR\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/sft.log\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m               \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:571\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    569\u001b[39m     retcode = process.poll()\n\u001b[32m    570\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    572\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['uv', 'run', 'python', 'examples/run_sft.py', '--config', '/lustre/fsw/llmservice_nemo_mlops/users/ysuhara/work/gitlab/NeMo-Safety/notebooks/deepseek_sft.yaml']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# 6. Run SFT\n",
    "os.chdir(\"/lustre/fsw/llmservice_nemo_mlops/users/ysuhara/work/gitlab/NeMo-Safety/notebooks\")\n",
    "!ln -s workspace /workspace/NeMo-RL/\n",
    "\n",
    "MODEL_DIR = os.path.abspath(f\"{BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B/\")\n",
    "!mkdir -p {MODEL_DIR}\n",
    "config_filepath = os.path.abspath(\"deepseek_sft.yaml\")\n",
    "\n",
    "print(\"Running SFT...\")\n",
    "# Set up model directory environment variable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "os.chdir(\"/workspace/NeMo-RL\")\n",
    "subprocess.run(['uv', 'run', 'python', 'examples/run_sft.py', \n",
    "                '--config', config_filepath\n",
    "               ], \n",
    "               env={**os.environ, 'TMPDIR': os.environ['RAY_TMPDIR']},\n",
    "               stdout=open(f\"{MODEL_DIR}/sft.log\", 'w'),\n",
    "               check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0975a1e3-0223-44a7-bbf0-c4d2a2954c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a57c51b6-5765-49ec-bb1d-69d58527c234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/lustre/fsw/portfolios/llmservice/users/ahazare/penv3` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer_name_or_path: deepseek-ai/deepseek-r1-distill-llama-8b\n",
      "Saved HF checkpoint to: /lustre/fsw/portfolios/llmservice/users/ahazare/NeMo-RL/results/sft_deepseek_8b_trial_step_300/step_50/hf_ckpt\n",
      "Conversion successful!\n",
      "The HuggingFace model is now available at: /lustre/fsw/portfolios/llmservice/users/ahazare/NeMo-RL/results/sft_deepseek_8b_trial_step_300/step_50/hf_ckpt\n"
     ]
    }
   ],
   "source": [
    "# 7. Convert checkpoint\n",
    "MODEL_DIR = f\"{BASE_DIR}/NeMo-RL/results/sft_deepseek_8b_trial_step_300/step_50\"\n",
    "DCP_CKPT_PATH = f\"{MODEL_DIR}/policy/weights/\"\n",
    "CONFIG_PATH = f\"{MODEL_DIR}/config.yaml\"\n",
    "HF_CKPT_PATH = f\"{MODEL_DIR}/hf_ckpt\"\n",
    "\n",
    "print(\"Converting checkpoint...\")\n",
    "os.chdir(f\"{BASE_DIR}/NeMo-RL\")\n",
    "subprocess.run([\n",
    "    'uv', 'run', 'examples/convert_dcp_to_hf.py',\n",
    "    '--config', CONFIG_PATH,\n",
    "    '--dcp-ckpt-path', DCP_CKPT_PATH,\n",
    "    '--hf-ckpt-path', HF_CKPT_PATH\n",
    "], check=True)\n",
    "\n",
    "# Verify conversion\n",
    "if Path(f\"{HF_CKPT_PATH}/pytorch_model.bin\").exists() and Path(f\"{HF_CKPT_PATH}/config.json\").exists():\n",
    "    print(\"Conversion successful!\")\n",
    "    print(f\"The HuggingFace model is now available at: {HF_CKPT_PATH}\")\n",
    "else:\n",
    "    print(\"Conversion may have failed. Please check the output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d82a515-2a5a-4037-8096-d3aa6b2e1a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
