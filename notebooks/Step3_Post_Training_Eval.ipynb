{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed4bbfc",
   "metadata": {},
   "source": [
    "# Notebook 3: Safety and Accuracy Evaluation for the Safety Post-trained Model\n",
    "\n",
    "In this notebook, we will rerun the same set of evaluations for the model and see how the safety scores on **Content Safety** and **Product Security** have improved. \n",
    "Since it's the second time, we'll skip the detailed explanation of each dataset but focus on collecting scores for them.\n",
    "\n",
    "### Before You Begin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42607cdb-1d5e-4c1b-81a5-6734e2862450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!pip install ipywidgets > {os.devnull} 2>&1\n",
    "\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import yaml\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "import pandas as pd\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Loading environment variables from .env\")\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "if os.environ.get(\"HF_TOKEN\", None) is None or os.environ.get(\"NVIDIA_API_KEY\", None) is None:\n",
    "    raise ValueError(\"HF_TOKEN and NVIDIA_API_KEY must be set.\")\n",
    "print(\"✅ HF_TOKEN and NVIDIA_API_TOKEN found\")\n",
    "\n",
    "if os.environ.get(\"RUN_FULL_EVAL\", None) is None:\n",
    "    print(\"RUN_FULL_EVAL is not configured. Use RUN_FULL_EVAL=0.\")\n",
    "    os.environ[\"RUN_FULL_EVAL\"] = \"0\"\n",
    "else:       \n",
    "    print(f\"RUN_FULL_EVAL found: RUN_FULL_EVA={os.environ['RUN_FULL_EVAL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf0864",
   "metadata": {},
   "source": [
    "Specify paths for data, evaluation results, and the base model to evaluate.\n",
    "\n",
    "```text\n",
    "workspace\n",
    "├── dataset\n",
    "│   └── aegis_v2\n",
    "└── results\n",
    "    └── DeepSeek-R1-Distill-Llama-8B-Safety-Trained\n",
    "        ├── accuracy-evals\n",
    "        │   ├── aa-math-500\n",
    "        │   ├── gpqa-diamond\n",
    "        │   └── ifeval\n",
    "        ├── content-safety-evals\n",
    "        │   ├── aegis_v2\n",
    "        │   └── wildguard\n",
    "        ├── logs\n",
    "        └── security-evals\n",
    "            └── garak\n",
    "                ├── configs\n",
    "                ├── logs\n",
    "                └── reports\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dace57a-3941-402e-bcc4-68cc5637d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./workspace/\"\n",
    "DATASET_DIR = f\"{BASE_DIR}/dataset/\"\n",
    "ORIGINAL_MODEL_TAG_NAME = \"DeepSeek-R1-Distill-Llama-8B\"\n",
    "#TRAINED_MODEL_PATH = f\"{BASE_DIR}/training/model/DeepSeek-R1-Distill-Llama-8B-Safety-Trained\"\n",
    "TRAINED_MODEL_PATH = f\"{BASE_DIR}/training/results/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B-Safety-Trained\"\n",
    "MODEL_NAME_OR_PATH = TRAINED_MODEL_PATH\n",
    "MODEL_TAG_NAME = TRAINED_MODEL_PATH.split(\"/\")[-1]\n",
    "MODEL_OUTPUT_DIR = f\"{BASE_DIR}/results/{MODEL_TAG_NAME}/\"\n",
    "LOG_DIR = f\"{MODEL_OUTPUT_DIR}/logs/\"\n",
    "\n",
    "# * Dataset\n",
    "NEMOGURD_MODEL_PATH = f\"{BASE_DIR}/model/llama-3.1-nemoguard-8b-content-safety\"\n",
    "AEGIS_V2_TEST_DIR = f\"{DATASET_DIR}/aegis_v2\"\n",
    "\n",
    "# * Content Safety benchmark\n",
    "CONTENT_SAFETY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/content-safety-evals\"\n",
    "AEGIS_V2_RESULTS_DIR = f\"{CONTENT_SAFETY_RESULTS_DIR}/aegis_v2\"\n",
    "WILDGUARD_RESULTS_DIR = f\"{CONTENT_SAFETY_RESULTS_DIR}/wildguard\"\n",
    "\n",
    "# * Security benchmark\n",
    "SECURITY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/security-evals\"\n",
    "GARAK_RESULTS_DIR = f\"{SECURITY_RESULTS_DIR}/garak\"\n",
    "GARAK_CONFIG_DIR = f\"{GARAK_RESULTS_DIR}/configs\"\n",
    "GARAK_LOG_DIR = f\"{GARAK_RESULTS_DIR}/logs\"\n",
    "GARAK_REPORT_DIR = f\"{GARAK_RESULTS_DIR}/reports\"\n",
    "\n",
    "# * Accuracy benchmark\n",
    "ACCURACY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/accuracy-evals\"\n",
    "GPQA_DIAMOND_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/gpqa-diamond\"\n",
    "AA_MATH_500_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/aa-math-500\"\n",
    "IFEVAL_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/ifeval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b819902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories to store logs and results\n",
    "for path in [LOG_DIR, AEGIS_V2_TEST_DIR, AEGIS_V2_RESULTS_DIR, WILDGUARD_RESULTS_DIR,\n",
    "             GARAK_RESULTS_DIR, GARAK_CONFIG_DIR, GARAK_LOG_DIR, GARAK_REPORT_DIR,\n",
    "             GPQA_DIAMOND_RESULTS_DIR, AA_MATH_500_RESULTS_DIR, IFEVAL_RESULTS_DIR]:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd2311-1b5e-4b0a-a597-f9ef166ae8ab",
   "metadata": {},
   "source": [
    "Specify credentials and paths in environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fc6a2c-b364-4651-8df0-8f263ee4af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "os.environ.update({\n",
    "    \"MY_API_KEY\":\"empty\", # Dummy API key for self-hosted vLLM servers\n",
    "})\n",
    "\n",
    "# Paths\n",
    "os.environ.update({\n",
    "    'BASE_DIR': f\"{BASE_DIR}\",\n",
    "    'TMPDIR': \"/tmp\",\n",
    "    'XDG_CACHE_HOME': f\"{BASE_DIR}/cache\",\n",
    "    'HF_HOME': f\"{BASE_DIR}/cache/huggingface\",\n",
    "    'UV_CACHE_DIR': f\"{BASE_DIR}/cache/uv\",\n",
    "    'TRITON_CACHE_DIR': f\"{BASE_DIR}/cache/triton\",\n",
    "    'DATASET_CACHE_DIR': f\"{BASE_DIR}/dataset_cache\",\n",
    "    'RAY_TMPDIR': \"/tmp/ray\",\n",
    "    'LOG_DIR': f\"{LOG_DIR}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf3324-43f5-48a8-93f8-ee45cdc4220e",
   "metadata": {},
   "source": [
    "## Serve the Safety Trained Model with vLLM\n",
    "\n",
    "Start a locally-running vLLM server to serve the safety trained model.\n",
    "\n",
    "Alternatively, you can use the following comamnd and run it on a terminal.\n",
    "\n",
    "```\n",
    "export POLICY_MODEL_GPUS=\"0,1,2,3\"\n",
    "export MODEL_NAME_OR_PATH=\"./workspace/training/model/DeepSeek-R1-Distill-Llama-8B-Safety-Trained\"\n",
    "export VLLM_TENSOR_PARALLEL_SIZE=4\n",
    "export HF_HOME=./workspace/cache/huggingface\n",
    "export LOG_DIR=./workspace/results/DeepSeek-R1-Distill-Llama-8B-Safety-Trained/logs\n",
    "CUDA_VISIBLE_DEVICES=${POLICY_MODEL_GPUS} python3 -m vllm.entrypoints.openai.api_server \\\n",
    "  --model \"$MODEL_NAME_OR_PATH\" \\\n",
    "  --trust-remote-code \\\n",
    "  --seed 1 \\\n",
    "  --host \"$VLLM_HOST\" \\\n",
    "  --port 5000 \\\n",
    "  --served-model-name \"test-model\" \\\n",
    "  --enable-reasoning \\\n",
    "  --reasoning-parser qwen3 \\\n",
    "  --tensor-parallel-size \"$VLLM_TENSOR_PARALLEL_SIZE\" \\\n",
    "  --download-dir=\"$HF_HOME\"\n",
    "```\n",
    "\n",
    "After you start the server, run each of the evaluation tools against the safety trained model to establish a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca15ed7-8a04-4890-b4a6-1c350dea2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VLLM Host \n",
    "os.environ.update({\n",
    "    'VLLM_ENGINE_ITERATION_TIMEOUT_S': '36000',\n",
    "    'VLLM_ALLOW_LONG_MAX_MODEL_LEN': '1',\n",
    "    'VLLM_HOST': '0.0.0.0',\n",
    "    'VLLM_TENSOR_PARALLEL_SIZE': '4',\n",
    "    'POLICY_MODEL_GPUS': '0,1,2,3',\n",
    "})\n",
    "\n",
    "print(\"Starting policy model server...\")\n",
    "policy_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', MODEL_NAME_OR_PATH,\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '5000',\n",
    "    '--served-model-name', 'test-model',\n",
    "    '--enable-reasoning', \n",
    "    '--reasoning-parser', 'qwen3',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'VLLM_USE_V1': '0', 'CUDA_VISIBLE_DEVICES': os.environ['POLICY_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-model.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)\n",
    "\n",
    "!sleep 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063ae8b-951c-4b2a-899d-4f842316e698",
   "metadata": {},
   "source": [
    "Check if the vLLM server has been correctly launched.\n",
    "\n",
    "Run the following code block. If you see the message like below, the server is ready to use.\n",
    "\n",
    "```\n",
    "INFO:     Started server process [<pid>]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6455ec-5d3e-4900-935b-59e8a7429815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the vLLM serve is correctly launched\n",
    "!tail {LOG_DIR}/vllm-server-model.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0023b9c-b0e0-49ea-82a2-4fd55c8fce01",
   "metadata": {},
   "source": [
    "In case you'd like to shutdown (to relaunch etc.) the vLLM server, run the following code block after uncommenting the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb704e8c-e0bf-49a0-b2ea-07c43b44bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0087a27",
   "metadata": {},
   "source": [
    "### Evaluating Content Safety with Aegis 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768204ca-ecb6-4735-9676-986dbfc46349",
   "metadata": {},
   "source": [
    "### Serve the NeMo Guard Model with vLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327b9d5-5b90-4d04-a8ef-bd46ab80e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(NEMOGURD_MODEL_PATH):\n",
    "    print(\"NeMo Guard model not exist.\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16)\n",
    "    model = PeftModel.from_pretrained(base_model, \"nvidia/llama-3.1-nemoguard-8b-content-safety\")\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    merged_model.save_pretrained(NEMOGURD_MODEL_PATH, torch_dtype=torch.bfloat16)\n",
    "    tokenizer.save_pretrained(NEMOGURD_MODEL_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfee9eb-5b0f-4ed1-b881-13332edd6ee5",
   "metadata": {},
   "source": [
    "Then, you need to launch a vLLM server using the model. We also launch a vllm server for WildGuard, which will be covered later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12354d58-8e83-4450-ab04-7a58f12416d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ.update({\n",
    "    'VLLM_ENGINE_ITERATION_TIMEOUT_S': '36000',\n",
    "    'VLLM_ALLOW_LONG_MAX_MODEL_LEN': '1',\n",
    "    'VLLM_HOST': '0.0.0.0',\n",
    "    'VLLM_TENSOR_PARALLEL_SIZE': '2',\n",
    "    'VLLM_USE_V1': '0',\n",
    "    'NEMOGUARD_MODEL_GPUS': '4,5',\n",
    "    'WILDGUARD_MODEL_GPUS': '6,7',\n",
    "    'HF_HOME': './workspace/cache/huggingface',\n",
    "    'NEMOGUARD_MODEL_PATH': './workspace/model/llama-3.1-nemoguard-8b-content-safety',\n",
    "    'WILDGUARD_MODEL_PATH': 'allenai/wildguard',\n",
    "    'TMPDIR': '/tmp' \n",
    "})\n",
    "\n",
    "print(\"Starting nemo guard model server...\")\n",
    "nemo_guard_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', os.environ['NEMOGUARD_MODEL_PATH'],\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '6000',\n",
    "    '--served-model-name', 'llama-3.1-nemoguard-8b-content-safety',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['NEMOGUARD_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-nemo-guard-model.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Starting wildguard model server...\")\n",
    "wildguard_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', os.environ['WILDGUARD_MODEL_PATH'],\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '7000',\n",
    "    '--served-model-name', 'allenai/wildguard',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['WILDGUARD_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-wildguard.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)\n",
    "\n",
    "!sleep 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baefd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the servers were properly launched\n",
    "print(\"Checking vLLM server for NeMo Guard\")\n",
    "!tail {LOG_DIR}/vllm-server-nemo-guard-model.log\n",
    "print(\"=====\\n\\n\")\n",
    "print(\"Checking vLLM server for WildGuard\")\n",
    "!tail {LOG_DIR}/vllm-server-wildguard.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a9925-52d7-46af-be02-7669a591eaa8",
   "metadata": {},
   "source": [
    "In case `subprocess.Popen()` does not work, you can open terminals and run `vllm` commands to launch vllm servers. \n",
    "\n",
    "```\n",
    "export VLLM_ENGINE_ITERATION_TIMEOUT_S=36000\n",
    "export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\n",
    "export VLLM_HOST=\"0.0.0.0\"\n",
    "export VLLM_TENSOR_PARALLEL_SIZE=2\n",
    "export VLLM_USE_V1=0\n",
    "export NEMOGUARD_MODEL_GPUS=\"4,5\"\n",
    "export HF_HOME=./workspace/cache/huggingface\n",
    "export NEMOGUARD_MODEL_PATH=\"./workspace/model/llama-3.1-nemoguard-8b-content-safety\"\n",
    "CUDA_VISIBLE_DEVICES=${NEMOGUARD_MODEL_GPUS} python3 -m vllm.entrypoints.openai.api_server \\\n",
    "  --model \"$NEMOGUARD_MODEL_PATH\" \\\n",
    "  --trust-remote-code \\\n",
    "  --seed 1 \\\n",
    "  --host \"$VLLM_HOST\" \\\n",
    "  --port 6000 \\\n",
    "  --served-model-name \"llama-3.1-nemoguard-8b-content-safety\" \\\n",
    "  --tensor-parallel-size \"$VLLM_TENSOR_PARALLEL_SIZE\" \\\n",
    "  --download-dir=\"$HF_HOME\"\n",
    "\n",
    "export HF_TOKEN=\"<Your HF Token>\"\n",
    "export VLLM_ENGINE_ITERATION_TIMEOUT_S=36000\n",
    "export VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\n",
    "export VLLM_HOST=\"0.0.0.0\"\n",
    "export VLLM_TENSOR_PARALLEL_SIZE=2\n",
    "export VLLM_USE_V1=0\n",
    "export HF_HOME=./workspace/cache/huggingface\n",
    "export WILDGUARD_MODEL_GPUS=\"6,7\"\n",
    "export WILDGUARD_MODEL_PATH=\"allenai/wildguard\"\n",
    "CUDA_VISIBLE_DEVICES=${WILDGUARD_MODEL_GPUS} python3 -m vllm.entrypoints.openai.api_server \\\n",
    "  --model \"$WILDGUARD_MODEL_PATH\" \\\n",
    "  --trust-remote-code \\\n",
    "  --seed 1 \\\n",
    "  --host \"$VLLM_HOST\" \\\n",
    "  --port 7000 \\\n",
    "  --served-model-name \"allenai/wildguard\" \\\n",
    "  --tensor-parallel-size \"$VLLM_TENSOR_PARALLEL_SIZE\" \\\n",
    "  --download-dir=\"$HF_HOME\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e71853-21ef-4a54-8b25-04b35396fd74",
   "metadata": {},
   "source": [
    "After you start the server, run each of the evaluation tools against the base model to establish a performance baseline.\n",
    "\n",
    "The evaluation requires approximately 30 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c85b6-932f-4bcd-b156-c74e880b50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"1\":\n",
    "    AEGIS_V2_SAMPLE_SIZE_CONFIG = \"\"    \n",
    "else:\n",
    "    AEGIS_V2_SAMPLE_SIZE_CONFIG = \",config.params.limit_samples=200\"\n",
    "\n",
    "# Run evaluation \n",
    "!core-evals-safety-eval run_eval \\\n",
    "               --output_dir {AEGIS_V2_RESULTS_DIR} \\\n",
    "               --model_id \"test-model\" \\\n",
    "               --model_url http://localhost:5000/v1 \\\n",
    "               --model_type chat \\\n",
    "               --eval_type aegis_v2 \\\n",
    "               --overrides=\"config.params.max_new_tokens=8192,config.params.extra.judge.url=http://localhost:6000/v1,config.params.parallelism=10,config.params.extra.judge.parallelism=10{AEGIS_V2_SAMPLE_SIZE_CONFIG}\" &> \"$LOG_DIR/safety-eval-aegis-v2-vllm.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87f60c-f2f5-4e49-b96f-66b5934e4688",
   "metadata": {},
   "source": [
    "Check the log file `$LOG_DIR/safety-eval-aegis-v2.log` for progress.\n",
    "\n",
    "Run the following command to complete evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff195b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python aegisv2_evaluation.py --input_file {AEGIS_V2_RESULTS_DIR}/output.csv &> {LOG_DIR}/safety-eval-aegis-v2-evaluation.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801d5f2",
   "metadata": {},
   "source": [
    "Let's take a look at the safety score and category breakdown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728592a4-932e-4f8b-9081-b9caf446b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def category_stats(df, categories_column=\"Safety Categories 0\"):\n",
    "    all_cats = list(df[categories_column])\n",
    "    cats = [cat.strip() for cats_str in all_cats for cat in cats_str.split(',')]\n",
    "    return Counter(cats)\n",
    "\n",
    "aegis_v2_df = pd.read_csv(f\"{AEGIS_V2_RESULTS_DIR}/output.csv\")\n",
    "\n",
    "aegis_v2_label_s = aegis_v2_df[\"Response Safety 0\"].value_counts()\n",
    "aegis_v2_safety_score = aegis_v2_label_s.loc[\"safe\"] / aegis_v2_label_s.sum()\n",
    "\n",
    "print(f\"Aegis v2 safety score: {aegis_v2_safety_score:.2f}\")\n",
    "print(\"===============================================\")\n",
    "print(\"The category distribution for unsafe responses:\")\n",
    "\n",
    "\n",
    "unsafe_df = aegis_v2_df[aegis_v2_df[\"response_label\"] == \"Response Safety 0\"]\n",
    "category_counts = category_stats(unsafe_df, categories_column=\"violated_categories\")\n",
    "pd.Series(category_counts).sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef86cf-10ec-4ec2-8f3b-9e739be909ce",
   "metadata": {},
   "source": [
    "### Evaluating Content Safety with WildGuard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940a9df-9918-49b1-9800-4086b7812bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"1\":\n",
    "    WILDGUARD_SAMPLE_SIZE_CONFIG = \"\"    \n",
    "else:\n",
    "    WILDGUARD_SAMPLE_SIZE_CONFIG = \",config.params.limit_samples=200\"\n",
    "\n",
    "# Run evaluation\n",
    "!core-evals-safety-eval run_eval \\\n",
    "               --output_dir {WILDGUARD_RESULTS_DIR} \\\n",
    "               --model_id \"test-model\" \\\n",
    "               --model_url http://localhost:5000/v1 \\\n",
    "               --model_type chat \\\n",
    "               --eval_type wildguard \\\n",
    "               --overrides=\"config.params.extra.judge.url=http://localhost:7000/v1,,config.params.parallelism=10,config.params.extra.judge.parallelism=10{WILDGUARD_SAMPLE_SIZE_CONFIG}\" &> \"$LOG_DIR/safety-eval-wildguard-vllm.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282b480",
   "metadata": {},
   "source": [
    "Check the `$LOG_DIR/safety-eval-wildguard-vllm.log` file for progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb1ddb-b759-4f96-8a71-c2031debecb3",
   "metadata": {},
   "source": [
    "When the evaluation completes, determine the Wildguard ratio.\n",
    "This notebook uses the ratio of safe responses to total responses as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8213d-93d6-4a35-a6bb-50093398c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildguard_results = json.load(open(f\"{WILDGUARD_RESULTS_DIR}/metrics.json\"))\n",
    "\n",
    "wg_safe_cnt = wildguard_results[\"safe\"] if \"safe\" in wildguard_results else 0\n",
    "wg_unsafe_cnt = wildguard_results[\"unsafe\"] if \"unsafe\" in wildguard_results else 0\n",
    "\n",
    "wildguard_safe_score = wg_safe_cnt / (wg_safe_cnt + wg_unsafe_cnt)\n",
    "\n",
    "print(f\"WildGuard safety score: {wildguard_safe_score * 100.:.2f}%\")\n",
    "print(\"===============================================\")\n",
    "print(\"The category distribution for unsafe responses:\")\n",
    "wildguard_df = pd.read_csv(f\"{WILDGUARD_RESULTS_DIR}/output.csv\")\n",
    "wildguard_df[wildguard_df[\"response_harm_label\"] == \"harmful\"][\"subcategory\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a410d3-a133-4137-bf4e-a230be79f2c6",
   "metadata": {},
   "source": [
    "### Shutting down guard models and re-launching vLLM server\n",
    "\n",
    "Now you can shutdown the vLLM servers for the two guard models and fully use all the GPUs for the target model for more efficient evaluaton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3ce66-3e16-442c-bb25-978a39419502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the running vllm processes\n",
    "subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])\n",
    "\n",
    "# VLLM Host \n",
    "os.environ.update({\n",
    "    'VLLM_ENGINE_ITERATION_TIMEOUT_S': '36000',\n",
    "    'VLLM_ALLOW_LONG_MAX_MODEL_LEN': '1',\n",
    "    'VLLM_HOST': '0.0.0.0',\n",
    "    'VLLM_TENSOR_PARALLEL_SIZE': '8',\n",
    "    'VLLM_USE_V1': '1',\n",
    "    'POLICY_MODEL_GPUS': '0,1,2,3,4,5,6,7',\n",
    "})\n",
    "\n",
    "print(\"Starting policy model server...\")\n",
    "policy_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', MODEL_NAME_OR_PATH,\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '5000',\n",
    "    '--served-model-name', 'test-model',\n",
    "    '--enable-reasoning', \n",
    "    '--reasoning-parser', 'qwen3',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['POLICY_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-model.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)\n",
    "\n",
    "!sleep 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee247b-581b-46b1-b5d7-0c4abae37b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the vLLM serve is correctly launched\n",
    "!tail {LOG_DIR}/vllm-server-model.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca2b8c",
   "metadata": {},
   "source": [
    "### Evaluating Product Security with Garak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355d522-4d4e-409d-b4d6-5381fb29973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"1\":\n",
    "    GARAK_SAMPLE_SIZE_CONFIG = \"\"    \n",
    "else:\n",
    "    GARAK_SAMPLE_SIZE_CONFIG = \"--target_probes tier1\"\n",
    "\n",
    "# Run evaluation\n",
    "!python run_garak.py --output_basedir {GARAK_RESULTS_DIR} --base_config_path ./garak_base_config.yaml --max_workers 4 {GARAK_SAMPLE_SIZE_CONFIG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac810d3f",
   "metadata": {},
   "source": [
    "garak stores the results of each probe individually and produces an HTML report with the success and failure rate for each probe.\n",
    "Aggregate and summarize the scan results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe885ef6-6581-4317-96df-b54d7c7b6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = os.path.join(GARAK_REPORT_DIR, \"garak_results.csv\")\n",
    "garak_df = pd.read_csv(output_csv)\n",
    "garak_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8113148-8dac-4c44-957b-00cee5b85c97",
   "metadata": {},
   "source": [
    "Let's take a look at `z_score`---a score calibrated using reference models. Negative z scores indicate vulnerability in the corresponding aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a46985-9046-45a3-80d1-b0407ec15119",
   "metadata": {},
   "outputs": [],
   "source": [
    "garak_df[garak_df[\"z_score\"] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c35ad",
   "metadata": {},
   "source": [
    "The Z-score is a measure of how many standard deviations the model performs from the mean.\n",
    "The mean is periodically calculated by garak developers from a _bag of models_ that represent state-of-the-art models at the time.\n",
    "For more information about the models and the statistics, refer to [Intepreting results with a bag of models](https://github.com/NVIDIA/garak/blob/main/garak/data/calibration/bag.md) in the garak repository on GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696ce49-322a-4601-a546-ba776d197c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"excellent\", \"competitive\", \"average\", \"below average\", \"poor\"]\n",
    "garak_label_df = garak_df[\"z_score_status\"].value_counts().reindex(labels)\n",
    "garak_resilience_score = garak_label_df[[\"excellent\", \"competitive\", \"average\"]].sum() / garak_label_df.sum()\n",
    "print(f\"Garak resilience score: {garak_resilience_score*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfabab9-4c00-4dd8-8b9c-031ee2ca973b",
   "metadata": {},
   "source": [
    "## Accuracy evaluation using NeMo Framework\n",
    "\n",
    "Some benchmarks use endpoints provided by build.nvidia.com for answer extraction. If your evaluaiton failed and see error messages like below, please visit [build.nvidia.com](https://build.nvidia.com/) and contact support.\n",
    "\n",
    "```\n",
    "Retry attempt 1/25 due to: ClientResponseError: 500, message='Internal Server Error', url='https://integrate.api.nvidia.com/v1/chat/completions'\n",
    "```\n",
    "\n",
    "\n",
    "### Evaluating Accuracy  with GPQA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8fa26-b550-4762-8418-a0f48bf1c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"1\":\n",
    "    GQPAD_SAMPLE_SIZE_CONFIG = \"\"    \n",
    "else:\n",
    "    GPQAD_SAMPLE_SIZE_CONFIG = \",config.params.limit_samples=20\"\n",
    "\n",
    "# Run evaluation\n",
    "!core_evals_simple_evals run_eval \\\n",
    "      --model_id 'test-model' \\\n",
    "      --model_url http://localhost:5000/v1/chat/completions \\\n",
    "      --eval_type gpqa_diamond \\\n",
    "      --output_dir {GPQA_DIAMOND_RESULTS_DIR} \\\n",
    "      --overrides=\"target.api_endpoint.type=chat, \\\n",
    "      config.params.temperature=0.6, \\\n",
    "      config.params.top_p=0.95, \\\n",
    "      config.params.max_new_tokens=16384, \\\n",
    "      config.params.max_retries=5, \\\n",
    "      config.params.parallelism=4, \\\n",
    "      config.params.request_timeout=600{GPQAD_SAMPLE_SIZE_CONFIG} \\\n",
    "      \" &> \"$LOG_DIR/core-evals-simple-evals-gpqa_diamond.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b94688-153d-4f48-9195-36ae3cdc9eb5",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy with MATH-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8d86b-a4ab-4ed7-8513-0b647efc64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"1\":\n",
    "    AA_MATH500_SAMPLE_SIZE_CONFIG = \"\"    \n",
    "else:\n",
    "    AA_MATH500_SAMPLE_SIZE_CONFIG = \",config.params.limit_samples=50\"\n",
    "\n",
    "# Run evaluation\n",
    "!core_evals_simple_evals run_eval \\\n",
    "    --eval_type AA_math_test_500 \\\n",
    "    --model_id test-model \\\n",
    "    --model_type chat \\\n",
    "    --model_url http://localhost:5000/v1/chat/completions \\\n",
    "    --output_dir {AA_MATH_500_RESULTS_DIR} \\\n",
    "    --overrides \"config.params.temperature=0.6,\\\n",
    "    config.params.top_p=0.95,\\\n",
    "    config.params.max_new_tokens=16384,\\\n",
    "    config.params.parallelism=4,\\\n",
    "    config.params.request_timeout=600{AA_MATH500_SAMPLE_SIZE_CONFIG}\\\n",
    "    \" &> \"$LOG_DIR/core-evals-simple-evals-aa-math-500.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3325f-efb4-4571-9fbe-b6552f4bedda",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy with IFEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b956872-2e01-4997-8fd7-ca628fd464be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"1\":\n",
    "    IFEVAL_SAMPLE_SIZE_CONFIG = \"\"    \n",
    "else:\n",
    "    IFEVAL_SAMPLE_SIZE_CONFIG = \",config.params.limit_samples=55\"\n",
    "\n",
    "# Run evaluation\n",
    "!core_evals_lm_eval run_eval \\\n",
    "    --eval_type ifeval \\\n",
    "    --model_id test-model \\\n",
    "    --model_type chat \\\n",
    "    --model_url http://localhost:5000/v1/chat/completions \\\n",
    "    --output_dir {IFEVAL_RESULTS_DIR} \\\n",
    "    --overrides \"config.params.temperature=0.6,\\\n",
    "    config.params.top_p=0.95,\\\n",
    "    config.params.max_new_tokens=32768,\\\n",
    "    config.params.request_timeout=150000000,\\\n",
    "    config.params.parallelism=4{IFEVAL_SAMPLE_SIZE_CONFIG}\\\n",
    "    \"  &> \"$LOG_DIR/core-eval-lm-eval-ifeval.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad51b7d8-3c66-447f-adc9-ba3bd026f809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "gpqa_diamond_score = None\n",
    "aa_math500_score = None\n",
    "ifeval_prompt_strict_score = None\n",
    "ifeval_inst_strict_score = None\n",
    "\n",
    "if os.path.exists(f\"{GPQA_DIAMOND_RESULTS_DIR}/gpqa_diamond.json\"):\n",
    "    gpqa_diamond_score = json.load(open(f\"{GPQA_DIAMOND_RESULTS_DIR}/gpqa_diamond.json\"))[\"score\"]\n",
    "\n",
    "if os.path.exists(f\"{AA_MATH_500_RESULTS_DIR}/AA_math_test_500.json\"):\n",
    "    aa_math500_score = json.load(open(f\"{AA_MATH_500_RESULTS_DIR}/AA_math_test_500.json\"))[\"score\"]\n",
    "\n",
    "pattern = os.path.join(IFEVAL_RESULTS_DIR, \"test-model\", \"results_*.json\")\n",
    "match_files = glob.glob(pattern)\n",
    "if len(match_files) > 0:\n",
    "    ifeval_results = json.load(open(match_files[0]))\n",
    "    ifeval_prompt_strict_score = ifeval_results[\"results\"][\"ifeval\"][\"prompt_level_strict_acc,none\"]\n",
    "    ifeval_inst_strict_score = ifeval_results[\"results\"][\"ifeval\"][\"inst_level_strict_acc,none\"]\n",
    "\n",
    "accuracy_s = pd.Series(\n",
    "    {\"gpqa_diamond\": gpqa_diamond_score,\n",
    "     \"aa_math_500\": aa_math500_score,\n",
    "     \"ifeval_prompt_strict\": ifeval_prompt_strict_score,\n",
    "     \"ifeval_inst_strict\": ifeval_inst_strict_score})\n",
    "accuracy_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89dd62-3f3f-4877-8d48-8df4d1afaccc",
   "metadata": {},
   "source": [
    "## Create a Report Card\n",
    "\n",
    "Now we have two set of evaluation results. Let's compare the safety and accuracy of the same model before and after safety post-training.\n",
    "Run the following code blocks to launch a server for visual analysis.\n",
    "\n",
    "You'll see a message like below. Go to Brev and on the instance page,\n",
    "\n",
    "```\n",
    "Access > Using Secure Links > Share a Service\n",
    "```\n",
    "\n",
    "Then, add 8501 so you can access to the port from your local machine.\n",
    "Open the browser and you'll be able to see the Report Card Dashboard.\n",
    "\n",
    "\n",
    "```\n",
    "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
    "\n",
    "\n",
    "  You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Local URL: http://localhost:8501\n",
    "  Network URL: http://<IP address>:8501\n",
    "  External URL: http://<IP address>:8501\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51ecf6e-71c1-4e6e-922b-6d27a802a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q streamlit > {os.devnull} 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f619e-d2a3-492f-a021-2021821d71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_MODEL_OUTPUT_DIR = f\"{BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B/\"\n",
    "TRAINED_MODEL_OUTPUT_DIR = f\"{BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B-Safety-Trained/\"\n",
    "\n",
    "!streamlit run app.py -- {ORIGINAL_MODEL_OUTPUT_DIR} {TRAINED_MODEL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df04d3-15fa-492f-8917-31f358435003",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "The series of notebooks walked you through and learn the following topics, leveraging NVIDIA's NeMo Framework Evaluation and NeMo-RL.\n",
    "\n",
    "- [Notebook 1]() Safety and Accuracy Evaluation using NeMo Eval\n",
    "- [Notebook 2]() Safety Post-training using Safety for Agentic AI's training recipe\n",
    "- [Notebook 3]() Re-running the same safety and accuracy evaluation to understand how the model has improved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
