{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed4bbfc",
   "metadata": {},
   "source": [
    "# Step 1 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d78ecde6-5a2f-4799-82b5-52396ecbbf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import openai\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dace57a-3941-402e-bcc4-68cc5637d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Run Baseline Evaluation - Run Evaluations in Background\n",
    "BASE_DIR = \"/workspace/\"\n",
    "LOG_DIR = \"/workspace/logs/\"\n",
    "DATASET_DIR = \"/workspace/dataset/\"\n",
    "MODEL_NAME_OR_PATH = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MODEL_TAG_NAME = MODEL_NAME_OR_PATH.split(\"/\")[-1]\n",
    "\n",
    "# * Dataset\n",
    "AEGIS_V2_TEST_DIR = f\"{DATASET_DIR}/aegis_v2\"\n",
    "\n",
    "# * Content Safety benchmark\n",
    "CONTENT_SAFETY_RESULTS_DIR = f\"{BASE_DIR}/results/{MODEL_TAG_NAME}/content-safety-evals\"\n",
    "AEGIS_V2_RESULTS_DIR = f\"{CONTENT_SAFETY_RESULTS_DIR}/aegis_v2\"\n",
    "WILDGUARD_RESULTS_DIR = f\"{CONTENT_SAFETY_RESULTS_DIR}/wildguard\"\n",
    "\n",
    "# * Security benchmark\n",
    "SECURITY_RESULTS_DIR = f\"{BASE_DIR}/results/{MODEL_TAG_NAME}/security-evals\"\n",
    "GARAK_RESULTS_DIR = f\"{SECURITY_RESULTS_DIR}/garak\"\n",
    "\n",
    "# * Accuracy benchmark\n",
    "ACCURACY_RESULTS_DIR = f\"{BASE_DIR}/results/{MODEL_TAG_NAME}/accuracy-evals\"\n",
    "GPQA_DIAMOND_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/gpqa-diamond\"\n",
    "AA_MATH_500_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/aa-math-500\"\n",
    "IFEVAL_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/ifeval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b819902",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir -p AEGIS_V2_TEST_DIR\n",
    "\n",
    "mkdir -p AEGIS_V2_RESULTS_DIR\n",
    "mkdir -p WILDGUARD_RESULTS_DIR\n",
    "mkdir -p GARAK_RESULTS_DIR\n",
    "mkdir -p GPQA_DIAMOND_RESULTS_DIR\n",
    "mkdir -p AA_MATH_500_RESULTS_DIR\n",
    "mkdir -p IFEVAL_RESULTS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00263b6c-132c-4a9e-83ba-6334607bdcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials\n",
    "os.environ.update({\n",
    "    \"MY_API_KEY\":\"empty\",\n",
    "    \"JUDGE_API_KEY\": \"nvapi-XXXXXXXXXXXXXX\",\n",
    "    \"HF_TOKEN\":\"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\"\n",
    "})\n",
    "\n",
    "os.environ.update({\n",
    "    'BASE_DIR': f\"{BASE_DIR}\",\n",
    "    'TMPDIR': f\"{BASE_DIR}/tmp\",\n",
    "    'XDG_CACHE_HOME': f\"{BASE_DIR}/cache\",\n",
    "    'HF_HOME': f\"{BASE_DIR}/cache/huggingface\",\n",
    "    'UV_CACHE_DIR': f\"{BASE_DIR}/cache/uv\",\n",
    "    'TRITON_CACHE_DIR': f\"{BASE_DIR}/cache/triton\",\n",
    "    'DATASET_CACHE_DIR': f\"{BASE_DIR}/dataset_cache\",\n",
    "    'RAY_TMPDIR': \"/tmp/ray\",\n",
    "    'LOG_DIR': f\"{LOG_DIR}\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0087a27",
   "metadata": {},
   "source": [
    "## Content Safety - Aegis v2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55099a66",
   "metadata": {},
   "source": [
    "Launch a vLLM server for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca15ed7-8a04-4890-b4a6-1c350dea2ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting policy model server...\n"
     ]
    }
   ],
   "source": [
    "# VLLM Host \n",
    "os.environ.update({\n",
    "    'VLLM_ENGINE_ITERATION_TIMEOUT_S': '36000',\n",
    "    'VLLM_ALLOW_LONG_MAX_MODEL_LEN': '1',\n",
    "    'VLLM_HOST': '0.0.0.0',\n",
    "    'VLLM_TENSOR_PARALLEL_SIZE': '1',\n",
    "    'POLICY_MODEL_GPUS': '0,1,2,3',\n",
    "    'SAFETY_MODEL_GPUS': '4,5'\n",
    "})\n",
    "\n",
    "print(\"Starting policy model server...\")\n",
    "policy_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', MODEL_NAME_OR_PATH,\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '5000',\n",
    "    '--served-model-name', 'test-model',\n",
    "    '--enable-reasoning', \n",
    "    '--reasoning-parser', 'qwen3',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['POLICY_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-model.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb704e8c-e0bf-49a0-b2ea-07c43b44bc4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['pkill', '-f', 'vllm.entrypoints.openai.api_server'], returncode=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49850be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Download the file\n",
    "huggingface-cli download nvidia/Aegis-AI-Content-Safety-Dataset-2.0 test.json --repo-type dataset\n",
    "mv test.json ${AEGIS_V2_TEST_DIR}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b769c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AegisV2Evaluator:\n",
    "    def __init__(self, model_name,\n",
    "                 port: int = 5000,\n",
    "                 ):\n",
    "        self.client = openai.OpenAI(\n",
    "            base_url=f\"http://localhost:{port}/v1\",\n",
    "            api_key=\"EMPTY\"\n",
    "        )\n",
    "        self.model_name = model_name\n",
    "        self.port = port\n",
    "        self.output_key = output_key\n",
    "\n",
    "    def query_server(self, client, messages, model_name, temperature, top_p, max_tokens, number_of_generations=1):\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            stream=False,\n",
    "            n=number_of_generations\n",
    "        )\n",
    "        if number_of_generations == 1:\n",
    "            return response.choices[0].message.content\n",
    "        else:\n",
    "            return [choice.message.content for choice in response.choices]\n",
    "\n",
    "    def query_single(self, data, idx, temperature=0.6, top_p=0.95, tokens_to_generate=8192, number_of_generations=1):\n",
    "        \"\"\"Function to wrap openai client code for a single data item\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": data['prompt']}]\n",
    "        try:\n",
    "            result = self.query_server(self.client, messages, self.model_name,\n",
    "                                       temperature,\n",
    "                                       top_p,\n",
    "                                       tokens_to_generate,\n",
    "                                       number_of_generations)\n",
    "            return result, idx\n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "\n",
    "    def query_batch(self,\n",
    "                    data_list,\n",
    "                    num_workers: int = 256,\n",
    "                    temperature: float = 0.6,\n",
    "                    top_p: float = 0.95,\n",
    "                    tokens_to_generate: int = 8192,\n",
    "                    number_of_generations: int = 1):\n",
    "        \"\"\"Add progress bar for query_batch\"\"\"\n",
    "        results = [None for _ in range(len(data_list))]\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            futures = [executor.submit(self.query_single, data, idx, temperature, top_p, tokens_to_generate, number_of_generations) for idx, data in enumerate(data_list)]\n",
    "            for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Querying\"):\n",
    "                pred, idx = future.result()\n",
    "                results[idx] = pred\n",
    "        return results\n",
    "    \n",
    "    def run_evaluation(self,\n",
    "                       num_workers: int = 256,\n",
    "                       temperature: float = 0.6,\n",
    "                       top_p: float = 0.95,\n",
    "                       tokens_to_generate: int = 8192,\n",
    "                       number_of_generations: int = 1,\n",
    "                       test_filepath: str = None,\n",
    "                       pred_filepath: str = None,\n",
    "                       output_key: str = \"generated_response\"):\n",
    "        \"\"\"Run evaluation for Aegis v2\"\"\"\n",
    "        test_examples = [json.loads(line) for line in open(test_filepath)]\n",
    "        predictions = self.query_batch(test_examples,\n",
    "                                       num_workers=num_workers,\n",
    "                                       temperature=temperature,\n",
    "                                       top_p=top_p,\n",
    "                                       tokens_to_generate=tokens_to_generate,\n",
    "                                       number_of_generations=number_of_generations)\n",
    "        results = []\n",
    "        for test, pred in zip(test_examples, predictions):\n",
    "            if isinstance(pred, list):\n",
    "                for p in pred:\n",
    "                    results.append({output_key: p, **test})\n",
    "            else:\n",
    "                test[output_key] = pred\n",
    "                results.append(test)\n",
    "\n",
    "        with open(pred_filepath, 'w') as f:\n",
    "            for d in results:\n",
    "                f.write(json.dumps(d) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03a34a",
   "metadata": {},
   "source": [
    "```\n",
    "# * Aegis v2\n",
    "$ safety-eval  --model-name  \"deepseek-ai/deepseek-r1-distill-llama-8b\" \\\n",
    "               --model-url http://localhost:8000/v1 \\\n",
    "               --judge-url  https://b319a99a-5241-4459-b641-7219ad0fd86d.invocation.api.nvcf.nvidia.com/v1  \\\n",
    "               --results-dir results/aegis \\\n",
    "               --concurrency 64  \\\n",
    "               --eval aegis_v2 \\\n",
    "               --inference_params temperature=0.6,top_p=0.95,max_completion_tokens=12000 --limit 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b66676",
   "metadata": {},
   "outputs": [],
   "source": [
    "aegis_v2_evaluator = AegisV2Evaluator(model_name=\"test-model\", port=5000)\n",
    "aegis_v2_evaluator.run_evaluation(test_filepath=f\"{DATASET_DIR}/aegis_v2/test.jsonl\",\n",
    "                                  pred_filepath=f\"{AEGIS_V2_RESULTS_DIR}/pred.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca2b8c",
   "metadata": {},
   "source": [
    "## Product Security - Garak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f656d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf228950",
   "metadata": {},
   "source": [
    "## Accuracy - GPQA-D, MATH-500, AIME2024, IFEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3386e1bb-2b98-4e76-9fc2-7edc0de78863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['simple_evals', '--model', 'test-model', '--url', 'http://localhost:5000/v1/chat/completions', '--eval_name', 'gpqa_diamond', '--temperature', '0.6', '--top_p', '0.95', '--max_tokens', '8192', '--out_dir', 'results/baseline-evals/gpqa-diamond', '--cache_dir', 'results/baseline-evals/gpqa-diamond', '--num_threads', '4', '--max_retries', '5', '--timeout', '150'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\n",
    "    [\n",
    "        \"simple_evals\",\n",
    "        \"--model\", 'test-model',\n",
    "        \"--url\", \"http://localhost:5000/v1/chat/completions\",\n",
    "        \"--eval_name\", \"gpqa_diamond\",\n",
    "        \"--temperature\", \"0.6\",\n",
    "        \"--top_p\", \"0.95\",\n",
    "        \"--max_tokens\", \"8192\",\n",
    "        \"--out_dir\", f\"results/baseline-evals/gpqa-diamond\",\n",
    "        \"--cache_dir\", f\"results/baseline-evals/gpqa-diamond\",\n",
    "        \"--num_threads\", \"4\",\n",
    "        \"--max_retries\", \"5\",\n",
    "        \"--timeout\", \"150\"\n",
    "    ],\n",
    "    stdout=open(f\"{os.getenv('LOG_DIR')}/baseline-eval-gpqa-diamond.log\", \"w\"),\n",
    "    stderr=subprocess.STDOUT,\n",
    "    start_new_session=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5671d79-dd04-4531-b377-5e5375edbebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['simple_evals', '--model', 'test-model', '--url', 'http://localhost:5000/v1/chat/completions', '--eval_name', 'AA_math_test_500', '--temperature', '0.6', '--top_p', '0.95', '--max_tokens', '8192', '--out_dir', 'results/baseline-evals/aa-math-500', '--cache_dir', 'results/baseline-evals/aa-math-500', '--num_threads', '4', '--max_retries', '5', '--timeout', '150'], returncode=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subprocess.run(\n",
    "    [\n",
    "        \"simple_evals\",\n",
    "        \"--model\", 'test-model',\n",
    "        \"--url\", \"http://localhost:5000/v1/chat/completions\",\n",
    "        \"--eval_name\", \"AA_math_test_500\",\n",
    "        \"--temperature\", \"0.6\",\n",
    "        \"--top_p\", \"0.95\",\n",
    "        \"--max_tokens\", \"8192\",\n",
    "        \"--out_dir\", f\"results/baseline-evals/aa-math-500\",\n",
    "        \"--cache_dir\", f\"results/baseline-evals/aa-math-500\",\n",
    "        \"--num_threads\", \"4\",\n",
    "        \"--max_retries\", \"5\",\n",
    "        \"--timeout\", \"150\"\n",
    "    ],\n",
    "    stdout=open(f\"{os.getenv('LOG_DIR')}/baseline-eval-aa-math-500.log\", \"w\"),\n",
    "    stderr=subprocess.STDOUT,\n",
    "    start_new_session=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec153c85-d822-480e-82e3-9c12344d87aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "process = subprocess.Popen(\n",
    "    [\n",
    "        \"lm-eval\",\n",
    "        \"--tasks\", \"ifeval\",\n",
    "        \"--num_fewshot\", \"0\",\n",
    "        \"--model\", \"local-chat-completions\",\n",
    "        \"--model_args\", \"base_url=http://localhost:5000/v1/chat/completions,model=test-model,tokenized_requests=false,num_concurrent=4,max_gen_toks=8192,timeout=150,max_retries=5,stream=False\",\n",
    "        \"--log_samples\",\n",
    "        \"--output_path\", f\"results/baseline-evals/ifeval\",\n",
    "        \"--use_cache\", f\"results/baseline-evals/ifeval\",\n",
    "        \"--fewshot_as_multiturn\",\n",
    "        \"--apply_chat_template\",\n",
    "        \"--gen_kwargs\", \"temperature=0.6,top_p=0.95\"\n",
    "    ],\n",
    "    stdout=open(f\"{os.getenv('LOG_DIR')}/baseline-eval-ifeval.log\", \"w\"),\n",
    "    stderr=subprocess.STDOUT,\n",
    "    start_new_session=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb7c8d7-749c-4a6b-860c-118527e6773c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (penv3)",
   "language": "python",
   "name": "penv3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
