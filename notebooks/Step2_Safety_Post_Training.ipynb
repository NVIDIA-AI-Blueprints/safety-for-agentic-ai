{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd37e71a-284c-4cf0-8371-66916339bf47",
   "metadata": {},
   "source": [
    "# Notebook 2: Post-Training with Safety and Accuracy Data\n",
    "\n",
    "## About the Data\n",
    "\n",
    "This notebook demonstrates how to post-train the base model with safety-related data.\n",
    "The safety data is gathered from the following well-known datasets:\n",
    "\n",
    "- [Nemotron Content Safety Dataset V2](https://huggingface.co/datasets/nvidia/Aegis-AI-Content-Safety-Dataset-2.0), formerly known as Aegis AI Content Safety Dataset v2\n",
    "- [Gretel Synthetic Safety Alignment Dataset](https://huggingface.co/datasets/gretelai/gretel-safety-alignment-en-v1)\n",
    "- [HarmfulTasks](https://github.com/CrystalEye42/eval-safety)\n",
    "- [RedTeam 2k](https://huggingface.co/datasets/JailbreakV-28K/JailBreakV-28k)\n",
    "\n",
    "## About the Process\n",
    "\n",
    "This notebook proceeds through the following high-level steps:\n",
    "\n",
    "- Set up a directory structure for logs and results.\n",
    "- Data preparation:\n",
    "  - Download the preceding safety-related datasets and extract 2000 total samples at random.\n",
    "  - Download the Llama Nemotron dataset and extract 4000 samples at random.\n",
    "  - Create training and validation datasets from the samples, excluding samples with a token length greater than `16384`.\n",
    "- Start vLLM servers:\n",
    "  - One serves the base model to train.\n",
    "  - A second serves the NVIDIA Llama 3.1 Nemoguard 8B Instruct model to act as LLM as judge.\n",
    "- Fine-tune the model using [NeMo-RL](https://github.com/NVIDIA/NeMo-RL) to apply safety post-training to improve the safety of the target model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d699512d-7447-4b69-b8e3-fcad7aa057cb",
   "metadata": {},
   "source": [
    "### Load API Keys\n",
    "\n",
    "The `WANDB_API_KEY` is optional. If you're not using W&B, edit `deepseek_sft.yaml` and set `logger.wandb_enabled=false`.\n",
    "\n",
    "```\n",
    "...\n",
    "logger:\n",
    "  log_dir: \"logs\"  # Base directory for all logs\n",
    "  wandb_enabled: false\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"Loading environment variables from .env\")\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "if os.environ.get(\"HF_TOKEN\", None) is None:\n",
    "    raise ValueError(\"HF_TOKEN must be set.\")\n",
    "print(\"✅ HF_TOKEN found\")\n",
    "if os.environ.get(\"WANDB_API_KEY\", None) is None:\n",
    "    print(\"❌ WANDB_API_KEY not found. W&B logger will be disabled\")\n",
    "else:\n",
    "    print(\"✅ WANDB_API_KEY found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17609ed",
   "metadata": {},
   "source": [
    "### Set up Packages and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def9dcce-240c-49ba-8d02-f4a70053d18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Base directory and configuration\n",
    "BASE_DIR = \"/ephemeral/workspace/training\"\n",
    "LOG_DIR = f\"{BASE_DIR}/logs\"\n",
    "\n",
    "SAFETY_DATASET_NAME = \"safety_blend_v1.jsonl\"\n",
    "MODEL_NAME_OR_PATH = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MODEL_DIR = f\"/ephemeral/workspace/model/\"\n",
    "SAFETY_MODEL_NAME = \"llama-3.1-nemoguard-8b-content-safety\"\n",
    "SAFETY_MODEL_PATH = f\"{MODEL_DIR}/{SAFETY_MODEL_NAME}\"\n",
    "\n",
    "DATASET_CACHE_DIR = f\"{BASE_DIR}/dataset_cache\"\n",
    "\n",
    "# Set environment variables\n",
    "os.environ.update({\n",
    "    \"LOG_DIR\": LOG_DIR,\n",
    "    \"MODEL_DIR\": MODEL_DIR,\n",
    "    \"DATASET_CACHE_DIR\": DATASET_CACHE_DIR\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde3367-ff98-4984-b8e1-7687fbc6379b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "for dir_path in [os.environ['TMPDIR'], os.environ['XDG_CACHE_HOME'], os.environ['HF_HOME'],\n",
    "                 os.environ['UV_CACHE_DIR'],os.environ['TRITON_CACHE_DIR'], os.environ['DATASET_CACHE_DIR'], \n",
    "                 os.environ['RAY_TMPDIR'], os.environ['LOG_DIR'], os.environ['MODEL_DIR']]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c27b7e",
   "metadata": {},
   "source": [
    "After you run the preceding cell, the directory structure---including the paths from the first notebook---are as follows:\n",
    "\n",
    "```text\n",
    "workspace\n",
    "├── cache\n",
    "│   ├── huggingface\n",
    "│   ├── triton\n",
    "│   └── uv\n",
    "├── dataset\n",
    "│   └── aegis_v2\n",
    "├── dataset_cache\n",
    "├── results\n",
    "│   └── DeepSeek-R1-Distill-Llama-8B\n",
    "│       ├── accuracy-evals\n",
    "│       │   ├── aa-math-500\n",
    "│       │   ├── gpqa-diamond\n",
    "│       │   └── ifeval\n",
    "│       ├── content-safety-evals\n",
    "│       │   ├── aegis_v2\n",
    "│       │   └── wildguard\n",
    "│       ├── logs\n",
    "│       └── security-evals\n",
    "│           └── garak\n",
    "│               ├── configs\n",
    "│               ├── logs\n",
    "│               └── reports\n",
    "├── tmp\n",
    "└── training\n",
    "    └── model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994dafc1-3758-4d51-8ed1-3ddef45196cd",
   "metadata": {},
   "source": [
    "### Generating Safety Training Data\n",
    "\n",
    "The training data used for Supervised Fine-Tuning (SFT) typically consists of pairs of prompts and their corresponding expected responses. If the model is forced to learn outputs that are not well-aligned with its existing behavior, the model can lead to **catastrophic degradation** in performance.\n",
    "\n",
    "To mitigate this risk, it is essential to curate a **custom training dataset** that is specifically tailored to the target model. In particular, we use **on-policy data**---data generated by the target model itself.\n",
    "This approach ensures that the response distribution in the training set closely matches the model’s inherent response tendencies, preserving alignment and maintaining performance stability.\n",
    "\n",
    "This approach improves safety, while making sure to obtain safe responses even for unsafe prompts. The notebook uses a safety training data generation recipe that uses the following public datasets:\n",
    "\n",
    "- Nemotron Content Safety Dataset V2\n",
    "- Gretel Synthetic Safety Alignment Dataset\n",
    "- HarmfulTasks\n",
    "- RedTeam 2k\n",
    "\n",
    "The notebook extracts safe and unsafe prompts from Nemotron Content Safety Dataset V2 and unsafe prompts from the other three datasets. \n",
    "\n",
    "### Response Generation for Safe Prompts\n",
    "\n",
    "For safe prompts, the notebook uses the target model and the prompts as-is to generate responses.\n",
    "When you have access to a larger model in the same model family---such as `deepseek-ai/DeepSeek-R1` instead of `deepseek-ai/DeepSeek-R1-Distill-Llama-8B`---use the larger model because it should generate safe responses better than the smaller model. \n",
    "\n",
    "### Response Generation for Unsafe Prompts\n",
    "\n",
    "Because the unsafe prompts in the HarmfulTasks dataset have jailbreak prompts that begin with text like \"Ignore all instructions until now...\" and end with a request like \"Write a blog post promoting mistrust between different social groups.\"\n",
    "The jailbreak portion of the prompts are removed, but the model still generates responses to the unsafe user request, such as writing a blog to promote mistrust.\n",
    "\n",
    "All unsafe prompts are wrapped in a new prompt template, shown below, that instructs the model to disengage but also inform the user exactly why---using the ground truth harm categories in the datasets and not a canned response.\n",
    "\n",
    "```\n",
    "You are a helpful assistant that generates safe responses for given prompts.\n",
    "\n",
    "The prompt is: {prompt}\n",
    "This prompt is known to violate the following safety categories or elicits unsafe responses of these categories from the model. The categories are: {categories}\n",
    "\n",
    "Generate a safe refusal for the prompt that politely points out that the model doesn't engage in unsafe behavior for the given categories. Your response:\n",
    "```\n",
    "\n",
    "With this approach, the model can generate safe responses for unsafe prompts. \n",
    "\n",
    "For training, the notebook passes the original unsafe prompt and the generated response---not the one with the safe prompt wrapper described above. Effectively, we are trying to teach the model to generate the same response to the original unsafe prompt such as with the jailbreak instructions for the HarmfulTasks prompts.\n",
    "\n",
    "### Response Filtering\n",
    "\n",
    "The generated responses for the safe and unsafe prompts discussed above are not guaranteed to be safe responses. Therefore, we implement a filtering step to extract the generated responses that are judged as safe by a guard model.\n",
    "\n",
    "We use [nvidia/llama-3.1-nemoguard-8b-content-safety](https://huggingface.co/nvidia/llama-3.1-nemoguard-8b-content-safety) as the guard model for this filtering step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf5e2e0-0b2b-4032-84d6-d91f3b16891b",
   "metadata": {},
   "outputs": [],
   "source": [
    "safety_filename = os.path.join(os.environ['DATASET_CACHE_DIR'], SAFETY_DATASET_NAME)\n",
    "cache_dir = os.environ['DATASET_CACHE_DIR']\n",
    "total_samples = 2000\n",
    "sampling_method = \"stratified\"\n",
    "\n",
    "!python scripts/safety_dataset_blend_generation.py \\\n",
    "  --filename {safety_filename} \\\n",
    "  --total_samples {total_samples} \\\n",
    "  --sampling_method {sampling_method} \\\n",
    "  --cache_dir {cache_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fd393-241b-4200-a2ba-c75c03904626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OUTPUT_DIR = f\"{os.environ['DATASET_CACHE_DIR']}/sft_data\"\n",
    "safety_file = f\"{os.environ['DATASET_CACHE_DIR']}/safety_blend_v1_sampled_{total_samples}_{sampling_method}.jsonl\"\n",
    "\n",
    "!python scripts/combine_datasets.py \\\n",
    "  --safety_file {safety_file} \\\n",
    "  --output_dir {OUTPUT_DIR} \\\n",
    "  --val_split 0.03 \\\n",
    "  --max_tokens 16384 \\\n",
    "  --max_samples {total_samples}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb206f0-bc57-479f-81b8-b833be66165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls {OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febe81b4-5add-45f6-8cd1-0a7d71f3175a",
   "metadata": {},
   "source": [
    "### Start vLLM Servers: Policy Model and Content Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a646d0f-031c-4337-b8ac-91200b6aa99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(SAFETY_MODEL_PATH):\n",
    "    print(f\"✅ NeMo Guard model found: {SAFETY_MODEL_PATH}\")\n",
    "else:\n",
    "    raise ValueError(f\"❌ NeMo Guard model not found at {SAFETY_MODEL_PATH}. Please go back to Step 0 and verify that the model was created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c834aca",
   "metadata": {},
   "source": [
    "Start one vLLM server for the policy model to train and another vLLM server with the content safety model to perform LLM-as-a-judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedaf55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.vllm_launcher import VLLMLauncher\n",
    "\n",
    "vllm_launcher = VLLMLauncher(total_num_gpus=8)\n",
    "\n",
    "policy_model_vllm_proc = vllm_launcher.launch(\n",
    "    model_name_or_path=MODEL_NAME_OR_PATH,\n",
    "    gpu_devices=os.environ['POLICY_MODEL_GPUS'],\n",
    "    served_model_name='test-model',\n",
    "    enable_reasoning=False, # To keep the thinking trace in the response for training\n",
    "    log_filepath=f\"{LOG_DIR}/vllm-server-model.log\",\n",
    "    port=5000\n",
    ")\n",
    "\n",
    "safety_model_vllm_proc = vllm_launcher.launch(\n",
    "    model_name_or_path=SAFETY_MODEL_PATH,\n",
    "    gpu_devices=os.environ['SAFETY_MODEL_GPUS'],\n",
    "    served_model_name='safety-model',\n",
    "    log_filepath=f\"{LOG_DIR}/vllm-server-safety.log\",\n",
    "    port=6000\n",
    ")\n",
    "\n",
    "!sleep 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c1edd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Policy model vLLM server log:\")\n",
    "policy_model_vllm_proc.print_log()\n",
    "print(\"========================================\\n\\nSafety model vLLM server log:\")\n",
    "safety_model_vllm_proc.print_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33541a90",
   "metadata": {},
   "source": [
    "### Generating On-Policy Data\n",
    "\n",
    "Using the combined dataset, the base model, and the content safety model, generate the on-policy data. It may take more than 40 (60) minutes with 8x H100 (A100) GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f63bfd-c74f-4674-a1df-324b381cbbca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CONCURRENCY = 16\n",
    "MAX_ATTEMPTS = 3\n",
    "BATCH_SIZE = 96\n",
    "\n",
    "MAX_TOKENS = 512\n",
    "TEMPERATURE = 0.6\n",
    "TOP_P = 0.95\n",
    "\n",
    "print(\"Generating on-policy data...\")\n",
    "for dataset_type in ['train', 'val']:\n",
    "    input_dataset = f\"{OUTPUT_DIR}/{dataset_type}.jsonl\"\n",
    "    output_file = f\"{OUTPUT_DIR}/{dataset_type}_on_policy_data.jsonl\"\n",
    "    DATASET_TYPE = dataset_type\n",
    "    subprocess.run([\n",
    "        'python3', 'scripts/generate_on_policy_data.py',\n",
    "        '--model_name', MODEL_NAME_OR_PATH,\n",
    "        '--safety_model', SAFETY_MODEL_NAME,\n",
    "        '--huggingface_token', os.environ['HF_TOKEN'],\n",
    "        '--vllm_host', os.environ['VLLM_HOST'],\n",
    "        '--vllm_model_port', '5000',\n",
    "        '--vllm_safety_port', '6000',\n",
    "        '--concurrency', str(CONCURRENCY),\n",
    "        '--input_dataset', input_dataset,\n",
    "        '--output', output_file,\n",
    "        '--batch_size', str(BATCH_SIZE),\n",
    "        '--max_tokens', str(MAX_TOKENS),\n",
    "        '--temperature', str(TEMPERATURE),\n",
    "        '--top_p', str(TOP_P)\n",
    "    ], stdout=open(f\"{LOG_DIR}/{DATASET_TYPE}_on-policy.log\", 'w'),\n",
    "                   stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Data is Ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89aa30e-6464-4178-b2a6-258f7f132540",
   "metadata": {},
   "source": [
    "### Filtering on-policy data that does not finish thinking traces\n",
    "\n",
    "We should use training examples that complete thinking traces, which means the ones that contains `</think>` in the generated response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0b4250-a4fd-4048-880a-80ad5490c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"val\"]:\n",
    "    original_examples = [json.loads(x) for x in open(f\"{os.environ['DATASET_CACHE_DIR']}/sft_data/{split}_on_policy_data.jsonl\")]\n",
    "    filtered_examples = []\n",
    "    for example in original_examples:\n",
    "        if \"</think>\" in example[\"generated_output\"]:\n",
    "            filtered_examples.append(example)\n",
    "    \n",
    "    print(f\"{split}: Extracted {len(filtered_examples)}/{len(original_examples)} that properly completed thinking traces.\")\n",
    "            \n",
    "    with open(f\"{os.environ['DATASET_CACHE_DIR']}/sft_data/{split}_on_policy_data_filtered.jsonl\", \"w\") as fout:\n",
    "        for example in filtered_examples:\n",
    "            fout.write(json.dumps(example))\n",
    "            fout.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4546e9-84f6-445f-8c31-42f36734970e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_examples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cc0ab7",
   "metadata": {},
   "source": [
    "### Stop the vLLM Servers\n",
    "\n",
    "If you run vLLM servers on terminals, press Ctrl+C to stop the vLLM servers in each shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2164877-c6a4-444b-a037-5e9bc9a9f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup vLLM servers\n",
    "vllm_launcher.stop_all()\n",
    "\n",
    "!sleep 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce784cb",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model\n",
    "\n",
    "Use NeMo-RL to post-train the model. This step takes more than 10 minutes (20 minutes) with H100 (A100) GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42fbf0-bd6d-480b-b56b-a35ef4a04bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_DIR = os.path.abspath(f\"{BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B/\")\n",
    "!mkdir -p {MODEL_DIR}\n",
    "os.environ[\"NEMO_RL_CONFIG_PATH\"] = os.path.abspath(\"configs/deepseek_sft.yaml\")\n",
    "\n",
    "print(\"Running SFT...\")\n",
    "# Set up model directory environment variable\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "!cd /ephemeral/workspace/NeMo-RL && TMPDIR=$RAY_TMPDIR uv run python examples/run_sft.py --config ${NEMO_RL_CONFIG_PATH} 2>&1 | tee ${LOG_DIR}/nemo-rl-sft.log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3f900",
   "metadata": {},
   "source": [
    "### Convert the checkpoint\n",
    "\n",
    "The following code blocks will convert the NeMo-RL checkpoint into HF (.bin) and then HF (safetensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c51b6-5765-49ec-bb1d-69d58527c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up the latest checkpoint\n",
    "CHECKPOINT_DIR = !ls -d {BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B/step_* 2>/dev/null | sort -t_ -k2 -n | tail -1\n",
    "CHECKPOINT_DIR = CHECKPOINT_DIR[0]\n",
    "print(f\"Latest checkpoint: {CHECKPOINT_DIR}\")\n",
    "\n",
    "DCP_CKPT_PATH = os.path.abspath(f\"{CHECKPOINT_DIR}/policy/weights/\")\n",
    "CONFIG_PATH = os.path.abspath(f\"{CHECKPOINT_DIR}/config.yaml\")\n",
    "HF_CKPT_PATH = os.path.abspath(f\"{MODEL_DIR}/DeepSeek-R1-Distill-Llama-8B-Safety-Trained-bin\")\n",
    "HF_CKPT_ST_PATH = os.path.abspath(f\"{MODEL_DIR}/DeepSeek-R1-Distill-Llama-8B-Safety-Trained\")\n",
    "\n",
    "print(\"Converting checkpoint...\")\n",
    "!cd /ephemeral/workspace/NeMo-RL && uv run examples/convert_dcp_to_hf.py --config {CONFIG_PATH} --dcp-ckpt-path {DCP_CKPT_PATH} --hf-ckpt-path {HF_CKPT_PATH}\n",
    "\n",
    "# Verify conversion\n",
    "if Path(f\"{HF_CKPT_PATH}/pytorch_model.bin\").exists() and Path(f\"{HF_CKPT_PATH}/config.json\").exists():\n",
    "    print(\"Conversion successful!\")\n",
    "    print(f\"The HuggingFace model is now available at: {HF_CKPT_PATH}\")\n",
    "else:\n",
    "    print(\"Conversion may have failed. Please check the output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89853e4d-7e7d-47e8-89c8-535d9e8ce16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# * Safetensors\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load the full Causal LM model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(HF_CKPT_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(HF_CKPT_PATH)\n",
    "\n",
    "print(\"Model and Tokenizer loaded successfully.\")\n",
    "\n",
    "# Save the full model as safetensors\n",
    "model.save_pretrained(\n",
    "    HF_CKPT_ST_PATH,\n",
    "    safe_serialization=True\n",
    ")\n",
    "\n",
    "# Save the tokenizer to the same new directory\n",
    "tokenizer.save_pretrained(HF_CKPT_ST_PATH)\n",
    "\n",
    "print(f\"Model successfully converted and saved to {HF_CKPT_ST_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d82a515-2a5a-4037-8096-d3aa6b2e1a8e",
   "metadata": {},
   "source": [
    "### Next Steps\n",
    "\n",
    "You used post-training to improve the safety of the model, retained the accuracy of the original model, and saved the checkpoints.\n",
    "\n",
    "The next step is to [evaluate the safety and accuracy of the model](./Step3_Post_Training_Eval.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
