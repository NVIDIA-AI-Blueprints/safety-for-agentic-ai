{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import concurrent.futures\n",
    "import openai\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting inferene Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 0.6\n",
    "top_p = 0.95\n",
    "tokens_to_generate = 12000\n",
    "\n",
    "port = 8000\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch VLLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "MODEL_GPUS=1\n",
    "MODEL_NAME=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "TENSOR_PARALLEL_SIZE=1\n",
    "DATA_PARALLEL_SIZE=1\n",
    "CUDA_VISIBLE_DEVICES=$MODEL_GPUS python3 -m vllm.entrypoints.openai.api_server \\\n",
    "  --model \"$MODEL_NAME\" \\\n",
    "  --reasoning-parser \"qwen3\" \\\n",
    "  --trust-remote-code \\\n",
    "  --seed 1 \\\n",
    "  --tensor-parallel-size \"$TENSOR_PARALLEL_SIZE\" \\\n",
    "  --data-parallel-size \"$DATA_PARALLEL_SIZE\" &> \"/workspace/vllm-server-model.log\" &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for Querying VLLM Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    base_url=f\"http://localhost:{port}/v1\",\n",
    "    api_key=\"EMPTY\"\n",
    ")\n",
    "number_of_generations = 1\n",
    "\n",
    "def query_server(client, messages, model_name, temperature, top_p, max_tokens, number_of_generations=1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=messages,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        stream=False,\n",
    "        n=number_of_generations\n",
    "    )\n",
    "    if number_of_generations == 1:\n",
    "        return response.choices[0].message.content\n",
    "    else:\n",
    "        return [choice.message.content for choice in response.choices]\n",
    "\n",
    "\n",
    "# Function to wrap openai client code for a single data item\n",
    "def query_single(data, idx):\n",
    "    messages = [{\"role\": \"user\", \"content\": data['prompt']}]\n",
    "    try:\n",
    "        result = query_server(client, messages, model_name, temperature, top_p, tokens_to_generate, number_of_generations)\n",
    "        return result, idx\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "\n",
    "# add progress bar for query_batch\n",
    "def query_batch(data_list, num_workers=32):\n",
    "    results = [None for _ in range(len(data_list))]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        futures = [executor.submit(query_single, data, idx) for idx, data in enumerate(data_list)]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Querying\"):\n",
    "            pred, idx = future.result()\n",
    "            results[idx] = pred\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl_data(data_path):\n",
    "    data = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Querying: 100%|██████████| 1928/1928 [11:42<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set the number of workers (adjust as needed)\n",
    "num_workers = 256\n",
    "\n",
    "input_file = '/home/zijiac/datasets/aegis_2_test.jsonl'\n",
    "output_key = 'generated_response'\n",
    "pred_output_file = '/home/zijiac/projects/NeMo-Safety/notebooks/aegis_v2_predictions.jsonl'\n",
    "eval_output_file = '/home/zijiac/projects/NeMo-Safety/notebooks/aegis_v2_eval.jsonl'\n",
    "\n",
    "data = load_jsonl_data(input_file)\n",
    "\n",
    "predictions = query_batch(data, num_workers)\n",
    "\n",
    "results = []\n",
    "\n",
    "for t,pred in zip(data, predictions):\n",
    "    if isinstance(pred, list):\n",
    "        for p in pred:\n",
    "            results.append({output_key: p, **t})\n",
    "    else:\n",
    "        t[output_key] = pred\n",
    "        results.append(t)\n",
    "\n",
    "\n",
    "with open(pred_output_file, 'w') as f:\n",
    "    for d in results:\n",
    "        f.write(json.dumps(d) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
