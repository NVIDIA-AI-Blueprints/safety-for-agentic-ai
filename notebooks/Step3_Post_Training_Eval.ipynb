{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aed4bbfc",
   "metadata": {},
   "source": [
    "# Notebook 3: Safety and Accuracy Evaluation for the Safety Post-trained Model\n",
    "\n",
    "In this notebook, we will rerun the same set of evaluations for the model and see how the safety scores on **Content Safety** and **Product Security** have improved. \n",
    "Since it's the second time, we'll skip the detailed explanation of each dataset but focus on collecting scores for them.\n",
    "\n",
    "### Before You Begin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab6a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!pip install ipywidgets > {os.devnull} 2>&1\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Reload .env to store the keys in os.environ\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "print(\"API keys have been stored in .env.\")\n",
    "\n",
    "if os.environ.get(\"HF_TOKEN\", None) is None or os.environ.get(\"JUDGE_API_KEY\", None) is None:\n",
    "    raise ValueError(\"HF_TOKEN and JUDGE_API_KEY must be set.\")\n",
    "print(\"✅ HF_TOKEN and JUDGE_API_KEY found\")\n",
    "\n",
    "if os.environ.get(\"RUN_FULL_EVAL\", None) is None:\n",
    "    print(\"RUN_FULL_EVAL is not configured. RUN_FULL_EVAL will be disabled\")\n",
    "    os.environ[\"RUN_FULL_EVAL\"] = \"0\"\n",
    "else:\n",
    "    print(f\"RUN_FULL_EVAL configuration found: {os.environ['RUN_FULL_EVAL']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf0864",
   "metadata": {},
   "source": [
    "Specify paths for data, evaluation results, and the base model to evaluate.\n",
    "\n",
    "```text\n",
    "workspace\n",
    "├── dataset\n",
    "│   └── aegis_v2\n",
    "└── results\n",
    "    └── DeepSeek-R1-Distill-Llama-8B-Safety-Trained\n",
    "        ├── accuracy-evals\n",
    "        │   ├── aa-math-500\n",
    "        │   ├── gpqa-diamond\n",
    "        │   └── ifeval\n",
    "        ├── content-safety-evals\n",
    "        │   ├── aegis_v2\n",
    "        │   └── wildguard\n",
    "        ├── logs\n",
    "        └── security-evals\n",
    "            └── garak\n",
    "                ├── configs\n",
    "                ├── logs\n",
    "                └── reports\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dace57a-3941-402e-bcc4-68cc5637d6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"/ephemeral/workspace\"\n",
    "DATASET_DIR = f\"{BASE_DIR}/dataset/\"\n",
    "ORIGINAL_MODEL_TAG_NAME = \"DeepSeek-R1-Distill-Llama-8B\"\n",
    "#TRAINED_MODEL_PATH = f\"{BASE_DIR}/training/model/DeepSeek-R1-Distill-Llama-8B-Safety-Trained\"\n",
    "TRAINED_MODEL_PATH = f\"{BASE_DIR}/training/results/DeepSeek-R1-Distill-Llama-8B/DeepSeek-R1-Distill-Llama-8B-Safety-Trained\"\n",
    "MODEL_NAME_OR_PATH = TRAINED_MODEL_PATH\n",
    "MODEL_TAG_NAME = TRAINED_MODEL_PATH.split(\"/\")[-1]\n",
    "MODEL_OUTPUT_DIR = f\"{BASE_DIR}/results/{MODEL_TAG_NAME}/\"\n",
    "LOG_DIR = f\"{MODEL_OUTPUT_DIR}/logs/\"\n",
    "\n",
    "# * Dataset\n",
    "NEMOGUARD_MODEL_PATH = f\"{BASE_DIR}/model/llama-3.1-nemoguard-8b-content-safety\"\n",
    "AEGIS_V2_TEST_DIR = f\"{DATASET_DIR}/aegis_v2\"\n",
    "\n",
    "# * Content Safety benchmark\n",
    "CONTENT_SAFETY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/content-safety-evals\"\n",
    "AEGIS_V2_RESULTS_DIR = f\"{CONTENT_SAFETY_RESULTS_DIR}/aegis_v2\"\n",
    "WILDGUARD_RESULTS_DIR = f\"{CONTENT_SAFETY_RESULTS_DIR}/wildguard\"\n",
    "\n",
    "# * Security benchmark\n",
    "SECURITY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/security-evals\"\n",
    "GARAK_RESULTS_DIR = f\"{SECURITY_RESULTS_DIR}/garak\"\n",
    "GARAK_CONFIG_DIR = f\"{GARAK_RESULTS_DIR}/configs\"\n",
    "GARAK_LOG_DIR = f\"{GARAK_RESULTS_DIR}/logs\"\n",
    "GARAK_REPORT_DIR = f\"{GARAK_RESULTS_DIR}/reports\"\n",
    "\n",
    "# * Accuracy benchmark\n",
    "ACCURACY_RESULTS_DIR = f\"{MODEL_OUTPUT_DIR}/accuracy-evals\"\n",
    "GPQA_DIAMOND_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/gpqa-diamond\"\n",
    "AA_MATH_500_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/aa-math-500\"\n",
    "IFEVAL_RESULTS_DIR = f\"{ACCURACY_RESULTS_DIR}/ifeval\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b819902",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories to store logs and results\n",
    "for path in [LOG_DIR, AEGIS_V2_TEST_DIR, AEGIS_V2_RESULTS_DIR, WILDGUARD_RESULTS_DIR,\n",
    "             GARAK_RESULTS_DIR, GARAK_CONFIG_DIR, GARAK_LOG_DIR, GARAK_REPORT_DIR,\n",
    "             GPQA_DIAMOND_RESULTS_DIR, AA_MATH_500_RESULTS_DIR, IFEVAL_RESULTS_DIR]:\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddf3324-43f5-48a8-93f8-ee45cdc4220e",
   "metadata": {},
   "source": [
    "## Serve the Safety Trained Model with vLLM\n",
    "\n",
    "Start a locally-running vLLM server to serve the safety trained model.\n",
    "After you start the server, run each of the evaluation tools against the safety trained model to establish a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca15ed7-8a04-4890-b4a6-1c350dea2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.vllm_launcher import VLLMLauncher\n",
    "\n",
    "vllm_launcher = VLLMLauncher(total_num_gpus=8)\n",
    "\n",
    "model_vllm_proc = vllm_launcher.launch(\n",
    "    model_name_or_path=MODEL_NAME_OR_PATH,\n",
    "    gpu_devices=os.environ['POLICY_MODEL_GPUS'],\n",
    "    served_model_name='test-model',\n",
    "    enable_reasoning=True,\n",
    "    log_filepath=f\"{LOG_DIR}/vllm-server-model.log\",\n",
    "    port=5000,\n",
    ")\n",
    "\n",
    "!sleep 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063ae8b-951c-4b2a-899d-4f842316e698",
   "metadata": {},
   "source": [
    "Check if the vLLM server has been correctly launched.\n",
    "\n",
    "Run the following code block. If you see the message like below, the server is ready to use.\n",
    "\n",
    "```\n",
    "INFO:     Started server process [<pid>]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6455ec-5d3e-4900-935b-59e8a7429815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the vLLM serve is correctly launched\n",
    "model_vllm_proc.print_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0023b9c-b0e0-49ea-82a2-4fd55c8fce01",
   "metadata": {},
   "source": [
    "In case you'd like to shutdown (to relaunch etc.) the vLLM server, run the following code block after uncommenting the command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb704e8c-e0bf-49a0-b2ea-07c43b44bc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_vllm_proc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0087a27",
   "metadata": {},
   "source": [
    "### Evaluating Content Safety with Aegis 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768204ca-ecb6-4735-9676-986dbfc46349",
   "metadata": {},
   "source": [
    "### Serve the NeMo Guard Model with vLLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327b9d5-5b90-4d04-a8ef-bd46ab80e4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(NEMOGUARD_MODEL_PATH):\n",
    "    print(f\"✅ NeMo Guard model found: {NEMOGUARD_MODEL_PATH}\")\n",
    "else:\n",
    "    raise ValueError(f\"❌ NeMo Guard model not found at {NEMOGUARD_MODEL_PATH}. Please go back to Step 0 and verify that the model was created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfee9eb-5b0f-4ed1-b881-13332edd6ee5",
   "metadata": {},
   "source": [
    "Then, you need to launch a vLLM server using the model. We also launch a vllm server for WildGuard, which will be covered later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12354d58-8e83-4450-ab04-7a58f12416d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch vLLM server for NeMo Guard model\n",
    "nemoguard_vllm_proc = vllm_launcher.launch(\n",
    "    model_name_or_path=NEMOGUARD_MODEL_PATH,\n",
    "    gpu_devices=os.environ['NEMOGUARD_MODEL_GPUS'],\n",
    "    served_model_name='llama-3.1-nemoguard-8b-content-safety',\n",
    "    log_filepath=f\"{LOG_DIR}/vllm-server-nemo-guard-model.log\",\n",
    "    port=6000,\n",
    ")\n",
    "\n",
    "# Launch vLLM server for WildGuard model\n",
    "WILDGUARD_MODEL_PATH = 'allenai/wildguard'\n",
    "wildguard_vllm_proc = vllm_launcher.launch(\n",
    "    model_name_or_path=WILDGUARD_MODEL_PATH,\n",
    "    gpu_devices=os.environ['WILDGUARD_MODEL_GPUS'],\n",
    "    served_model_name='allenai/wildguard',\n",
    "    log_filepath=f\"{LOG_DIR}/vllm-server-wildguard.log\",\n",
    "    port=7000,\n",
    ")\n",
    "\n",
    "!sleep 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baefd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the servers were properly launched\n",
    "print(\"Checking vLLM server for NeMo Guard\")\n",
    "nemoguard_vllm_proc.print_log()\n",
    "\n",
    "print(\"=====\\n\\n\")\n",
    "print(\"Checking vLLM server for WildGuard\")\n",
    "wildguard_vllm_proc.print_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e71853-21ef-4a54-8b25-04b35396fd74",
   "metadata": {},
   "source": [
    "After you start the server, run each of the evaluation tools against the base model to establish a performance baseline.\n",
    "\n",
    "The evaluation requires approximately 30 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0c85b6-932f-4bcd-b156-c74e880b50f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "aegis_overrides = os.environ[\"AEGIS_CONFIG_OVERRIDES\"]\n",
    "aegis_overrides += \",config.params.extra.judge.url=http://localhost:6000/v1\"  # Change this if you use a different endpoint\n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"0\":\n",
    "    aegis_overrides += \",config.params.limit_samples=200\"\n",
    "\n",
    "# Run evaluation \n",
    "!core-evals-safety-eval run_eval \\\n",
    "               --output_dir {AEGIS_V2_RESULTS_DIR} \\\n",
    "               --model_id \"test-model\" \\\n",
    "               --model_url http://localhost:5000/v1 \\\n",
    "               --model_type chat \\\n",
    "               --eval_type aegis_v2 \\\n",
    "               --overrides=\"{aegis_overrides}\" &> \"$LOG_DIR/safety-eval-aegis-v2-vllm.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f87f60c-f2f5-4e49-b96f-66b5934e4688",
   "metadata": {},
   "source": [
    "Check the log file `$LOG_DIR/safety-eval-aegis-v2.log` for progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0801d5f2",
   "metadata": {},
   "source": [
    "Let's take a look at the safety score and category breakdown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728592a4-932e-4f8b-9081-b9caf446b290",
   "metadata": {},
   "outputs": [],
   "source": [
    "aegis_v2_df = pd.read_csv(f\"{AEGIS_V2_RESULTS_DIR}/output.csv\")\n",
    "\n",
    "aegis_v2_label_s = aegis_v2_df[\"Response Safety 0\"].value_counts()\n",
    "aegis_v2_safety_score = aegis_v2_label_s.loc[\"safe\"] / (aegis_v2_label_s.loc[\"safe\"] + aegis_v2_label_s.loc[\"unsafe\"])\n",
    "\n",
    "print(f\"Aegis v2 safety score: {aegis_v2_safety_score*100:.2f}%\")\n",
    "print(\"===============================================\")\n",
    "print(\"The category distribution for unsafe responses:\")\n",
    "\n",
    "unsafe_aegis_v2_df = aegis_v2_df[aegis_v2_df[\"Response Safety 0\"] == \"unsafe\"]\n",
    "unsafe_aegis_v2_df[\"Safety Categories 0\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef86cf-10ec-4ec2-8f3b-9e739be909ce",
   "metadata": {},
   "source": [
    "### Evaluating Content Safety with WildGuard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e940a9df-9918-49b1-9800-4086b7812bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "wildguard_overrides = os.environ[\"WILDGUARD_CONFIG_OVERRIDES\"]\n",
    "wildguard_overrides += \",config.params.extra.judge.url=http://localhost:7000/v1\"  # Change this if you use a different endpoint\n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"0\":\n",
    "    wildguard_overrides += \",config.params.limit_samples=200\"\n",
    "   \n",
    "# Run evaluation\n",
    "!core-evals-safety-eval run_eval \\\n",
    "               --output_dir {WILDGUARD_RESULTS_DIR} \\\n",
    "               --model_id \"test-model\" \\\n",
    "               --model_url http://localhost:5000/v1 \\\n",
    "               --model_type chat \\\n",
    "               --eval_type wildguard \\\n",
    "               --overrides=\"{wildguard_overrides}\" &> \"$LOG_DIR/safety-eval-wildguard-vllm.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8282b480",
   "metadata": {},
   "source": [
    "Check the `$LOG_DIR/safety-eval-wildguard-vllm.log` file for progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeb1ddb-b759-4f96-8a71-c2031debecb3",
   "metadata": {},
   "source": [
    "When the evaluation completes, determine the Wildguard ratio.\n",
    "This notebook uses the ratio of safe responses to total responses as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a8213d-93d6-4a35-a6bb-50093398c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wildguard_results = json.load(open(f\"{WILDGUARD_RESULTS_DIR}/metrics.json\"))\n",
    "wildguard_safe_score = wildguard_results[\"safe\"] / (wildguard_results[\"safe\"] + wildguard_results[\"unsafe\"])\n",
    "\n",
    "print(f\"WildGuard safety score: {wildguard_safe_score * 100.:.2f}%\")\n",
    "print(\"===============================================\")\n",
    "print(\"The category distribution for unsafe responses:\")\n",
    "wildguard_df = pd.DataFrame([json.loads(x) for x in open(f\"{WILDGUARD_RESULTS_DIR}/report.json\")])\n",
    "wildguard_unsafe_df = wildguard_df[wildguard_df[\"wildguard_response_safety_classification\"] == \"unsafe\"]\n",
    "wildguard_unsafe_df[\"subcategory\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a410d3-a133-4137-bf4e-a230be79f2c6",
   "metadata": {},
   "source": [
    "### Shutting down guard models and re-launching vLLM server\n",
    "\n",
    "Now you can shutdown the vLLM servers for the two guard models and fully use all the GPUs for the target model for more efficient evaluaton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba3ce66-3e16-442c-bb25-978a39419502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the running vllm processes\n",
    "vllm_launcher.stop_all()\n",
    "\n",
    "!sleep 10\n",
    "\n",
    "print(\"Starting policy model server...\")\n",
    "model_vllm_full_proc = vllm_launcher.launch(\n",
    "    model_name_or_path=MODEL_NAME_OR_PATH,\n",
    "    gpu_devices=os.environ['POLICY_MODEL_GPUS_FULL'],\n",
    "    served_model_name='test-model',\n",
    "    enable_reasoning=True,\n",
    "    log_filepath=f\"{LOG_DIR}/vllm-server-model.log\",\n",
    "    port=5000\n",
    ")    \n",
    "\n",
    "!sleep 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee247b-581b-46b1-b5d7-0c4abae37b19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the vLLM serve is correctly launched\n",
    "model_vllm_full_proc.print_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dca2b8c",
   "metadata": {},
   "source": [
    "### Evaluating Product Security with Garak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c355d522-4d4e-409d-b4d6-5381fb29973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"1\":\n",
    "    GARAK_SAMPLE_SIZE_CONFIG = \"\"    \n",
    "else:\n",
    "    GARAK_SAMPLE_SIZE_CONFIG = \"--target_probes tier1\"\n",
    "\n",
    "# Run evaluation\n",
    "!python scripts/run_garak.py --output_basedir {GARAK_RESULTS_DIR} --base_config_path ./configs/garak_base_config.yaml --max_workers 4 {GARAK_SAMPLE_SIZE_CONFIG}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac810d3f",
   "metadata": {},
   "source": [
    "garak stores the results of each probe individually and produces an HTML report with the success and failure rate for each probe.\n",
    "Aggregate and summarize the scan results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe885ef6-6581-4317-96df-b54d7c7b6c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = os.path.join(GARAK_REPORT_DIR, \"garak_results.csv\")\n",
    "garak_df = pd.read_csv(output_csv)\n",
    "garak_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8113148-8dac-4c44-957b-00cee5b85c97",
   "metadata": {},
   "source": [
    "Let's take a look at `z_score`---a score calibrated using reference models. Negative z scores indicate vulnerability in the corresponding aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a46985-9046-45a3-80d1-b0407ec15119",
   "metadata": {},
   "outputs": [],
   "source": [
    "garak_df[garak_df[\"z_score\"] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4c35ad",
   "metadata": {},
   "source": [
    "The Z-score is a measure of how many standard deviations the model performs from the mean.\n",
    "The mean is periodically calculated by garak developers from a _bag of models_ that represent state-of-the-art models at the time.\n",
    "For more information about the models and the statistics, refer to [Intepreting results with a bag of models](https://github.com/NVIDIA/garak/blob/main/garak/data/calibration/bag.md) in the garak repository on GitHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d696ce49-322a-4601-a546-ba776d197c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"excellent\", \"competitive\", \"average\", \"below average\", \"poor\"]\n",
    "garak_label_df = garak_df[\"z_score_status\"].value_counts().reindex(labels)\n",
    "garak_resilience_score = garak_label_df[[\"excellent\", \"competitive\"]].sum() / garak_label_df.sum()\n",
    "print(f\"Garak resilience score: {garak_resilience_score*100.:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfabab9-4c00-4dd8-8b9c-031ee2ca973b",
   "metadata": {},
   "source": [
    "## Accuracy evaluation using NeMo Framework\n",
    "\n",
    "Some benchmarks use endpoints provided by build.nvidia.com for answer extraction. If your evaluaiton failed and see error messages like below, please visit [build.nvidia.com](https://build.nvidia.com/) and contact support.\n",
    "\n",
    "```\n",
    "Retry attempt 1/25 due to: ClientResponseError: 500, message='Internal Server Error', url='https://integrate.api.nvidia.com/v1/chat/completions'\n",
    "```\n",
    "\n",
    "\n",
    "### Evaluating Accuracy  with GPQA-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8fa26-b550-4762-8418-a0f48bf1c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation \n",
    "gpqad_overrides = os.environ[\"GPQAD_CONFIG_OVERRIDES\"]\n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"0\":\n",
    "    gpqad_overrides += \",config.params.limit_samples=50\"\n",
    "\n",
    "# Run evaluation\n",
    "!core_evals_simple_evals run_eval \\\n",
    "      --model_id 'test-model' \\\n",
    "      --model_url http://localhost:5000/v1/chat/completions \\\n",
    "      --eval_type gpqa_diamond \\\n",
    "      --output_dir {GPQA_DIAMOND_RESULTS_DIR} \\\n",
    "      --overrides=\"{gpqad_overrides}\" &> \"$LOG_DIR/core-evals-simple-evals-gpqa_diamond.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b94688-153d-4f48-9195-36ae3cdc9eb5",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy with MATH-500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad8d86b-a4ab-4ed7-8513-0b647efc64d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or partial evaluation\n",
    "aa_math_500_overrides = os.environ[\"AA_MATH_500_CONFIG_OVERRIDES\"]\n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"0\":\n",
    "    aa_math_500_overrides += \",config.params.limit_samples=100\"\n",
    "\n",
    "# Run evaluation\n",
    "!core_evals_simple_evals run_eval \\\n",
    "    --eval_type AA_math_test_500 \\\n",
    "    --model_id test-model \\\n",
    "    --model_type chat \\\n",
    "    --model_url http://localhost:5000/v1/chat/completions \\\n",
    "    --output_dir {AA_MATH_500_RESULTS_DIR} \\\n",
    "    --overrides \"{aa_math_500_overrides}\" &> \"$LOG_DIR/core-evals-simple-evals-aa-math-500.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d3325f-efb4-4571-9fbe-b6552f4bedda",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy with IFEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b956872-2e01-4997-8fd7-ca628fd464be",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifeval_overrides = os.environ[\"IFEVAL_CONFIG_OVERRIDES\"]\n",
    "# Full or partial evaluation \n",
    "if os.environ[\"RUN_FULL_EVAL\"] == \"0\":\n",
    "    ifeval_overrides += \",config.params.limit_samples=100\"\n",
    "\n",
    "# Run evaluation\n",
    "!core_evals_lm_eval run_eval \\\n",
    "    --eval_type ifeval \\\n",
    "    --model_id test-model \\\n",
    "    --model_type chat \\\n",
    "    --model_url http://localhost:5000/v1/chat/completions \\\n",
    "    --output_dir {IFEVAL_RESULTS_DIR} \\\n",
    "    --overrides \"{ifeval_overrides}\" &> \"$LOG_DIR/core-eval-lm-eval-ifeval.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847456dc",
   "metadata": {},
   "source": [
    "Shutdown the vLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fdd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Terminate the running vllm process\n",
    "vllm_launcher.stop_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedec79-5b89-48fe-b3e5-6fd970395888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "gpqa_diamond_score = None\n",
    "aa_math500_score = None\n",
    "ifeval_prompt_strict_score = None\n",
    "ifeval_inst_strict_score = None\n",
    "\n",
    "if os.path.exists(f\"{GPQA_DIAMOND_RESULTS_DIR}/gpqa_diamond.json\"):\n",
    "    gpqa_diamond_score = json.load(open(f\"{GPQA_DIAMOND_RESULTS_DIR}/gpqa_diamond.json\"))[\"score\"]\n",
    "\n",
    "if os.path.exists(f\"{AA_MATH_500_RESULTS_DIR}/AA_math_test_500.json\"):\n",
    "    aa_math500_score = json.load(open(f\"{AA_MATH_500_RESULTS_DIR}/AA_math_test_500.json\"))[\"score\"]\n",
    "\n",
    "pattern = os.path.join(IFEVAL_RESULTS_DIR, \"test-model\", \"results_*.json\")\n",
    "match_files = glob.glob(pattern)\n",
    "if len(match_files) > 0:\n",
    "    ifeval_results = json.load(open(match_files[0]))\n",
    "    ifeval_prompt_strict_score = ifeval_results[\"results\"][\"ifeval\"][\"prompt_level_strict_acc,none\"]\n",
    "    ifeval_inst_strict_score = ifeval_results[\"results\"][\"ifeval\"][\"inst_level_strict_acc,none\"]\n",
    "\n",
    "accuracy_s = pd.Series(\n",
    "    {\"gpqa_diamond\": gpqa_diamond_score,\n",
    "     \"aa_math_500\": aa_math500_score,\n",
    "     \"ifeval_prompt_strict\": ifeval_prompt_strict_score,\n",
    "     \"ifeval_inst_strict\": ifeval_inst_strict_score})\n",
    "accuracy_s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef89dd62-3f3f-4877-8d48-8df4d1afaccc",
   "metadata": {},
   "source": [
    "## Create a Report Card\n",
    "\n",
    "Now we have two set of evaluation results. Let's compare the safety and accuracy of the same model before and after safety post-training.\n",
    "Run the following code blocks to launch a server for visual analysis.\n",
    "\n",
    "You'll see a message like below. Go to Brev and on the instance page,\n",
    "\n",
    "```\n",
    "Access > Using Secure Links > Share a Service\n",
    "```\n",
    "\n",
    "Then, add 8501 so you can access to the port from your local machine.\n",
    "Open the browser and you'll be able to see the Report Card Dashboard.\n",
    "\n",
    "With this method, you need to access it through the secure link instead of the direct link shown in the message. Opening the port to the public is not recommended, as it can pose security risks.\n",
    "\n",
    "```\n",
    "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
    "\n",
    "\n",
    "  You can now view your Streamlit app in your browser.\n",
    "\n",
    "  Local URL: http://localhost:8501\n",
    "  Network URL: http://<IP address>:8501\n",
    "  External URL: http://<IP address>:8501\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556f619e-d2a3-492f-a021-2021821d71af",
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_MODEL_OUTPUT_DIR = f\"{BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B/\"\n",
    "TRAINED_MODEL_OUTPUT_DIR = f\"{BASE_DIR}/results/DeepSeek-R1-Distill-Llama-8B-Safety-Trained/\"\n",
    "\n",
    "!streamlit run scripts/app.py -- {ORIGINAL_MODEL_OUTPUT_DIR} {TRAINED_MODEL_OUTPUT_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df04d3-15fa-492f-8917-31f358435003",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "The series of notebooks walked you through and learn the following topics, leveraging NVIDIA's NeMo Framework Evaluation and NeMo-RL.\n",
    "\n",
    "- [Notebook 1]() Safety and Accuracy Evaluation using NeMo Eval\n",
    "- [Notebook 2]() Safety Post-training using Safety for Agentic AI's training recipe\n",
    "- [Notebook 3]() Re-running the same safety and accuracy evaluation to understand how the model has improved"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
