{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e26fe745",
   "metadata": {},
   "source": [
    "\n",
    "Notebook 2: Dataset Preparation & Safety Training\n",
    "\n",
    "In this notebook, we will prepare the dataset for safety training and train the model to be more robust against harmful content attacks.\n",
    "\n",
    "\n",
    "The safety training dataset consists of safety training data to improve the safety of the model and \n",
    "post-training dataset to retrain the accuracy of the model.\n",
    "\n",
    "The safety training dataset is a curated dataset of safe and unsafe prompts, collected from the following sources:\n",
    "\n",
    "- Aegis v2\n",
    "- TODO\n",
    "- TODO\n",
    "- TODO\n",
    "\n",
    "We sample general training data from the [Llama-Nemotron-Post-Trainin-Dataset](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset)\n",
    "\n",
    "# Preparation\n",
    "\n",
    "You need to add `HF_TOKEN` and `WANDB_API_KEY` to the environment variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "import shutil\n",
    "\n",
    "# Base directory and configuration\n",
    "BASE_DIR = \"/lustre/fsw/portfolios/llmservice/users/ahazare\"\n",
    "LOG_DIR = \"/lustre/fsw/portfolios/llmservice/users/ahazare/gtc_paris/logs\"\n",
    "SAFETY_DATASET_NAME = \"nvidia/Nemotron-Safety-Training-Dataset\"\n",
    "POST_TRAINING_DATASET_NAME = \"nvidia/Llama-Nemotron-Post-Training-Dataset\"\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "SAFETY_MODEL_NAME = \"/lustre/fsw/portfolios/llmservice/users/ahazare/cache/huggingface/llama-3.1-nemoguard-8b-content-safety\"\n",
    "\n",
    "# Credentials\n",
    "os.environ.update({\n",
    "    \"HF_TOKEN\":\"<Add your HF_TOKEN>\",\n",
    "    \"WANDB_API_KEY\": \"<Add your WANDB_API_KEY>\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37424ce5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "os.environ.update({\n",
    "    'TMPDIR': f\"{BASE_DIR}/tmp\",\n",
    "    'XDG_CACHE_HOME': f\"{BASE_DIR}/cache\",\n",
    "    'HF_HOME': f\"{BASE_DIR}/cache/huggingface\",\n",
    "    'UV_CACHE_DIR': f\"{BASE_DIR}/cache/uv\",\n",
    "    'TRITON_CACHE_DIR': f\"{BASE_DIR}/cache/triton\",\n",
    "    'DATASET_CACHE_DIR': f\"{BASE_DIR}/dataset_cache\",\n",
    "    'RAY_TMPDIR': \"/tmp/ray_ahazare\",\n",
    "    'LOG_DIR': f\"{LOG_DIR}\",\n",
    "\n",
    "})\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [os.environ['TMPDIR'], os.environ['XDG_CACHE_HOME'], os.environ['HF_HOME'],\n",
    "                 os.environ['UV_CACHE_DIR'],os.environ['TRITON_CACHE_DIR'], os.environ['DATASET_CACHE_DIR'], \n",
    "                 os.environ['RAY_TMPDIR'], os.environ['LOG_DIR']]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f52c5a",
   "metadata": {},
   "source": [
    "\n",
    "# Step 1-a: Create Safety Training Data\n",
    "\n",
    "We download the safety training dataset and the post-training dataset.\n",
    "\n",
    "TODO: safety training dataset creation should be done in a separate script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d94ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download NV Safety Dataset\n",
    "result = subprocess.run([\n",
    "    'python3', 'download_safety_dataset.py',\n",
    "    '--dataset_name', SAFETY_DATASET_NAME,\n",
    "    '--filename', 'nemotron-safety-sft-training-blend-v1.0.jsonl',\n",
    "    '--total_samples', '1000',\n",
    "    '--cache_dir', os.environ['DATASET_CACHE_DIR']\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66a8900",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "# Step 1-b: Download Post-Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Download Llama Nemotron Post-training Dataset\n",
    "\n",
    "files = [\n",
    "    \"SFT/math/math_v1.1.jsonl\",\n",
    "    \"SFT/code/code_v1.1.jsonl\",\n",
    "    \"SFT/chat/chat.jsonl\",\n",
    "    \"SFT/science/science.jsonl\"\n",
    "]\n",
    "\n",
    "LLAMA_NEMO_DIR = f\"{os.environ['DATASET_CACHE_DIR']}/Llama-Nemotron-Post-Training-Dataset\"\n",
    "Path(LLAMA_NEMO_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for file in files:\n",
    "    print(f\"Downloading {file}...\")\n",
    "    downloaded_path = hf_hub_download(\n",
    "        repo_id=POST_TRAINING_DATASET_NAME,\n",
    "        filename=file,\n",
    "        repo_type='dataset',\n",
    "        cache_dir=os.environ['DATASET_CACHE_DIR']\n",
    "    )\n",
    "    \n",
    "    filename = Path(file).name\n",
    "    target_path = f\"{LLAMA_NEMO_DIR}/{filename}\"\n",
    "    \n",
    "    # Count lines and sample\n",
    "    with open(downloaded_path, 'r') as f:\n",
    "        total_lines = sum(1 for _ in f)\n",
    "    \n",
    "    print(f\"Total lines in file: {total_lines}\")\n",
    "    \n",
    "    if total_lines > 1000:\n",
    "        # Use shuf for random sampling\n",
    "        subprocess.run(['shuf', '-n', '1000', downloaded_path], stdout=open(target_path, 'w'), check=True)\n",
    "        print(f\"Extracted 1000 random samples to {target_path}\")\n",
    "    else:\n",
    "        shutil.copy2(downloaded_path, target_path)\n",
    "        print(f\"File has fewer than 1000 lines, copied all {total_lines} lines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da0b364",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 3. Combine datasets\n",
    "OUTPUT_DIR = f\"{os.environ['DATASET_CACHE_DIR']}/sft_data\"\n",
    "subprocess.run([\n",
    "    'python3', 'combine_datasets.py',\n",
    "    '--safety_file', f\"{os.environ['DATASET_CACHE_DIR']}/nv_safety_sampled.jsonl\",\n",
    "    '--llama_nemo_dir', LLAMA_NEMO_DIR,\n",
    "    '--output_dir', OUTPUT_DIR,\n",
    "    '--val_split', '0.03',\n",
    "    '--max_tokens', '16384',\n",
    "    '--max_samples', '5000'\n",
    "], check=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be7568",
   "metadata": {},
   "source": [
    "\n",
    "# Step 2: On-policy Data Generation\n",
    "\n",
    "The key idea of the safety training data is to generate on-policy data with the target model for the safety and post-training datasets.dataset_type\n",
    "To generate the on-policy data, we need to start the vLLM servers for the target model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab691e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Start vLLM servers\n",
    "os.environ.update({\n",
    "    'VLLM_ENGINE_ITERATION_TIMEOUT_S': '36000',\n",
    "    'VLLM_ALLOW_LONG_MAX_MODEL_LEN': '1',\n",
    "    'VLLM_HOST': '0.0.0.0',\n",
    "    'VLLM_TENSOR_PARALLEL_SIZE': '1',\n",
    "    'POLICY_MODEL_GPUS': '0,1,2,3',\n",
    "    'SAFETY_MODEL_GPUS': '4,5'\n",
    "})\n",
    "\n",
    "print(\"Starting policy model server...\")\n",
    "policy_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', MODEL_NAME,\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '5000',\n",
    "    '--served-model-name', 'test-model',\n",
    "    '--enable-reasoning', \n",
    "    '--reasoning-parser', 'deepseek_r1',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['POLICY_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-model.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)\n",
    "\n",
    "print(\"Starting safety model server...\")\n",
    "safety_server = subprocess.Popen([\n",
    "    'python3', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', SAFETY_MODEL_NAME,\n",
    "    '--trust-remote-code',\n",
    "    '--seed', '1',\n",
    "    '--host', os.environ['VLLM_HOST'],\n",
    "    '--port', '6000',\n",
    "    '--served-model-name', 'safety-model',\n",
    "    '--tensor-parallel-size', os.environ['VLLM_TENSOR_PARALLEL_SIZE'],\n",
    "    '--download-dir', os.environ['HF_HOME']\n",
    "], env={**os.environ, 'CUDA_VISIBLE_DEVICES': os.environ['SAFETY_MODEL_GPUS']},\n",
    "   stdout=open(f\"{LOG_DIR}/vllm-server-safety.log\", 'w'),\n",
    "   stderr=subprocess.STDOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d85b4d",
   "metadata": {},
   "source": [
    "\n",
    "To terminate the launched vLLM servers, you can run the following command:\n",
    "\n",
    "```python\n",
    "subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9ce651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup vLLM servers\n",
    "# subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ca358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_on_policy_data(input_dataset, output_file, log_file, model_name, safety_model, hf_token, \n",
    "                            vllm_host=\"0.0.0.0\", vllm_model_port=5000, vllm_safety_port=6000,\n",
    "                            batch_size=32, max_tokens=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"\n",
    "    Generate on-policy data using the specified model and parameters.\n",
    "    \n",
    "    Args:\n",
    "        input_dataset (str): Path to input dataset file\n",
    "        output_file (str): Path to output file\n",
    "        log_file (str): Path to log file\n",
    "        model_name (str): Name of the model to use\n",
    "        safety_model (str): Path to safety model\n",
    "        hf_token (str): HuggingFace token\n",
    "        vllm_host (str): vLLM host address\n",
    "        vllm_model_port (int): Port for model server\n",
    "        vllm_safety_port (int): Port for safety server\n",
    "        batch_size (int): Batch size for generation\n",
    "        max_tokens (int): Maximum tokens to generate\n",
    "        temperature (float): Temperature for generation\n",
    "        top_p (float): Top-p sampling parameter\n",
    "    \n",
    "    Returns:\n",
    "        subprocess.Popen: Process object for the generation\n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    print(f\"Generating responses and safety predictions...\")\n",
    "    print(f\"Input dataset: {input_dataset}\")\n",
    "    print(f\"Output file: {output_file}\")\n",
    "    \n",
    "    with open(log_file, 'w') as log:\n",
    "        process = subprocess.Popen([\n",
    "            'python3', 'generate_on_policy_data.py',\n",
    "            '--model_name', model_name,\n",
    "            '--safety_model', safety_model,\n",
    "            '--huggingface_token', hf_token,\n",
    "            '--vllm_host', vllm_host,\n",
    "            '--vllm_model_port', str(vllm_model_port),\n",
    "            '--vllm_safety_port', str(vllm_safety_port),\n",
    "            '--input_dataset', input_dataset,\n",
    "            '--output', output_file,\n",
    "            '--batch_size', str(batch_size),\n",
    "            '--max_tokens', str(max_tokens),\n",
    "            '--temperature', str(temperature),\n",
    "            '--top_p', str(top_p)\n",
    "        ], stdout=log, stderr=subprocess.STDOUT)\n",
    "    \n",
    "    return process\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# For training set\n",
    "train_process = generate_on_policy_data(\n",
    "    input_dataset=f\"{OUTPUT_DIR}/train.jsonl\",\n",
    "    output_file=f\"{OUTPUT_DIR}/train_on_policy_data.jsonl\",\n",
    "    log_file=f\"{BASE_DIR}/generation_train.log\",\n",
    "    model_name=MODEL_NAME,\n",
    "    safety_model=SAFETY_MODEL_NAME,\n",
    "    hf_token=HF_TOKEN\n",
    ")\n",
    "train_process.wait()\n",
    "\n",
    "# For validation set\n",
    "val_process = generate_on_policy_data(\n",
    "    input_dataset=f\"{OUTPUT_DIR}/val.jsonl\",\n",
    "    output_file=f\"{OUTPUT_DIR}/val_on_policy_data.jsonl\",\n",
    "    log_file=f\"{BASE_DIR}/generation_val.log\",\n",
    "    model_name=MODEL_NAME,\n",
    "    safety_model=SAFETY_MODEL_NAME,\n",
    "    hf_token=HF_TOKEN\n",
    ")\n",
    "val_process.wait()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f64872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill on-policy\n",
    "subprocess.run(['pkill', '-f', 'generate_on_policy_data.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a833d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{OUTPUT_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0b511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Generate on-policy data\n",
    "\n",
    "# On-Policy Data Generation parameters\n",
    "SAFETY_THRESHOLD = 0.8\n",
    "CONCURRENCY = 16\n",
    "MAX_ATTEMPTS = 3\n",
    "BATCH_SIZE = 64\n",
    "MAX_TOKENS = 512\n",
    "TEMPERATURE = 0.7\n",
    "TOP_P = 0.9\n",
    "\n",
    "print(\"Generating on-policy data...\")\n",
    "for dataset_type in ['train', 'val']:\n",
    "    input_dataset = f\"{OUTPUT_DIR}/{dataset_type}.jsonl\"\n",
    "    output_file = f\"{OUTPUT_DIR}/{dataset_type}_on_policy_data.jsonl\"\n",
    "    DATASET_TYPE = dataset_type\n",
    "    subprocess.run([\n",
    "        'python3', 'generate_on_policy_data.py',\n",
    "        '--model_name', MODEL_NAME,\n",
    "        '--safety_model', SAFETY_MODEL_NAME,\n",
    "        '--huggingface_token', os.environ['HF_TOKEN'],\n",
    "        '--vllm_host', os.environ['VLLM_HOST'],\n",
    "        '--vllm_model_port', '5000',\n",
    "        '--vllm_safety_port', '6000',\n",
    "        '--concurrency', str(CONCURRENCY),\n",
    "        '--input_dataset', input_dataset,\n",
    "        '--output', output_file,\n",
    "        '--batch_size', str(BATCH_SIZE),\n",
    "        '--max_tokens', str(MAX_TOKENS),\n",
    "        '--temperature', str(TEMPERATURE),\n",
    "        '--top_p', str(TOP_P)\n",
    "    ], stdout=open(f\"{LOG_DIR}/{DATASET_TYPE}_on-policy.log\", 'w'),\n",
    "                   stderr=subprocess.STDOUT)\n",
    "\n",
    "# Cleanup vLLM servers\n",
    "# subprocess.run(['pkill', '-f', 'vllm.entrypoints.openai.api_server'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -aux | grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed877bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "\n",
    "async def single_request(session, payload, base_url=\"http://0.0.0.0:5000/v1/chat/completions\"):\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    async with session.post(base_url, headers=headers, json=payload) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def query_server():\n",
    "    concurrency = 4\n",
    "    parallel_requests = []\n",
    "\n",
    "    async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=3600)) as session:\n",
    "        for i in range(8):\n",
    "            payload = {\n",
    "                \"model\": \"test-model\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": f\"What is {i} + 1?\"}],\n",
    "                \"temperature\": 0.6,\n",
    "                \"max_tokens\": 32768,\n",
    "                \"top_p\": 0.95,\n",
    "                \"n\": 1,\n",
    "            }\n",
    "            parallel_requests.append(asyncio.create_task(single_request(session, payload)))\n",
    "\n",
    "            if len(parallel_requests) >= concurrency:\n",
    "                responses = await asyncio.gather(*parallel_requests)\n",
    "                parallel_requests = []\n",
    "                print(len(responses))\n",
    "                print(responses)\n",
    "\n",
    "        # handle any remaining tasks\n",
    "        if parallel_requests:\n",
    "            responses = await asyncio.gather(*parallel_requests)\n",
    "            print(len(responses))\n",
    "            print(responses)\n",
    "\n",
    "await query_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1361081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d3015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfb9489",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bb1144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b727f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e6943d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25eac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 clients.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322722aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe7a01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6e0692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23688df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbefa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../dataset_cache/sft_data/train_on_policy_data.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2981d5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_json('../dataset_cache/sft_data/train.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441b4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e7de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['safety_metrics'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22147c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01cd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c16c283",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 clients.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Run SFT\n",
    "print(\"Running SFT...\")\n",
    "os.chdir(f\"{BASE_DIR}/NeMo-RL\")\n",
    "# Set up model directory environment variable\n",
    "MODEL_DIR = f\"{BASE_DIR}/NeMo-RL/results/sft_deepseek_8b_trial_step_300\"\n",
    "\n",
    "subprocess.run(['uv', 'run', 'python', 'examples/run_sft.py', \n",
    "                '--config', f\"{BASE_DIR}/gtc_paris/deepseek_sft.yaml\"\n",
    "               ], \n",
    "               env={**os.environ, 'TMPDIR': os.environ['RAY_TMPDIR']},\n",
    "               stdout=open(f\"{MODEL_DIR}/sft.log\", 'w'),\n",
    "               check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2900aa62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4396a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Convert checkpoint\n",
    "MODEL_DIR = f\"{BASE_DIR}/NeMo-RL/results/sft_deepseek_8b_trial_step_300/step_200\"\n",
    "DCP_CKPT_PATH = f\"{MODEL_DIR}/policy/weights/\"\n",
    "CONFIG_PATH = f\"{MODEL_DIR}/config.yaml\"\n",
    "HF_CKPT_PATH = f\"{MODEL_DIR}/hf_ckpt\"\n",
    "\n",
    "print(\"Converting checkpoint...\")\n",
    "os.chdir(f\"{BASE_DIR}/NeMo-RL\")\n",
    "subprocess.run([\n",
    "    'uv', 'run', 'examples/convert_dcp_to_hf.py',\n",
    "    '--config', CONFIG_PATH,\n",
    "    '--dcp-ckpt-path', DCP_CKPT_PATH,\n",
    "    '--hf-ckpt-path', HF_CKPT_PATH\n",
    "], check=True)\n",
    "\n",
    "# Verify conversion\n",
    "if Path(f\"{HF_CKPT_PATH}/pytorch_model.bin\").exists() and Path(f\"{HF_CKPT_PATH}/config.json\").exists():\n",
    "    print(\"Conversion successful!\")\n",
    "    print(f\"The HuggingFace model is now available at: {HF_CKPT_PATH}\")\n",
    "else:\n",
    "    print(\"Conversion may have failed. Please check the output.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b8f07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (penv3)",
   "language": "python",
   "name": "penv3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
