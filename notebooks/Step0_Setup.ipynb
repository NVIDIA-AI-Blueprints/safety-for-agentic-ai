{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Setup\n",
    "\n",
    "In this initial step, we prepare the environment for running the notebook by installing required packages and configuring vLLM with a custom reasoning parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages\n",
    "\n",
    "This section installs the necessary dependencies for running vLLM and other components used throughout the notebook. In particular, we:\n",
    "\n",
    "- Install a specific version of vLLM that is compatible with our custom modifications.\n",
    "- Install additional libraries needed for result visualization.\n",
    "- Add a custom reasoning parser to vLLM using the `scripts/setup_parser.sh` script. This enables the model to support advanced reasoning capabilities, such as structured intermediate outputs or special token handling used in agentic workflows.\n",
    "\n",
    "\n",
    "⚠️ **Note**: If you're running this notebook on a clean machine or fresh container, this setup step is required before executing any inference or evaluation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q streamlit==1.45.1 > /tmp/package-installation.log 2>&1\n",
    "\n",
    "# Install custom reasoning parser\n",
    "!bash scripts/setup_parser.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure API Keys and Evaluation Setting\n",
    "\n",
    "Run the following code block and add your API keys that are required for the notebooks.\n",
    "Alternatvely, you can directly edit `.env` and add the information information there. Other environment variables will be automatically added.\n",
    "\n",
    "```\n",
    "# .env\n",
    "HF_TOKEN=<Your HF_TOKEN>\n",
    "JUDGE_API_KEY=<Your NVIDIA_API_KEY>\n",
    "WANDB_API_KEY=<Your WANDB_API_KEY>\n",
    "RUN_FULL_EVAL=0 or 1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Load API keys from .env\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "\n",
    "# Check HF_TOKEN\n",
    "if \"HF_TOKEN\" in os.environ:\n",
    "    HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "else:\n",
    "    HF_TOKEN = getpass(\"Enter your HF_TOKEN: \")\n",
    "\n",
    "# Check NVIDIA_API_KEY\n",
    "if \"JUDGE_API_KEY\" in os.environ:\n",
    "    JUDGE_API_KEY = os.environ[\"JUDGE_API_KEY\"]\n",
    "else:\n",
    "    JUDGE_API_KEY = getpass(\"Enter your NVIDIA API KEY (used for Judge API): \")\n",
    "\n",
    "# Check WANDB_API_KEY\n",
    "if \"WANDB_API_KEY\" in os.environ:\n",
    "    WANDB_API_KEY = os.environ[\"WANDB_API_KEY\"]\n",
    "else:\n",
    "    WANDB_API_KEY = getpass(\"Enter your WANDB_API_KEY (press Enter to skip): \")\n",
    "    if not WANDB_API_KEY:\n",
    "        print(\"WANDB_API_KEY not provided. Skipping wandb integration.\")\n",
    "        # Update the config file for SFT to skip W&B logging\n",
    "        with open(\"configs/deepseek_sft.yaml\", \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        config[\"logger\"][\"wandb_enabled\"] = False\n",
    "        with open(\"configs/deepseek_sft.yaml\", \"w\") as f:\n",
    "            yaml.dump(config, f, default_flow_style=False)        \n",
    "\n",
    "# Check WANDB_API_KEY\n",
    "if \"RUN_FULL_EVAL\" in os.environ:\n",
    "    RUN_FULL_EVAL = os.environ[\"RUN_FULL_EVAL\"]\n",
    "else:\n",
    "    RUN_FULL_EVAL = getpass(\"Run full evaluation? (No=0 or Yes=1): \")\n",
    "    if RUN_FULL_EVAL not in [\"0\", \"1\"]:\n",
    "        print(\"Invalid input was provided. The value must be 0 or 1. Set it to 0.\")\n",
    "        RUN_FUL_EVAL = 0\n",
    "\n",
    "# Add python path to the script directory\n",
    "if \"PYTHONPATH\" in os.environ:\n",
    "    target_path = \"/ephemeral/workspace/safety-for-agentic-ai/notebooks/scripts\"\n",
    "    if target_path not in os.environ[\"PYTHONPATH\"]:\n",
    "        PYTHONPATH = os.environ[\"PYTHONPATH\"] + f\":{target_path}\"\n",
    "    else:\n",
    "        PYTHONPATH = os.environ[\"PYTHONPATH\"]\n",
    "else:\n",
    "    PYTHONPATH = \"/ephemeral/workspace/safety-for-agentic-ai/notebooks/scripts\"\n",
    "\n",
    "BASE_DIR = \"/ephemeral/workspace\"\n",
    "TMPDIR = \"/tmp\"\n",
    "XDG_CACHE_HOME = f\"{BASE_DIR}/cache\"\n",
    "HF_HOME = f\"{BASE_DIR}/cache/huggingface\"\n",
    "UV_CACHE_DIR = f\"{BASE_DIR}/cache/uv\"\n",
    "TRITON_CACHE_DIR = f\"{BASE_DIR}/cache/triton\"\n",
    "DATASET_CACHE_DIR = f\"{BASE_DIR}/dataset_cache\"\n",
    "RAY_TMPDIR = \"/tmp/ray\"\n",
    "\n",
    "# This Developer Blueprint requires and has been tested with 8x H100 or A100 GPUs.\n",
    "# If using fewer GPUs, update the following environment variables accordingly.\n",
    "POLICY_MODEL_GPUS = \"0,1,2,3\"\n",
    "POLICY_MODEL_GPUS_FULL = \"0,1,2,3,4,5,6,7\"\n",
    "NEMOGUARD_MODEL_GPUS = \"4,5\"\n",
    "WILDGUARD_MODEL_GPUS = \"6,7\"\n",
    "SAFETY_MODEL_GPUS = \"4,5,6,7\"\n",
    "\n",
    "# vLLM settings\n",
    "VLLM_ENGINE_ITERATION_TIMEOUT_S = \"36000\"\n",
    "VLLM_ALLOW_LONG_MAX_MODEL_LEN = \"1\"\n",
    "VLLM_HOST = \"0.0.0.0\"\n",
    "\n",
    "# * Evaluation configuration\n",
    "# Accuracy benchmarks\n",
    "EVAL_CONFIG_OVERRIDES = \"target.api_endpoint.type=chat,config.params.temperature=0.6,config.params.top_p=0.95,config.params.max_new_tokens=12288,config.params.max_retries=5,config.params.parallelism=4,config.params.request_timeout=600\"\n",
    "GPQAD_CONFIG_OVERRIDES = AA_MATH_500_CONFIG_OVERRIDES = IFEVAL_CONFIG_OVERRIDES = EVAL_CONFIG_OVERRIDES\n",
    "\n",
    "# Safety benchmarks\n",
    "AEGIS_CONFIG_OVERRIDES = \"config.params.max_new_tokens=8192,config.params.parallelism=10,config.params.extra.judge.parallelism=10\"\n",
    "WILDGUARD_CONFIG_OVERRIDES = \"config.params.max_new_tokens=8192,config.params.parallelism=10,config.params.extra.judge.parallelism=10\"\n",
    "\n",
    "env_var_names = [\n",
    "    \"HF_TOKEN\", \"JUDGE_API_KEY\", \"WANDB_API_KEY\", \"RUN_FULL_EVAL\",\n",
    "    \"PYTHONPATH\", \"BASE_DIR\", \"TMPDIR\", \"XDG_CACHE_HOME\", \"HF_HOME\",\n",
    "    \"UV_CACHE_DIR\", \"TRITON_CACHE_DIR\", \"DATASET_CACHE_DIR\", \"RAY_TMPDIR\",\n",
    "    \"POLICY_MODEL_GPUS\", \"POLICY_MODEL_GPUS_FULL\", \"NEMOGUARD_MODEL_GPUS\",\n",
    "    \"WILDGUARD_MODEL_GPUS\", \"SAFETY_MODEL_GPUS\",\n",
    "    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"VLLM_HOST\",\n",
    "    \"EVAL_CONFIG_OVERRIDES\", \"GPQAD_CONFIG_OVERRIDES\", \"AA_MATH_500_CONFIG_OVERRIDES\",\n",
    "    \"IFEVAL_CONFIG_OVERRIDES\", \"AEGIS_CONFIG_OVERRIDES\", \"WILDGUARD_CONFIG_OVERRIDES\"\n",
    "]\n",
    "\n",
    "with open(\".env\", \"w\") as f:\n",
    "    for var in env_var_names:\n",
    "        value = globals().get(var)\n",
    "        if value is not None:\n",
    "            f.write(f\"{var}={value}\\n\")\n",
    "\n",
    "# Reload .env to store the keys in os.environ\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "print(\"API keys have been stored in .env.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create directories\n",
    "for dir_path in [os.environ['TMPDIR'], os.environ['XDG_CACHE_HOME'], os.environ['HF_HOME'],\n",
    "                 os.environ['UV_CACHE_DIR'],os.environ['TRITON_CACHE_DIR'], os.environ['DATASET_CACHE_DIR'], \n",
    "                 os.environ['RAY_TMPDIR']]:\n",
    "    Path(dir_path).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Download and prepare the NeMo Guard Model for Content Safety\n",
    "\n",
    "For content safety evaluation, we need a guard model (often called a guard model) that classifies whether the model's response is safe or not. As the NeMo Guard model's weights are distributed as LoRA adaptor weights, you need to download them and merge witht the Llama 3.1 8B Instruct.\n",
    "\n",
    "⚠️ **Note**: Make sure to complete the model creation before running Notebooks 1-3 as they require this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "NEMOGUARD_MODEL_PATH = \"/ephemeral/workspace/model/llama-3.1-nemoguard-8b-content-safety\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3.1-8B-Instruct\", torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(base_model, \"nvidia/llama-3.1-nemoguard-8b-content-safety\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Save merged model\n",
    "merged_model.save_pretrained(NEMOGUARD_MODEL_PATH, torch_dtype=torch.bfloat16)\n",
    "tokenizer.save_pretrained(NEMOGUARD_MODEL_PATH) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting familiar with vLLM server launcher\n",
    "\n",
    "In the Developer Blueprint Notebooks, you will launch vLLM servers to perform inference and evaluation. To simplify this process, you'll use `VLLMLauncher`, which provides a more Pythonic and user-friendly interface for managing vLLM server instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scripts.vllm_launcher import VLLMLauncher\n",
    "\n",
    "vllm_launcher = VLLMLauncher(total_num_gpus=8)\n",
    "vllm_server_proc = vllm_launcher.launch(model_name_or_path=\"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\",\n",
    "                                        gpu_devices=\"4,5,6,7\",\n",
    "                                        port=5000,\n",
    "                                        served_model_name=\"test-model\",\n",
    "                                        log_filepath=\"/tmp/vllm-server-model.log\",\n",
    "                                        seed=1,\n",
    "                                        vllm_host=\"0.0.0.0\")\n",
    "\n",
    "!sleep 120                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check if the vLLM server process is running using  `.is_alive()`, and view the latest lines of the server log using `print_log()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vllm_server_proc.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vllm_server_proc.print_log()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also verify GPU usage by the vLLM server process by running `nvidia-smi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop the server process using `.stop()`. If you want to terminate all vLLM server processes launched `VLLMServer` launcher, you can use `vllm_launcher.stop_all()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "vllm_server_proc.stop()\n",
    "# or vllm_launcher.stop_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "You set up the environment for the notebooks. The next step is to run [evaluation of the target model](Step1_Evaluation.ipynb) using safety and accuracy datasets to assess the model’s current performance in both areas.\n",
    "\n",
    "\n",
    "⚠️ **Note**: To ensure GPU VRAM is properly released, it is recommended to select **\"Shut Down Kernel\"** after completing each notebook, including this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
